{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries \n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import random \n",
    "import numpy.random as rand\n",
    "from random import randrange\n",
    "from scipy.stats import bernoulli, binom\n",
    "import seaborn as sns\n",
    "from scipy import stats as stat\n",
    "import pylab \n",
    "from tueplots import axes, bundles , figsizes, fonts,fontsizes\n",
    "from matplotlib.lines import Line2D\n",
    "from imblearn.combine import SMOTEENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for LR\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree          \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
    "from sklearn import metrics\n",
    "# Import libraries \n",
    "from sklearn import tree          \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Import libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import graphviz  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import  DataLoader,SubsetRandomSampler \n",
    "import torch.optim as optim # Optimization package\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that returns dataset with propensity scores and propensity logits from logistic regression\n",
    "def propensity_score_funct(dataset,model):\n",
    "    # Generate propensity score prediction \n",
    "    probabilities = model.predict_proba(dataset.drop('treat', axis=1))\n",
    "    probabilities = pd.DataFrame(probabilities)\n",
    "    ps = probabilities[1] # propensity score \n",
    "    # merge prediction and existing dataset \n",
    "    dataset_proba = pd.merge(dataset, ps, left_index=True, right_index=True)\n",
    "    dataset_proba.rename(index=int, columns={1:'propensity_score'}, inplace = True) # rename column\n",
    "    \n",
    "    #dataset_proba['propensity_logit'] = np.log(dataset_proba['propensity_score'] / (1-dataset_proba['propensity_score']))\n",
    "    dataset_proba['propensity_logit'] = pd.DataFrame(np.nan_to_num(np.log(dataset_proba['propensity_score'] / (1-dataset_proba['propensity_score'])),posinf=0,neginf=0))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample=SMOTEENN(random_state=0)\n",
    "# KFold cross validation \n",
    "def kfold_evaluation_SMOTEENN(input_model,features,target,store):\n",
    "\t# prepare the cross-validation procedure\n",
    "\tskf = StratifiedKFold(n_splits=5, shuffle=True,random_state=1)\n",
    "\t\n",
    "\t#Function to get model scores\t\n",
    "\tdef get_score(model,X_train , X_test, y_train , y_test,fold,store):\n",
    "\t\tmodel.fit(X_train,y_train)\n",
    "\t\tprediction = model.predict(X_test)\n",
    "\t\tpredicted_proba = model.predict_proba(X_test)\n",
    "\t\tprediction_proba_df = pd.DataFrame(predicted_proba)\n",
    "\t\tpos_predicted_proba = prediction_proba_df[1] # Take positive probability predictions for ROC_AUC score  \n",
    "\t\tstore.loc['Accuracy:', fold] = metrics.balanced_accuracy_score(y_test, prediction)\n",
    "\t\tstore.loc['Precision:', fold] = metrics.precision_score(y_test, prediction)\n",
    "\t\tstore.loc['Recall:', fold] = metrics.recall_score(y_test, prediction)\n",
    "\t\tstore.loc['MSE:', fold] =  metrics.mean_squared_error(y_test, prediction)\n",
    "\t\tstore.loc['MAE:', fold] =  metrics.mean_absolute_error(y_test, prediction)\n",
    "\t\tstore.loc['R^2:', fold] =  metrics.r2_score(y_test, prediction)\n",
    "\t\tstore.loc['auc:', fold] =  metrics.roc_auc_score(y_test, prediction)\n",
    "\t\tstore.loc['roc_auc:', fold] =  metrics.roc_auc_score(y_test, pos_predicted_proba)\n",
    "\t\tstore.loc['F1:', fold] =  metrics.f1_score(y_test, prediction)\n",
    "\t\tstore.loc['log-loss:', fold] = metrics.log_loss(y_test, predicted_proba)\n",
    "\t\n",
    "\t#Kfold training loop\n",
    "\tfor fold, (train_index , test_index) in enumerate(skf.split(features,target)):\n",
    "\t\t#print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "\t\tX_train , X_test, y_train , y_test = features.iloc[train_index,:],features.iloc[test_index,:],\\\n",
    "\t\t\t                                 target[train_index] , target[test_index]\n",
    "\t\t\t\t\t\t\t\t\t\t\t \n",
    "\t\tX_train, y_train = resample.fit_resample(X_train, y_train)\t\n",
    "\t\tshuffled = pd.concat([pd.DataFrame(X_train),pd.DataFrame(y_train)],axis=1).sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "\t\t#resplit into test and train \n",
    "\t\tX_train, y_train = shuffled.iloc[:,:-1], shuffled.iloc[:,-1]\n",
    "\t\tget_score(input_model,X_train , X_test, y_train , y_test,fold,store)\n",
    "\t\t# return statement here with python dataframe of metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Load data sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load all datasets.\n",
    "nsw = Male sub sample of treated and control units as used by Lalonde (1986).\n",
    "nswre74  = Further subsample extracted by Dehejia and Wahba (1999) which includes information on earnings in 1974 (re74)\n",
    "\n",
    "cps1 = full Current Population Survey dataset\n",
    "cps2 , cps3 and cps4 are further subsamples exctracted by lalonde to with distributions which better match the nsw treated group.\n",
    "\n",
    "psid1 = full Panel Study of Income Dynamics dataset\n",
    "psid2,psid3,psid4 are again subsamples extracted by Lalonde \n",
    "\n",
    "'''\n",
    "# experimental data \n",
    "\n",
    "nsw = pd.read_stata(\"/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/nsw.dta\")\n",
    "nsw.drop('data_id', axis=1, inplace=True)\n",
    "nswre74_control = pd.read_excel('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/nswre74_control.xls')\n",
    "nswre74_treated = pd.read_excel('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/nswre74_treated.xlsx')\n",
    "nswre74 = pd.concat([nswre74_control,nswre74_treated],axis=0,ignore_index=True) #Combine treatment into a single control as pandas dataframe\n",
    "\n",
    "# non experimental data (cps)\n",
    "\n",
    "cps = pd.read_stata(\"/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/cps_controls.dta\")\n",
    "cps.drop('data_id', axis=1, inplace=True)\n",
    "\n",
    "#cps['treat'] == 0 # created treated column\n",
    "#load non experimental data (psid)\n",
    "\n",
    "psid = pd.read_stata(\"/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/psid_controls.dta\")\n",
    "psid.drop('data_id', axis=1, inplace=True)\n",
    "\n",
    "# reset indexes\n",
    "\n",
    "nsw = nsw.reset_index(drop=True)\n",
    "nswre74 = nswre74.reset_index(drop=True)\n",
    "cps = cps.reset_index(drop=True)\n",
    "psid = psid.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PS estimation helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propensity_score_funct(dataset,model,dataset2,dehwab):\n",
    "    \n",
    "    dataset2.columns = dataset2.columns.str.strip() \n",
    "    \n",
    "    if dehwab == True: \n",
    "        dataset2 = dataset2[['treat','age','education*','black','hispanic','married','nodegree','re74','re75','re78']]\n",
    "    else:\n",
    "        dataset2 = dataset2[['treat','age','education*','black','hispanic','married','nodegree','re75','re78']]\n",
    "\n",
    "    treat =  dataset.iloc[:,0]\n",
    "    dataset =  dataset.iloc[:,1:len(dataset)]\n",
    "\n",
    "    probabilities = model.predict_proba(dataset)\n",
    "    probabilities = pd.DataFrame(probabilities)\n",
    "    ps = probabilities[1]  # propensity score \n",
    "\n",
    "    dataset_proba = pd.merge(dataset2, ps, left_index=True, right_index=True)\n",
    "    dataset_proba.rename(index=int, columns={1:'propensity_score'}, inplace = True) # rename column\n",
    "    dataset_proba['propensity_logit'] = np.log(dataset_proba['propensity_score'] / (1-dataset_proba['propensity_score']))\n",
    "    return dataset_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample=SMOTEENN(random_state=0)\n",
    "# KFold cross validation \n",
    "def kfold_evaluation_SMOTEENN(input_model,features,target,store):\n",
    "\t# prepare the cross-validation procedure\n",
    "\tskf = StratifiedKFold(n_splits=5, shuffle=True,random_state=1)\n",
    "\t\n",
    "\t#Function to get model scores\t\n",
    "\tdef get_score(model,X_train , X_test, y_train , y_test,fold,store):\n",
    "\t\tmodel.fit(X_train,y_train)\n",
    "\t\tprediction = model.predict(X_test)\n",
    "\t\tpredicted_proba = model.predict_proba(X_test)\n",
    "\t\tprediction_proba_df = pd.DataFrame(predicted_proba)\n",
    "\t\tpos_predicted_proba = prediction_proba_df[1] # Take positive probability predictions for ROC_AUC score  \n",
    "\t\tstore.loc['Accuracy:', fold] = metrics.balanced_accuracy_score(y_test, prediction)\n",
    "\t\tstore.loc['Precision:', fold] = metrics.precision_score(y_test, prediction)\n",
    "\t\tstore.loc['Recall:', fold] = metrics.recall_score(y_test, prediction)\n",
    "\t\tstore.loc['MSE:', fold] =  metrics.mean_squared_error(y_test, prediction)\n",
    "\t\tstore.loc['MAE:', fold] =  metrics.mean_absolute_error(y_test, prediction)\n",
    "\t\tstore.loc['R^2:', fold] =  metrics.r2_score(y_test, prediction)\n",
    "\t\tstore.loc['auc:', fold] =  metrics.roc_auc_score(y_test, prediction)\n",
    "\t\tstore.loc['roc_auc:', fold] =  metrics.roc_auc_score(y_test, pos_predicted_proba)\n",
    "\t\tstore.loc['F1:', fold] =  metrics.f1_score(y_test, prediction)\n",
    "\t\tstore.loc['log-loss:', fold] = metrics.log_loss(y_test, predicted_proba)\n",
    "\t\n",
    "\n",
    "\t#Kfold training loop\n",
    "\tfor fold, (train_index , test_index) in enumerate(skf.split(features,target)):\n",
    "\t\t#print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "\t\tX_train , X_test, y_train , y_test = features.iloc[train_index,:],features.iloc[test_index,:],\\\n",
    "\t\t\t                                 target[train_index] , target[test_index]\n",
    "\t\t\t\t\t\t\t\t\t\t\t \n",
    "\t\tX_train, y_train = resample.fit_resample(X_train, y_train)\t\n",
    "\t\t#shuffled = pd.concat([pd.DataFrame(X_train),pd.DataFrame(y_train)],axis=1).sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "\t\tshuffled = pd.concat([pd.DataFrame(X_train),pd.DataFrame(y_train)],axis=1).sample(frac=1,random_state=42)\n",
    "\t\t#resplit into test and train \n",
    "\t\tX_train, y_train = shuffled.iloc[:,:-1], shuffled.iloc[:,-1]\n",
    "\t\tget_score(input_model,X_train , X_test, y_train , y_test,fold,store)\n",
    "\t\t# return statement here with python dataframe of metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Propensity score estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load quasi-experimental dataset\n",
    "nswCps_lalonde =  pd.read_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/unmatched data/Quasi_NswCps_lalonde.csv')\n",
    "nswPsid_lalonde = pd.read_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/unmatched data/Quasi_NswPsid_lalonde.csv')\n",
    "nswCps_dehWab =   pd.read_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/unmatched data/Quasi_NswCps_dehWab.csv')\n",
    "nswPsid_dehWab =  pd.read_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/unmatched data/Quasi_NswPsid_dehWab.csv')\n",
    "\n",
    "nswCps_lalonde.columns = nswCps_lalonde.columns.str.strip() \n",
    "nswPsid_lalonde.columns = nswPsid_lalonde.columns.str.strip() \n",
    "nswCps_dehWab.columns = nswCps_dehWab.columns.str.strip() \n",
    "nswPsid_dehWab.columns = nswPsid_dehWab.columns.str.strip() \n",
    "\n",
    "# Store outcome variable prior to modelling\n",
    "nswCps_lalonde_re78 = nswCps_lalonde.re78\n",
    "nswPsid_lalonde_re78 = nswPsid_lalonde.re78\n",
    "nswCps_dehWab_re78 = nswCps_dehWab.re78\n",
    "nswPsid_dehWab_re78 = nswPsid_dehWab.re78"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PS specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_specification_logit_1 = ['treat','education*','black','hispanic','married','married_u75','nodegree','re75']\n",
    "ps_specification_logit_2 = ['treat','age','education*','black','married','hispanic','black_education','hisp_re75','nodegree','re75']\n",
    "ps_specification_logit_3 = ['treat','age','education*','black','hispanic','nodegree','black_age','married','re74','re75']\n",
    "ps_specification_logit_4 = ['treat','age','education*','black','married','hispanic','married_u75','nodegree','re74','re75']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#['treat','age','education*','black','married','nodegree','re75'] # CPS Lalonde sample \n",
    "markov_blanket_specification_1 = ['treat','age','black','education*','married','nodegree','re75'] # CPS Lalonde sample \n",
    "markov_blanket_specification_2 = ['treat','age','education*','married','nodegree','re75']  # PSID Lalonde sample \n",
    "markov_blanket_specification_3 = ['treat','age','black','education*','married','nodegree','re74','re75'] # CPS Dehwab sample\n",
    "markov_blanket_specification_4 = ['treat','age','education*','married','nodegree','re74','re75'] # PSID Dehwab and wahba sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "annSpecification_1 = ['treat','ageboxcox','education*','black','married','nodegree','re75'] # CPS Lalonde sample \n",
    "annSpecification_2 = ['treat','ageboxcox','education*','married','nodegree','re75']# PSID Lalonde sample  \n",
    "annSpecification_3 = ['treat','ageboxcox','education*','black','married','re74','re75'] # CPS Dehwab sample\n",
    "annSpecification_4 = ['treat','ageboxcox','education*','married','nodegree','re74','re75'] # PSID Dehwab and wahba sample \n",
    "nsw_continuos_vars1 = ['ageboxcox','re75'] \n",
    "nsw_continuos_vars2 = ['ageboxcox','re75'] \n",
    "nswre74_continuos_vars1 = ['ageboxcox','re75',] \n",
    "nswre74_continuos_vars2 = ['ageboxcox','re74','re75',] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model (1) Logistic Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select covariates for use in Logit model \n",
    "\n",
    "# ======== Lalonde (1986) data ======== #\n",
    "\n",
    "nswCps_lalonde_subset = nswCps_lalonde[ps_specification_logit_1]\n",
    "nswPsid_lalonde_subset = nswPsid_lalonde[ps_specification_logit_2]\n",
    "\n",
    "nswCps_lalonde_target = nswCps_lalonde_subset['treat']\n",
    "nswCps_lalonde_features = nswCps_lalonde_subset.drop('treat', axis=1)\n",
    "nswPsid_lalonde_target = nswPsid_lalonde_subset['treat']\n",
    "nswPsid_lalonde_features = nswPsid_lalonde_subset.drop('treat', axis=1)\n",
    "\n",
    "# ======== Dehejia & Wahba (1986) sub sample ======== #\n",
    "\n",
    "nswCps_dehWab_subset = nswCps_dehWab[ps_specification_logit_3]\n",
    "nswPsid_dehWab_subset = nswPsid_dehWab[ps_specification_logit_4]\n",
    "\n",
    "nswCps_dehWab_target = nswCps_dehWab_subset['treat']\n",
    "nswCps_dehWab_features = nswCps_dehWab_subset.drop('treat', axis=1)\n",
    "nswPsid_dehWab_target = nswPsid_dehWab_subset['treat']\n",
    "nswPsid_dehWab_features = nswPsid_dehWab_subset.drop('treat', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
    "from sklearn import metrics\n",
    "\n",
    "# KFold cross validation \n",
    "def kfold_evaluation(input_model,features,target,store):\n",
    "\t# prepare the cross-validation procedure\n",
    "\tskf = StratifiedKFold(n_splits=5, shuffle=True,random_state=1)\n",
    "\t\n",
    "\t#Function to get model scores\t\n",
    "\tdef get_score(model,X_train , X_test, y_train , y_test,fold,store):\n",
    "\t\tmodel.fit(X_train,y_train)\n",
    "\t\tprediction = model.predict(X_test)\n",
    "\t\tpredicted_proba = model.predict_proba(X_test)\n",
    "\t\tprediction_proba_df = pd.DataFrame(predicted_proba)\n",
    "\t\tpos_predicted_proba = prediction_proba_df[1] # Take positive probability predictions for ROC_AUC score  \n",
    "\t\tstore.loc['Accuracy:', fold] = metrics.balanced_accuracy_score(y_test, prediction)\n",
    "\t\tstore.loc['Precision:', fold] = metrics.precision_score(y_test, prediction)\n",
    "\t\tstore.loc['Recall:', fold] = metrics.recall_score(y_test, prediction)\n",
    "\t\tstore.loc['MSE:', fold] =  metrics.mean_squared_error(y_test, prediction)\n",
    "\t\tstore.loc['MAE:', fold] =  metrics.mean_absolute_error(y_test, prediction)\n",
    "\t\tstore.loc['R^2:', fold] =  metrics.r2_score(y_test, prediction)\n",
    "\t\tstore.loc['auc:', fold] =  metrics.roc_auc_score(y_test, prediction)\n",
    "\t\tstore.loc['roc_auc:', fold] =  metrics.roc_auc_score(y_test, pos_predicted_proba)\n",
    "\t\tstore.loc['F1:', fold] =  metrics.f1_score(y_test, prediction)\n",
    "\t\tstore.loc['log-loss:', fold] = metrics.log_loss(y_test, predicted_proba)\n",
    "\t\t\n",
    "\n",
    "\t#Kfold training loop\n",
    "\t\n",
    "\tfor fold, (train_index , test_index) in enumerate(skf.split(features,target)):\n",
    "\t\t#print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "\t\tX_train , X_test, y_train , y_test = features.iloc[train_index,:],features.iloc[test_index,:],\\\n",
    "\t\t\t                                 target[train_index] , target[test_index]\n",
    "\t\t\t\t\t\t\t\t\t\t\t \n",
    "\t\tget_score(input_model,X_train , X_test, y_train , y_test,fold,store)\n",
    "\t\t# return statement here with python dataframe of metrics \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# dataframes to store resutls \n",
    "logit1_metrics = pd.DataFrame()\n",
    "logit2_metrics = pd.DataFrame()\n",
    "logit3_metrics = pd.DataFrame()\n",
    "logit4_metrics = pd.DataFrame()\n",
    "#models \n",
    "logit1 = LogisticRegression(penalty='none',solver='lbfgs', class_weight=\"balanced\",max_iter=500,random_state=0)\n",
    "logit2 = LogisticRegression(penalty='none',solver='lbfgs', class_weight=\"balanced\",max_iter=500,random_state=0)\n",
    "logit3 = LogisticRegression(penalty='none',solver='lbfgs', class_weight=\"balanced\",max_iter=500,random_state=0)\n",
    "logit4 = LogisticRegression(penalty='none',solver='lbfgs', class_weight=\"balanced\",max_iter=500,random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model validation - K fold \n",
    "# ======== Lalonde (1986) sample ======== #\n",
    "kfold_evaluation(logit1,nswCps_lalonde_features,nswCps_lalonde_target,logit1_metrics) # cps\n",
    "kfold_evaluation(logit2,nswPsid_lalonde_features,nswPsid_lalonde_target,logit2_metrics) # psid\n",
    "# ======== Dehejia & Wahba (1999) sub sample ======== #\n",
    "kfold_evaluation(logit3,nswCps_dehWab_features,nswCps_dehWab_target,logit3_metrics) # cps\n",
    "kfold_evaluation(logit4,nswPsid_dehWab_features,nswPsid_dehWab_target,logit4_metrics) # psid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit1_metrics['avg'] = np.mean(logit1_metrics,axis=1)\n",
    "logit2_metrics['avg'] = np.mean(logit2_metrics,axis=1)\n",
    "logit3_metrics['avg'] = np.mean(logit3_metrics,axis=1)\n",
    "logit4_metrics['avg'] = np.mean(logit4_metrics,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy:</th>\n",
       "      <td>0.888868</td>\n",
       "      <td>0.864759</td>\n",
       "      <td>0.861991</td>\n",
       "      <td>0.900891</td>\n",
       "      <td>0.892650</td>\n",
       "      <td>0.881832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision:</th>\n",
       "      <td>0.121348</td>\n",
       "      <td>0.130890</td>\n",
       "      <td>0.112360</td>\n",
       "      <td>0.129496</td>\n",
       "      <td>0.096552</td>\n",
       "      <td>0.118129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall:</th>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.847458</td>\n",
       "      <td>0.915254</td>\n",
       "      <td>0.949153</td>\n",
       "      <td>0.889040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE:</th>\n",
       "      <td>0.121854</td>\n",
       "      <td>0.104972</td>\n",
       "      <td>0.124002</td>\n",
       "      <td>0.112953</td>\n",
       "      <td>0.161805</td>\n",
       "      <td>0.125117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE:</th>\n",
       "      <td>0.121854</td>\n",
       "      <td>0.104972</td>\n",
       "      <td>0.124002</td>\n",
       "      <td>0.112953</td>\n",
       "      <td>0.161805</td>\n",
       "      <td>0.125117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R^2:</th>\n",
       "      <td>-5.740807</td>\n",
       "      <td>-4.806942</td>\n",
       "      <td>-5.973747</td>\n",
       "      <td>-5.352324</td>\n",
       "      <td>-8.096994</td>\n",
       "      <td>-5.994163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc:</th>\n",
       "      <td>0.888868</td>\n",
       "      <td>0.864759</td>\n",
       "      <td>0.861991</td>\n",
       "      <td>0.900891</td>\n",
       "      <td>0.892650</td>\n",
       "      <td>0.881832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc:</th>\n",
       "      <td>0.962044</td>\n",
       "      <td>0.947871</td>\n",
       "      <td>0.945158</td>\n",
       "      <td>0.959619</td>\n",
       "      <td>0.964756</td>\n",
       "      <td>0.955889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1:</th>\n",
       "      <td>0.213861</td>\n",
       "      <td>0.226244</td>\n",
       "      <td>0.198413</td>\n",
       "      <td>0.226891</td>\n",
       "      <td>0.175274</td>\n",
       "      <td>0.208137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log-loss:</th>\n",
       "      <td>0.263571</td>\n",
       "      <td>0.241323</td>\n",
       "      <td>0.262708</td>\n",
       "      <td>0.270125</td>\n",
       "      <td>0.337827</td>\n",
       "      <td>0.275111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4       avg\n",
       "Accuracy:   0.888868  0.864759  0.861991  0.900891  0.892650  0.881832\n",
       "Precision:  0.121348  0.130890  0.112360  0.129496  0.096552  0.118129\n",
       "Recall:     0.900000  0.833333  0.847458  0.915254  0.949153  0.889040\n",
       "MSE:        0.121854  0.104972  0.124002  0.112953  0.161805  0.125117\n",
       "MAE:        0.121854  0.104972  0.124002  0.112953  0.161805  0.125117\n",
       "R^2:       -5.740807 -4.806942 -5.973747 -5.352324 -8.096994 -5.994163\n",
       "auc:        0.888868  0.864759  0.861991  0.900891  0.892650  0.881832\n",
       "roc_auc:    0.962044  0.947871  0.945158  0.959619  0.964756  0.955889\n",
       "F1:         0.213861  0.226244  0.198413  0.226891  0.175274  0.208137\n",
       "log-loss:   0.263571  0.241323  0.262708  0.270125  0.337827  0.275111"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit1_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'class_weight': 'balanced',\n",
       " 'dual': False,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1,\n",
       " 'l1_ratio': None,\n",
       " 'max_iter': 500,\n",
       " 'multi_class': 'auto',\n",
       " 'n_jobs': None,\n",
       " 'penalty': 'none',\n",
       " 'random_state': 0,\n",
       " 'solver': 'lbfgs',\n",
       " 'tol': 0.0001,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit1.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy:</th>\n",
       "      <td>0.869779</td>\n",
       "      <td>0.913855</td>\n",
       "      <td>0.862603</td>\n",
       "      <td>0.904976</td>\n",
       "      <td>0.917790</td>\n",
       "      <td>0.893801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision:</th>\n",
       "      <td>0.481132</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.546392</td>\n",
       "      <td>0.467213</td>\n",
       "      <td>0.523295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall:</th>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.813559</td>\n",
       "      <td>0.898305</td>\n",
       "      <td>0.966102</td>\n",
       "      <td>0.885593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE:</th>\n",
       "      <td>0.114695</td>\n",
       "      <td>0.075269</td>\n",
       "      <td>0.098743</td>\n",
       "      <td>0.089767</td>\n",
       "      <td>0.120287</td>\n",
       "      <td>0.099752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE:</th>\n",
       "      <td>0.114695</td>\n",
       "      <td>0.075269</td>\n",
       "      <td>0.098743</td>\n",
       "      <td>0.089767</td>\n",
       "      <td>0.120287</td>\n",
       "      <td>0.099752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R^2:</th>\n",
       "      <td>-0.195181</td>\n",
       "      <td>0.215663</td>\n",
       "      <td>-0.042645</td>\n",
       "      <td>0.052141</td>\n",
       "      <td>-0.270131</td>\n",
       "      <td>-0.048031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc:</th>\n",
       "      <td>0.869779</td>\n",
       "      <td>0.913855</td>\n",
       "      <td>0.862603</td>\n",
       "      <td>0.904976</td>\n",
       "      <td>0.917790</td>\n",
       "      <td>0.893801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc:</th>\n",
       "      <td>0.960793</td>\n",
       "      <td>0.972122</td>\n",
       "      <td>0.951603</td>\n",
       "      <td>0.972262</td>\n",
       "      <td>0.974032</td>\n",
       "      <td>0.966162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1:</th>\n",
       "      <td>0.614458</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.635762</td>\n",
       "      <td>0.679487</td>\n",
       "      <td>0.629834</td>\n",
       "      <td>0.655908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log-loss:</th>\n",
       "      <td>0.250577</td>\n",
       "      <td>0.200025</td>\n",
       "      <td>0.241414</td>\n",
       "      <td>0.204177</td>\n",
       "      <td>0.283941</td>\n",
       "      <td>0.236027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4       avg\n",
       "Accuracy:   0.869779  0.913855  0.862603  0.904976  0.917790  0.893801\n",
       "Precision:  0.481132  0.600000  0.521739  0.546392  0.467213  0.523295\n",
       "Recall:     0.850000  0.900000  0.813559  0.898305  0.966102  0.885593\n",
       "MSE:        0.114695  0.075269  0.098743  0.089767  0.120287  0.099752\n",
       "MAE:        0.114695  0.075269  0.098743  0.089767  0.120287  0.099752\n",
       "R^2:       -0.195181  0.215663 -0.042645  0.052141 -0.270131 -0.048031\n",
       "auc:        0.869779  0.913855  0.862603  0.904976  0.917790  0.893801\n",
       "roc_auc:    0.960793  0.972122  0.951603  0.972262  0.974032  0.966162\n",
       "F1:         0.614458  0.720000  0.635762  0.679487  0.629834  0.655908\n",
       "log-loss:   0.250577  0.200025  0.241414  0.204177  0.283941  0.236027"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit2_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'class_weight': 'balanced',\n",
       " 'dual': False,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1,\n",
       " 'l1_ratio': None,\n",
       " 'max_iter': 500,\n",
       " 'multi_class': 'auto',\n",
       " 'n_jobs': None,\n",
       " 'penalty': 'none',\n",
       " 'random_state': 0,\n",
       " 'solver': 'lbfgs',\n",
       " 'tol': 0.0001,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit2.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy:</th>\n",
       "      <td>0.879603</td>\n",
       "      <td>0.878040</td>\n",
       "      <td>0.875767</td>\n",
       "      <td>0.893413</td>\n",
       "      <td>0.936143</td>\n",
       "      <td>0.892593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision:</th>\n",
       "      <td>0.086486</td>\n",
       "      <td>0.084211</td>\n",
       "      <td>0.100977</td>\n",
       "      <td>0.089431</td>\n",
       "      <td>0.100559</td>\n",
       "      <td>0.092333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall:</th>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.891892</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.886486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE:</th>\n",
       "      <td>0.105995</td>\n",
       "      <td>0.109085</td>\n",
       "      <td>0.087172</td>\n",
       "      <td>0.105100</td>\n",
       "      <td>0.099845</td>\n",
       "      <td>0.101440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE:</th>\n",
       "      <td>0.105995</td>\n",
       "      <td>0.109085</td>\n",
       "      <td>0.087172</td>\n",
       "      <td>0.105100</td>\n",
       "      <td>0.099845</td>\n",
       "      <td>0.101440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R^2:</th>\n",
       "      <td>-8.377491</td>\n",
       "      <td>-8.650888</td>\n",
       "      <td>-6.709802</td>\n",
       "      <td>-8.295506</td>\n",
       "      <td>-7.830730</td>\n",
       "      <td>-7.972883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc:</th>\n",
       "      <td>0.879603</td>\n",
       "      <td>0.878040</td>\n",
       "      <td>0.875767</td>\n",
       "      <td>0.893413</td>\n",
       "      <td>0.936143</td>\n",
       "      <td>0.892593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc:</th>\n",
       "      <td>0.961863</td>\n",
       "      <td>0.948616</td>\n",
       "      <td>0.969677</td>\n",
       "      <td>0.953949</td>\n",
       "      <td>0.983055</td>\n",
       "      <td>0.963432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1:</th>\n",
       "      <td>0.157248</td>\n",
       "      <td>0.153477</td>\n",
       "      <td>0.180233</td>\n",
       "      <td>0.162562</td>\n",
       "      <td>0.182278</td>\n",
       "      <td>0.167160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log-loss:</th>\n",
       "      <td>0.233103</td>\n",
       "      <td>0.260611</td>\n",
       "      <td>0.213819</td>\n",
       "      <td>0.247897</td>\n",
       "      <td>0.239693</td>\n",
       "      <td>0.239025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4       avg\n",
       "Accuracy:   0.879603  0.878040  0.875767  0.893413  0.936143  0.892593\n",
       "Precision:  0.086486  0.084211  0.100977  0.089431  0.100559  0.092333\n",
       "Recall:     0.864865  0.864865  0.837838  0.891892  0.972973  0.886486\n",
       "MSE:        0.105995  0.109085  0.087172  0.105100  0.099845  0.101440\n",
       "MAE:        0.105995  0.109085  0.087172  0.105100  0.099845  0.101440\n",
       "R^2:       -8.377491 -8.650888 -6.709802 -8.295506 -7.830730 -7.972883\n",
       "auc:        0.879603  0.878040  0.875767  0.893413  0.936143  0.892593\n",
       "roc_auc:    0.961863  0.948616  0.969677  0.953949  0.983055  0.963432\n",
       "F1:         0.157248  0.153477  0.180233  0.162562  0.182278  0.167160\n",
       "log-loss:   0.233103  0.260611  0.213819  0.247897  0.239693  0.239025"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit3_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.get_params of LogisticRegression(class_weight='balanced', max_iter=500, penalty='none',\n",
       "                   random_state=0)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit3.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy:</th>\n",
       "      <td>0.942310</td>\n",
       "      <td>0.897292</td>\n",
       "      <td>0.957831</td>\n",
       "      <td>0.885244</td>\n",
       "      <td>0.957831</td>\n",
       "      <td>0.928102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision:</th>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.477612</td>\n",
       "      <td>0.468354</td>\n",
       "      <td>0.405063</td>\n",
       "      <td>0.468354</td>\n",
       "      <td>0.453877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall:</th>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.940541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE:</th>\n",
       "      <td>0.084112</td>\n",
       "      <td>0.074766</td>\n",
       "      <td>0.078505</td>\n",
       "      <td>0.097196</td>\n",
       "      <td>0.078505</td>\n",
       "      <td>0.082617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE:</th>\n",
       "      <td>0.084112</td>\n",
       "      <td>0.074766</td>\n",
       "      <td>0.078505</td>\n",
       "      <td>0.097196</td>\n",
       "      <td>0.078505</td>\n",
       "      <td>0.082617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R^2:</th>\n",
       "      <td>-0.306578</td>\n",
       "      <td>-0.161402</td>\n",
       "      <td>-0.219472</td>\n",
       "      <td>-0.509823</td>\n",
       "      <td>-0.219472</td>\n",
       "      <td>-0.283350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc:</th>\n",
       "      <td>0.942310</td>\n",
       "      <td>0.897292</td>\n",
       "      <td>0.957831</td>\n",
       "      <td>0.885244</td>\n",
       "      <td>0.957831</td>\n",
       "      <td>0.928102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc:</th>\n",
       "      <td>0.983122</td>\n",
       "      <td>0.962254</td>\n",
       "      <td>0.982036</td>\n",
       "      <td>0.949256</td>\n",
       "      <td>0.986215</td>\n",
       "      <td>0.972577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1:</th>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.637931</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.637931</td>\n",
       "      <td>0.611671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log-loss:</th>\n",
       "      <td>0.192810</td>\n",
       "      <td>0.177873</td>\n",
       "      <td>0.200565</td>\n",
       "      <td>0.260109</td>\n",
       "      <td>0.206918</td>\n",
       "      <td>0.207655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4       avg\n",
       "Accuracy:   0.942310  0.897292  0.957831  0.885244  0.957831  0.928102\n",
       "Precision:  0.450000  0.477612  0.468354  0.405063  0.468354  0.453877\n",
       "Recall:     0.972973  0.864865  1.000000  0.864865  1.000000  0.940541\n",
       "MSE:        0.084112  0.074766  0.078505  0.097196  0.078505  0.082617\n",
       "MAE:        0.084112  0.074766  0.078505  0.097196  0.078505  0.082617\n",
       "R^2:       -0.306578 -0.161402 -0.219472 -0.509823 -0.219472 -0.283350\n",
       "auc:        0.942310  0.897292  0.957831  0.885244  0.957831  0.928102\n",
       "roc_auc:    0.983122  0.962254  0.982036  0.949256  0.986215  0.972577\n",
       "F1:         0.615385  0.615385  0.637931  0.551724  0.637931  0.611671\n",
       "log-loss:   0.192810  0.177873  0.200565  0.260109  0.206918  0.207655"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit4_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'class_weight': 'balanced',\n",
       " 'dual': False,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1,\n",
       " 'l1_ratio': None,\n",
       " 'max_iter': 500,\n",
       " 'multi_class': 'auto',\n",
       " 'n_jobs': None,\n",
       " 'penalty': 'none',\n",
       " 'random_state': 0,\n",
       " 'solver': 'lbfgs',\n",
       " 'tol': 0.0001,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit4.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;, max_iter=500, penalty=&#x27;none&#x27;,\n",
       "                   random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;, max_iter=500, penalty=&#x27;none&#x27;,\n",
       "                   random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(class_weight='balanced', max_iter=500, penalty='none',\n",
       "                   random_state=0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit Logistic Regression using sklearn\n",
    "# ======== Lalonde (1986) sample ======== #\n",
    "logit1.fit(nswCps_lalonde_features,nswCps_lalonde_target) # cps\n",
    "logit2.fit(nswPsid_lalonde_features,nswPsid_lalonde_target) # psid\n",
    "# ======== Dehejia & Wahba (1999) sub sample ======== #\n",
    "logit3.fit(nswCps_dehWab_features,nswCps_dehWab_target) # cps\n",
    "logit4.fit(nswPsid_dehWab_features,nswPsid_dehWab_target) # psid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change logisistic regression to STAT's model implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict get scores on NSW , CPS and PSID \n",
    "# ============ Lalonde Subsample ============ # \n",
    "nswCps_lalonde_ps_LOGIT_withRe78 = propensity_score_funct(nswCps_lalonde_subset,logit1,nswCps_lalonde,False)\n",
    "nswPsid_lalonde_ps_LOGIT_withRe78= propensity_score_funct(nswPsid_lalonde_subset,logit2,nswPsid_lalonde,False)\n",
    "# ============ Dehejia & Wahba sub sample ============ # \n",
    "# predict propensity scores\n",
    "nswCps_dehWab_ps_LOGIT_withRe78 = propensity_score_funct(nswCps_dehWab_subset,logit3,nswCps_dehWab,True)\n",
    "nswPsid_dehWab_ps_LOGIT_withRe78 = propensity_score_funct(nswPsid_dehWab_subset,logit4,nswPsid_dehWab,True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save unmatched logit datasets \n",
    "nswCps_lalonde_ps_LOGIT_withRe78.to_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/logit/unmatched/nswCps_lalonde_ps_unmatched_LOGIT_FS1.csv')\n",
    "nswPsid_lalonde_ps_LOGIT_withRe78.to_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/logit/unmatched/nswPsid_lalonde_ps_unmatched_LOGIT_FS1.csv')\n",
    "nswCps_dehWab_ps_LOGIT_withRe78.to_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/logit/unmatched/nswCps_dehWab_ps_unmatched_LOGIT_FS1.csv')\n",
    "nswPsid_dehWab_ps_LOGIT_withRe78.to_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/logit/unmatched/nswPsid_dehWab_ps_unmatched_LOGIT_FS1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model (2) ; CART "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load quasi-experimental dataset\n",
    "nswCps_lalonde =  pd.read_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/unmatched data/Quasi_NswCps_lalonde.csv')\n",
    "nswPsid_lalonde = pd.read_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/unmatched data/Quasi_NswPsid_lalonde.csv')\n",
    "nswCps_dehWab =   pd.read_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/unmatched data/Quasi_NswCps_dehWab.csv')\n",
    "nswPsid_dehWab =  pd.read_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/unmatched data/Quasi_NswPsid_dehWab.csv')\n",
    "\n",
    "nswCps_lalonde.columns = nswCps_lalonde.columns.str.strip() \n",
    "nswPsid_lalonde.columns = nswPsid_lalonde.columns.str.strip() \n",
    "nswCps_dehWab.columns = nswCps_dehWab.columns.str.strip() \n",
    "nswPsid_dehWab.columns = nswPsid_dehWab.columns.str.strip() \n",
    "\n",
    "# Store outcome variable prior to modelling\n",
    "\n",
    "nswCps_lalonde_re78 = nswCps_lalonde.re78\n",
    "nswPsid_lalonde_re78 = nswPsid_lalonde.re78\n",
    "nswCps_dehWab_re78 = nswCps_dehWab.re78\n",
    "nswPsid_dehWab_re78 = nswPsid_dehWab.re78"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CART - Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Select covariates for use in CART model - from GNN features \"\"\"\n",
    "# ======== Lalonde (1986) data ======== #\n",
    "nswCps_lalonde_subset = nswCps_lalonde[markov_blanket_specification_1]\n",
    "nswPsid_lalonde_subset = nswPsid_lalonde[markov_blanket_specification_2]\n",
    "\n",
    "nswCps_lalonde_target = nswCps_lalonde_subset['treat']\n",
    "nswCps_lalonde_features = nswCps_lalonde_subset.drop('treat', axis=1)\n",
    "nswPsid_lalonde_target = nswPsid_lalonde_subset['treat']\n",
    "nswPsid_lalonde_features = nswPsid_lalonde_subset.drop('treat', axis=1)\n",
    "\n",
    "# ======== Dehejia & Wahba (1986) sub sample ======== #\n",
    "\n",
    "# Apply selection\n",
    "\n",
    "nswCps_dehWab_subset = nswCps_dehWab[markov_blanket_specification_3]\n",
    "nswPsid_dehWab_subset = nswPsid_dehWab[markov_blanket_specification_4]\n",
    "\n",
    "nswCps_dehWab_target = nswCps_dehWab_subset['treat']\n",
    "nswCps_dehWab_features = nswCps_dehWab_subset.drop('treat', axis=1)\n",
    "nswPsid_dehWab_target = nswPsid_dehWab_subset['treat']\n",
    "nswPsid_dehWab_features = nswPsid_dehWab_subset.drop('treat', axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search \n",
    "tree_param = [{'criterion': ['entropy', 'gini'], \n",
    "               'max_depth': [1,2,3,4,5,6,7,8,9,10,None],\n",
    "               'max_leaf_nodes':list(range(0, 100)),\n",
    "              'min_samples_leaf': [2, 3, 4]}]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cart1_metrics = pd.DataFrame()\n",
    "cart2_metrics = pd.DataFrame()\n",
    "cart3_metrics = pd.DataFrame()\n",
    "cart4_metrics = pd.DataFrame()\n",
    "\n",
    "grid_search_cart1 = tree.DecisionTreeClassifier(random_state=0,criterion = 'entropy',max_depth = 5,max_leaf_nodes = 12,min_samples_leaf = 2)\n",
    "grid_search_cart2 = tree.DecisionTreeClassifier(random_state=0,criterion = 'entropy',max_depth = 7,max_leaf_nodes = 10,min_samples_leaf = 2)\n",
    "grid_search_cart3 = tree.DecisionTreeClassifier(random_state=0,criterion = 'entropy',max_depth = 5,max_leaf_nodes = 10,min_samples_leaf = 2)\n",
    "grid_search_cart4 = tree.DecisionTreeClassifier(random_state=0,criterion = 'entropy',max_depth = 5,max_leaf_nodes = 10,min_samples_leaf = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_evaluation_SMOTEENN(grid_search_cart1,nswCps_lalonde_features,nswCps_lalonde_target,cart1_metrics) # cps\n",
    "kfold_evaluation_SMOTEENN(grid_search_cart2,nswPsid_lalonde_features,nswPsid_lalonde_target,cart2_metrics) # psid\n",
    "kfold_evaluation_SMOTEENN(grid_search_cart3,nswCps_dehWab_features,nswCps_dehWab_target,cart3_metrics) # cps\n",
    "kfold_evaluation_SMOTEENN(grid_search_cart4,nswPsid_dehWab_features,nswPsid_dehWab_target,cart4_metrics) # psid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> model evaluation </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cart1_metrics['avg'] = np.mean(cart1_metrics,axis=1)\n",
    "cart2_metrics['avg'] = np.mean(cart2_metrics,axis=1)\n",
    "cart3_metrics['avg'] = np.mean(cart3_metrics,axis=1)\n",
    "cart4_metrics['avg'] = np.mean(cart4_metrics,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy:</th>\n",
       "      <td>0.828377</td>\n",
       "      <td>0.844575</td>\n",
       "      <td>0.811093</td>\n",
       "      <td>0.855063</td>\n",
       "      <td>0.861876</td>\n",
       "      <td>0.840197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision:</th>\n",
       "      <td>0.325397</td>\n",
       "      <td>0.328244</td>\n",
       "      <td>0.351852</td>\n",
       "      <td>0.278481</td>\n",
       "      <td>0.167857</td>\n",
       "      <td>0.290366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall:</th>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.644068</td>\n",
       "      <td>0.745763</td>\n",
       "      <td>0.796610</td>\n",
       "      <td>0.717288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE:</th>\n",
       "      <td>0.031921</td>\n",
       "      <td>0.032228</td>\n",
       "      <td>0.027931</td>\n",
       "      <td>0.039595</td>\n",
       "      <td>0.075223</td>\n",
       "      <td>0.041380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE:</th>\n",
       "      <td>0.031921</td>\n",
       "      <td>0.032228</td>\n",
       "      <td>0.027931</td>\n",
       "      <td>0.039595</td>\n",
       "      <td>0.075223</td>\n",
       "      <td>0.041380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R^2:</th>\n",
       "      <td>-0.765854</td>\n",
       "      <td>-0.782833</td>\n",
       "      <td>-0.570819</td>\n",
       "      <td>-1.226766</td>\n",
       "      <td>-3.229153</td>\n",
       "      <td>-1.315085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc:</th>\n",
       "      <td>0.828377</td>\n",
       "      <td>0.844575</td>\n",
       "      <td>0.811093</td>\n",
       "      <td>0.855063</td>\n",
       "      <td>0.861876</td>\n",
       "      <td>0.840197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc:</th>\n",
       "      <td>0.912344</td>\n",
       "      <td>0.904073</td>\n",
       "      <td>0.934929</td>\n",
       "      <td>0.952414</td>\n",
       "      <td>0.950856</td>\n",
       "      <td>0.930923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1:</th>\n",
       "      <td>0.440860</td>\n",
       "      <td>0.450262</td>\n",
       "      <td>0.455090</td>\n",
       "      <td>0.405530</td>\n",
       "      <td>0.277286</td>\n",
       "      <td>0.405806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log-loss:</th>\n",
       "      <td>0.203878</td>\n",
       "      <td>0.153560</td>\n",
       "      <td>0.163919</td>\n",
       "      <td>0.171362</td>\n",
       "      <td>0.270913</td>\n",
       "      <td>0.192726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4       avg\n",
       "Accuracy:   0.828377  0.844575  0.811093  0.855063  0.861876  0.840197\n",
       "Precision:  0.325397  0.328244  0.351852  0.278481  0.167857  0.290366\n",
       "Recall:     0.683333  0.716667  0.644068  0.745763  0.796610  0.717288\n",
       "MSE:        0.031921  0.032228  0.027931  0.039595  0.075223  0.041380\n",
       "MAE:        0.031921  0.032228  0.027931  0.039595  0.075223  0.041380\n",
       "R^2:       -0.765854 -0.782833 -0.570819 -1.226766 -3.229153 -1.315085\n",
       "auc:        0.828377  0.844575  0.811093  0.855063  0.861876  0.840197\n",
       "roc_auc:    0.912344  0.904073  0.934929  0.952414  0.950856  0.930923\n",
       "F1:         0.440860  0.450262  0.455090  0.405530  0.277286  0.405806\n",
       "log-loss:   0.203878  0.153560  0.163919  0.171362  0.270913  0.192726"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cart1_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy:</th>\n",
       "      <td>0.826205</td>\n",
       "      <td>0.878916</td>\n",
       "      <td>0.848104</td>\n",
       "      <td>0.916020</td>\n",
       "      <td>0.904414</td>\n",
       "      <td>0.874732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision:</th>\n",
       "      <td>0.573333</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.484536</td>\n",
       "      <td>0.616279</td>\n",
       "      <td>0.504673</td>\n",
       "      <td>0.574895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall:</th>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.796610</td>\n",
       "      <td>0.898305</td>\n",
       "      <td>0.915254</td>\n",
       "      <td>0.825367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE:</th>\n",
       "      <td>0.087814</td>\n",
       "      <td>0.059140</td>\n",
       "      <td>0.111311</td>\n",
       "      <td>0.070018</td>\n",
       "      <td>0.104129</td>\n",
       "      <td>0.086482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE:</th>\n",
       "      <td>0.087814</td>\n",
       "      <td>0.059140</td>\n",
       "      <td>0.111311</td>\n",
       "      <td>0.070018</td>\n",
       "      <td>0.104129</td>\n",
       "      <td>0.086482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R^2:</th>\n",
       "      <td>0.084940</td>\n",
       "      <td>0.383735</td>\n",
       "      <td>-0.175345</td>\n",
       "      <td>0.260670</td>\n",
       "      <td>-0.099517</td>\n",
       "      <td>0.090896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc:</th>\n",
       "      <td>0.826205</td>\n",
       "      <td>0.878916</td>\n",
       "      <td>0.848104</td>\n",
       "      <td>0.916020</td>\n",
       "      <td>0.904414</td>\n",
       "      <td>0.874732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc:</th>\n",
       "      <td>0.886044</td>\n",
       "      <td>0.932112</td>\n",
       "      <td>0.918930</td>\n",
       "      <td>0.952709</td>\n",
       "      <td>0.960724</td>\n",
       "      <td>0.930104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1:</th>\n",
       "      <td>0.637037</td>\n",
       "      <td>0.744186</td>\n",
       "      <td>0.602564</td>\n",
       "      <td>0.731034</td>\n",
       "      <td>0.650602</td>\n",
       "      <td>0.673085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log-loss:</th>\n",
       "      <td>1.055793</td>\n",
       "      <td>0.325228</td>\n",
       "      <td>0.364697</td>\n",
       "      <td>0.233530</td>\n",
       "      <td>0.288515</td>\n",
       "      <td>0.453553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4       avg\n",
       "Accuracy:   0.826205  0.878916  0.848104  0.916020  0.904414  0.874732\n",
       "Precision:  0.573333  0.695652  0.484536  0.616279  0.504673  0.574895\n",
       "Recall:     0.716667  0.800000  0.796610  0.898305  0.915254  0.825367\n",
       "MSE:        0.087814  0.059140  0.111311  0.070018  0.104129  0.086482\n",
       "MAE:        0.087814  0.059140  0.111311  0.070018  0.104129  0.086482\n",
       "R^2:        0.084940  0.383735 -0.175345  0.260670 -0.099517  0.090896\n",
       "auc:        0.826205  0.878916  0.848104  0.916020  0.904414  0.874732\n",
       "roc_auc:    0.886044  0.932112  0.918930  0.952709  0.960724  0.930104\n",
       "F1:         0.637037  0.744186  0.602564  0.731034  0.650602  0.673085\n",
       "log-loss:   1.055793  0.325228  0.364697  0.233530  0.288515  0.453553"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cart2_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy:</th>\n",
       "      <td>0.907040</td>\n",
       "      <td>0.883055</td>\n",
       "      <td>0.891490</td>\n",
       "      <td>0.894305</td>\n",
       "      <td>0.922270</td>\n",
       "      <td>0.899632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision:</th>\n",
       "      <td>0.289720</td>\n",
       "      <td>0.173410</td>\n",
       "      <td>0.252101</td>\n",
       "      <td>0.297030</td>\n",
       "      <td>0.329897</td>\n",
       "      <td>0.268431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall:</th>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.827027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE:</th>\n",
       "      <td>0.025340</td>\n",
       "      <td>0.046354</td>\n",
       "      <td>0.029675</td>\n",
       "      <td>0.024111</td>\n",
       "      <td>0.021638</td>\n",
       "      <td>0.029424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE:</th>\n",
       "      <td>0.025340</td>\n",
       "      <td>0.046354</td>\n",
       "      <td>0.029675</td>\n",
       "      <td>0.024111</td>\n",
       "      <td>0.021638</td>\n",
       "      <td>0.029424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R^2:</th>\n",
       "      <td>-1.241849</td>\n",
       "      <td>-3.100944</td>\n",
       "      <td>-1.624613</td>\n",
       "      <td>-1.132498</td>\n",
       "      <td>-0.913781</td>\n",
       "      <td>-1.602737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc:</th>\n",
       "      <td>0.907040</td>\n",
       "      <td>0.883055</td>\n",
       "      <td>0.891490</td>\n",
       "      <td>0.894305</td>\n",
       "      <td>0.922270</td>\n",
       "      <td>0.899632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc:</th>\n",
       "      <td>0.946613</td>\n",
       "      <td>0.921542</td>\n",
       "      <td>0.908499</td>\n",
       "      <td>0.974351</td>\n",
       "      <td>0.974059</td>\n",
       "      <td>0.945013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1:</th>\n",
       "      <td>0.430556</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.477612</td>\n",
       "      <td>0.402656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log-loss:</th>\n",
       "      <td>0.107183</td>\n",
       "      <td>0.154493</td>\n",
       "      <td>0.116569</td>\n",
       "      <td>0.138555</td>\n",
       "      <td>0.106300</td>\n",
       "      <td>0.124620</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4       avg\n",
       "Accuracy:   0.907040  0.883055  0.891490  0.894305  0.922270  0.899632\n",
       "Precision:  0.289720  0.173410  0.252101  0.297030  0.329897  0.268431\n",
       "Recall:     0.837838  0.810811  0.810811  0.810811  0.864865  0.827027\n",
       "MSE:        0.025340  0.046354  0.029675  0.024111  0.021638  0.029424\n",
       "MAE:        0.025340  0.046354  0.029675  0.024111  0.021638  0.029424\n",
       "R^2:       -1.241849 -3.100944 -1.624613 -1.132498 -0.913781 -1.602737\n",
       "auc:        0.907040  0.883055  0.891490  0.894305  0.922270  0.899632\n",
       "roc_auc:    0.946613  0.921542  0.908499  0.974351  0.974059  0.945013\n",
       "F1:         0.430556  0.285714  0.384615  0.434783  0.477612  0.402656\n",
       "log-loss:   0.107183  0.154493  0.116569  0.138555  0.106300  0.124620"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cart3_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy:</th>\n",
       "      <td>0.933355</td>\n",
       "      <td>0.874824</td>\n",
       "      <td>0.941848</td>\n",
       "      <td>0.860767</td>\n",
       "      <td>0.918376</td>\n",
       "      <td>0.905834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision:</th>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.530303</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.581278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall:</th>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.859459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE:</th>\n",
       "      <td>0.054206</td>\n",
       "      <td>0.046729</td>\n",
       "      <td>0.061682</td>\n",
       "      <td>0.072897</td>\n",
       "      <td>0.035514</td>\n",
       "      <td>0.054206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE:</th>\n",
       "      <td>0.054206</td>\n",
       "      <td>0.046729</td>\n",
       "      <td>0.061682</td>\n",
       "      <td>0.072897</td>\n",
       "      <td>0.035514</td>\n",
       "      <td>0.054206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R^2:</th>\n",
       "      <td>0.157983</td>\n",
       "      <td>0.274124</td>\n",
       "      <td>0.041843</td>\n",
       "      <td>-0.132367</td>\n",
       "      <td>0.448334</td>\n",
       "      <td>0.157983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc:</th>\n",
       "      <td>0.933355</td>\n",
       "      <td>0.874824</td>\n",
       "      <td>0.941848</td>\n",
       "      <td>0.860767</td>\n",
       "      <td>0.918376</td>\n",
       "      <td>0.905834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc:</th>\n",
       "      <td>0.976419</td>\n",
       "      <td>0.943721</td>\n",
       "      <td>0.979893</td>\n",
       "      <td>0.886356</td>\n",
       "      <td>0.925594</td>\n",
       "      <td>0.942397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1:</th>\n",
       "      <td>0.701031</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.679612</td>\n",
       "      <td>0.597938</td>\n",
       "      <td>0.771084</td>\n",
       "      <td>0.689692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log-loss:</th>\n",
       "      <td>0.200567</td>\n",
       "      <td>0.563904</td>\n",
       "      <td>0.612808</td>\n",
       "      <td>0.612825</td>\n",
       "      <td>0.417297</td>\n",
       "      <td>0.481480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4       avg\n",
       "Accuracy:   0.933355  0.874824  0.941848  0.860767  0.918376  0.905834\n",
       "Precision:  0.566667  0.630435  0.530303  0.483333  0.695652  0.581278\n",
       "Recall:     0.918919  0.783784  0.945946  0.783784  0.864865  0.859459\n",
       "MSE:        0.054206  0.046729  0.061682  0.072897  0.035514  0.054206\n",
       "MAE:        0.054206  0.046729  0.061682  0.072897  0.035514  0.054206\n",
       "R^2:        0.157983  0.274124  0.041843 -0.132367  0.448334  0.157983\n",
       "auc:        0.933355  0.874824  0.941848  0.860767  0.918376  0.905834\n",
       "roc_auc:    0.976419  0.943721  0.979893  0.886356  0.925594  0.942397\n",
       "F1:         0.701031  0.698795  0.679612  0.597938  0.771084  0.689692\n",
       "log-loss:   0.200567  0.563904  0.612808  0.612825  0.417297  0.481480"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cart4_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/pandas/core/arraylike.py:397: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/pandas/core/arraylike.py:397: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/pandas/core/arraylike.py:397: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "nswCps_lalonde_ps_CART_withRe78 = propensity_score_funct(nswCps_lalonde_subset,grid_search_cart1,nswCps_lalonde,False)\n",
    "nswPsid_lalonde_ps_CART_withRe78= propensity_score_funct(nswPsid_lalonde_subset,grid_search_cart2,nswPsid_lalonde,False)\n",
    "# ============ Dehejia & Wahba sub sample ============ # \n",
    "# predict propensity scores\n",
    "nswCps_dehWab_ps_CART_withRe78 = propensity_score_funct(nswCps_dehWab_subset,grid_search_cart3,nswCps_dehWab,True)\n",
    "nswPsid_dehWab_ps_CART_withRe78 = propensity_score_funct(nswPsid_dehWab_subset,grid_search_cart4,nswPsid_dehWab,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>treat</th>\n",
       "      <th>age</th>\n",
       "      <th>education*</th>\n",
       "      <th>black</th>\n",
       "      <th>hispanic</th>\n",
       "      <th>married</th>\n",
       "      <th>nodegree</th>\n",
       "      <th>re74</th>\n",
       "      <th>re75</th>\n",
       "      <th>re78</th>\n",
       "      <th>propensity_score</th>\n",
       "      <th>propensity_logit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9930</td>\n",
       "      <td>0.061947</td>\n",
       "      <td>-2.717529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3595</td>\n",
       "      <td>0.999251</td>\n",
       "      <td>7.196687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24909</td>\n",
       "      <td>0.999251</td>\n",
       "      <td>7.196687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>27</td>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7506</td>\n",
       "      <td>0.999251</td>\n",
       "      <td>7.196687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>33</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>289</td>\n",
       "      <td>0.999251</td>\n",
       "      <td>7.196687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2670</th>\n",
       "      <td>0.0</td>\n",
       "      <td>47</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44667</td>\n",
       "      <td>33837</td>\n",
       "      <td>38568</td>\n",
       "      <td>0.004071</td>\n",
       "      <td>-5.499897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2671</th>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>47022</td>\n",
       "      <td>67137</td>\n",
       "      <td>59109</td>\n",
       "      <td>0.004071</td>\n",
       "      <td>-5.499897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2672</th>\n",
       "      <td>0.0</td>\n",
       "      <td>47</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>48197</td>\n",
       "      <td>47968</td>\n",
       "      <td>55710</td>\n",
       "      <td>0.004071</td>\n",
       "      <td>-5.499897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2673</th>\n",
       "      <td>0.0</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>49228</td>\n",
       "      <td>44220</td>\n",
       "      <td>20540</td>\n",
       "      <td>0.004071</td>\n",
       "      <td>-5.499897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2674</th>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50940</td>\n",
       "      <td>55500</td>\n",
       "      <td>53198</td>\n",
       "      <td>0.004071</td>\n",
       "      <td>-5.499897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2675 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      treat  age  education*  black  hispanic  married  nodegree   re74  \\\n",
       "0       1.0   37          11    1.0       0.0      1.0       1.0      0   \n",
       "1       1.0   22           9    0.0       1.0      0.0       1.0      0   \n",
       "2       1.0   30          12    1.0       0.0      0.0       0.0      0   \n",
       "3       1.0   27          11    1.0       0.0      0.0       1.0      0   \n",
       "4       1.0   33           8    1.0       0.0      0.0       1.0      0   \n",
       "...     ...  ...         ...    ...       ...      ...       ...    ...   \n",
       "2670    0.0   47           8    0.0       0.0      1.0       1.0  44667   \n",
       "2671    0.0   32           8    0.0       0.0      1.0       1.0  47022   \n",
       "2672    0.0   47          10    0.0       0.0      1.0       1.0  48197   \n",
       "2673    0.0   54           0    0.0       1.0      1.0       1.0  49228   \n",
       "2674    0.0   40           8    0.0       0.0      1.0       1.0  50940   \n",
       "\n",
       "       re75   re78  propensity_score  propensity_logit  \n",
       "0         0   9930          0.061947         -2.717529  \n",
       "1         0   3595          0.999251          7.196687  \n",
       "2         0  24909          0.999251          7.196687  \n",
       "3         0   7506          0.999251          7.196687  \n",
       "4         0    289          0.999251          7.196687  \n",
       "...     ...    ...               ...               ...  \n",
       "2670  33837  38568          0.004071         -5.499897  \n",
       "2671  67137  59109          0.004071         -5.499897  \n",
       "2672  47968  55710          0.004071         -5.499897  \n",
       "2673  44220  20540          0.004071         -5.499897  \n",
       "2674  55500  53198          0.004071         -5.499897  \n",
       "\n",
       "[2675 rows x 12 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nswPsid_dehWab_ps_CART_withRe78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save unmatched CART datasets \n",
    "nswCps_lalonde_ps_CART_withRe78.to_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/cart/unmatched/nswCps_lalonde_ps_unmatched_CART_FS1.csv')\n",
    "nswPsid_lalonde_ps_CART_withRe78.to_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/cart/unmatched/nswPsid_lalonde_ps_unmatched_CART_FS1.csv')\n",
    "nswCps_dehWab_ps_CART_withRe78.to_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/cart/unmatched/nswCps_dehWab_ps_unmatched_CART_FS1.csv')\n",
    "nswPsid_dehWab_ps_CART_withRe78.to_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/cart/unmatched/nswPsid_dehWab_ps_unmatched_CART_FS1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model (3) ; Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load quasi-experimental dataset\n",
    "nswCps_lalonde =  pd.read_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/unmatched data/Quasi_NswCps_lalonde.csv')\n",
    "nswPsid_lalonde = pd.read_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/unmatched data/Quasi_NswPsid_lalonde.csv')\n",
    "nswCps_dehWab =   pd.read_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/unmatched data/Quasi_NswCps_dehWab.csv')\n",
    "nswPsid_dehWab =  pd.read_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/unmatched data/Quasi_NswPsid_dehWab.csv')\n",
    "\n",
    "nswCps_lalonde.columns = nswCps_lalonde.columns.str.strip() \n",
    "nswPsid_lalonde.columns = nswPsid_lalonde.columns.str.strip() \n",
    "nswCps_dehWab.columns = nswCps_dehWab.columns.str.strip() \n",
    "nswPsid_dehWab.columns = nswPsid_dehWab.columns.str.strip() \n",
    "\n",
    "# Store outcome variable prior to modelling\n",
    "nswCps_lalonde_re78 = nswCps_lalonde.re78\n",
    "nswPsid_lalonde_re78 = nswPsid_lalonde.re78\n",
    "nswCps_dehWab_re78 = nswCps_dehWab.re78\n",
    "nswPsid_dehWab_re78 = nswPsid_dehWab.re78\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Select covariates for use in CART model \"\"\"\n",
    "#sc = StandardScaler()\n",
    "# ======== Lalonde (1986) data ======== #\n",
    "\n",
    "nswCps_lalonde_subset = nswCps_lalonde[markov_blanket_specification_1]\n",
    "nswPsid_lalonde_subset = nswPsid_lalonde[markov_blanket_specification_2]\n",
    "\n",
    "nswCps_lalonde_target = nswCps_lalonde_subset['treat']\n",
    "nswCps_lalonde_features = nswCps_lalonde_subset.drop('treat', axis=1)\n",
    "nswPsid_lalonde_target = nswPsid_lalonde_subset['treat']\n",
    "nswPsid_lalonde_features = nswPsid_lalonde_subset.drop('treat', axis=1)\n",
    "\n",
    "# ======== Dehejia & Wahba (1986) sub sample ======== #\n",
    "\n",
    "nswCps_dehWab_subset = nswCps_dehWab[markov_blanket_specification_3]\n",
    "nswPsid_dehWab_subset = nswPsid_dehWab[markov_blanket_specification_4]\n",
    "\n",
    "nswCps_dehWab_target = nswCps_dehWab_subset['treat']\n",
    "nswCps_dehWab_features = nswCps_dehWab_subset.drop('treat', axis=1)\n",
    "nswPsid_dehWab_target = nswPsid_dehWab_subset['treat']\n",
    "nswPsid_dehWab_features = nswPsid_dehWab_subset.drop('treat', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest1_metrics = pd.DataFrame()\n",
    "forest2_metrics = pd.DataFrame()\n",
    "forest3_metrics = pd.DataFrame()\n",
    "forest4_metrics = pd.DataFrame()\n",
    "\n",
    "grid_search_forest1 = RandomForestClassifier(random_state=0,criterion = 'entropy',max_depth = 8,max_features = 'sqrt',min_samples_split = 2,n_estimators= 100)\n",
    "grid_search_forest2 = RandomForestClassifier(random_state=0,criterion = 'gini',max_depth = 8,max_features = 'sqrt',min_samples_split = 2,n_estimators= 100)\n",
    "grid_search_forest3 = RandomForestClassifier(random_state=0,criterion = 'entropy',max_depth = 8,max_features = 'log2',min_samples_split = 3,n_estimators= 500)\n",
    "grid_search_forest4 = RandomForestClassifier(random_state=0,criterion = 'gini',max_depth = 8,max_features = 'log2',min_samples_leaf = 2,n_estimators= 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_evaluation_SMOTEENN(grid_search_forest1,nswCps_lalonde_features,nswCps_lalonde_target,forest1_metrics) # cps\n",
    "kfold_evaluation_SMOTEENN(grid_search_forest2,nswPsid_lalonde_features,nswPsid_lalonde_target,forest2_metrics) # psid\n",
    "kfold_evaluation_SMOTEENN(grid_search_forest3,nswCps_dehWab_features,nswCps_dehWab_target,forest3_metrics) # cps\n",
    "kfold_evaluation_SMOTEENN(grid_search_forest4,nswPsid_dehWab_features,nswPsid_dehWab_target,forest4_metrics) # psid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest1_metrics['avg'] = np.mean(forest1_metrics,axis=1)\n",
    "forest2_metrics['avg'] = np.mean(forest2_metrics,axis=1)\n",
    "forest3_metrics['avg'] = np.mean(forest3_metrics,axis=1)\n",
    "forest4_metrics['avg'] = np.mean(forest4_metrics,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy:</th>\n",
       "      <td>0.840353</td>\n",
       "      <td>0.847702</td>\n",
       "      <td>0.821321</td>\n",
       "      <td>0.866039</td>\n",
       "      <td>0.920040</td>\n",
       "      <td>0.859091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision:</th>\n",
       "      <td>0.272152</td>\n",
       "      <td>0.387387</td>\n",
       "      <td>0.261438</td>\n",
       "      <td>0.314685</td>\n",
       "      <td>0.282609</td>\n",
       "      <td>0.303654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall:</th>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.762712</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>0.751073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE:</th>\n",
       "      <td>0.040516</td>\n",
       "      <td>0.026090</td>\n",
       "      <td>0.040516</td>\n",
       "      <td>0.034377</td>\n",
       "      <td>0.042677</td>\n",
       "      <td>0.036835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE:</th>\n",
       "      <td>0.040516</td>\n",
       "      <td>0.026090</td>\n",
       "      <td>0.040516</td>\n",
       "      <td>0.034377</td>\n",
       "      <td>0.042677</td>\n",
       "      <td>0.036835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R^2:</th>\n",
       "      <td>-1.241276</td>\n",
       "      <td>-0.443246</td>\n",
       "      <td>-1.278551</td>\n",
       "      <td>-0.933316</td>\n",
       "      <td>-1.399397</td>\n",
       "      <td>-1.059157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc:</th>\n",
       "      <td>0.840353</td>\n",
       "      <td>0.847702</td>\n",
       "      <td>0.821321</td>\n",
       "      <td>0.866039</td>\n",
       "      <td>0.920040</td>\n",
       "      <td>0.859091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc:</th>\n",
       "      <td>0.952116</td>\n",
       "      <td>0.952283</td>\n",
       "      <td>0.933788</td>\n",
       "      <td>0.969426</td>\n",
       "      <td>0.979028</td>\n",
       "      <td>0.957328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1:</th>\n",
       "      <td>0.394495</td>\n",
       "      <td>0.502924</td>\n",
       "      <td>0.377358</td>\n",
       "      <td>0.445545</td>\n",
       "      <td>0.427984</td>\n",
       "      <td>0.429661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log-loss:</th>\n",
       "      <td>0.108171</td>\n",
       "      <td>0.101750</td>\n",
       "      <td>0.132206</td>\n",
       "      <td>0.112388</td>\n",
       "      <td>0.123123</td>\n",
       "      <td>0.115528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4       avg\n",
       "Accuracy:   0.840353  0.847702  0.821321  0.866039  0.920040  0.859091\n",
       "Precision:  0.272152  0.387387  0.261438  0.314685  0.282609  0.303654\n",
       "Recall:     0.716667  0.716667  0.677966  0.762712  0.881356  0.751073\n",
       "MSE:        0.040516  0.026090  0.040516  0.034377  0.042677  0.036835\n",
       "MAE:        0.040516  0.026090  0.040516  0.034377  0.042677  0.036835\n",
       "R^2:       -1.241276 -0.443246 -1.278551 -0.933316 -1.399397 -1.059157\n",
       "auc:        0.840353  0.847702  0.821321  0.866039  0.920040  0.859091\n",
       "roc_auc:    0.952116  0.952283  0.933788  0.969426  0.979028  0.957328\n",
       "F1:         0.394495  0.502924  0.377358  0.445545  0.427984  0.429661\n",
       "log-loss:   0.108171  0.101750  0.132206  0.112388  0.123123  0.115528"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest1_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy:</th>\n",
       "      <td>0.833233</td>\n",
       "      <td>0.881928</td>\n",
       "      <td>0.867181</td>\n",
       "      <td>0.890154</td>\n",
       "      <td>0.924495</td>\n",
       "      <td>0.879398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision:</th>\n",
       "      <td>0.632353</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.602564</td>\n",
       "      <td>0.662162</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.649008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall:</th>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.796610</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.915254</td>\n",
       "      <td>0.811808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE:</th>\n",
       "      <td>0.075269</td>\n",
       "      <td>0.053763</td>\n",
       "      <td>0.077199</td>\n",
       "      <td>0.062837</td>\n",
       "      <td>0.068223</td>\n",
       "      <td>0.067458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE:</th>\n",
       "      <td>0.075269</td>\n",
       "      <td>0.053763</td>\n",
       "      <td>0.077199</td>\n",
       "      <td>0.062837</td>\n",
       "      <td>0.068223</td>\n",
       "      <td>0.067458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R^2:</th>\n",
       "      <td>0.215663</td>\n",
       "      <td>0.439759</td>\n",
       "      <td>0.184841</td>\n",
       "      <td>0.336499</td>\n",
       "      <td>0.279627</td>\n",
       "      <td>0.291278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc:</th>\n",
       "      <td>0.833233</td>\n",
       "      <td>0.881928</td>\n",
       "      <td>0.867181</td>\n",
       "      <td>0.890154</td>\n",
       "      <td>0.924495</td>\n",
       "      <td>0.879398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc:</th>\n",
       "      <td>0.924916</td>\n",
       "      <td>0.960609</td>\n",
       "      <td>0.930502</td>\n",
       "      <td>0.967820</td>\n",
       "      <td>0.972075</td>\n",
       "      <td>0.951184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1:</th>\n",
       "      <td>0.671875</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.686131</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.739726</td>\n",
       "      <td>0.719296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log-loss:</th>\n",
       "      <td>0.394292</td>\n",
       "      <td>0.222957</td>\n",
       "      <td>0.390829</td>\n",
       "      <td>0.185341</td>\n",
       "      <td>0.217657</td>\n",
       "      <td>0.282215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4       avg\n",
       "Accuracy:   0.833233  0.881928  0.867181  0.890154  0.924495  0.879398\n",
       "Precision:  0.632353  0.727273  0.602564  0.662162  0.620690  0.649008\n",
       "Recall:     0.716667  0.800000  0.796610  0.830508  0.915254  0.811808\n",
       "MSE:        0.075269  0.053763  0.077199  0.062837  0.068223  0.067458\n",
       "MAE:        0.075269  0.053763  0.077199  0.062837  0.068223  0.067458\n",
       "R^2:        0.215663  0.439759  0.184841  0.336499  0.279627  0.291278\n",
       "auc:        0.833233  0.881928  0.867181  0.890154  0.924495  0.879398\n",
       "roc_auc:    0.924916  0.960609  0.930502  0.967820  0.972075  0.951184\n",
       "F1:         0.671875  0.761905  0.686131  0.736842  0.739726  0.719296\n",
       "log-loss:   0.394292  0.222957  0.390829  0.185341  0.217657  0.282215"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest2_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy:</th>\n",
       "      <td>0.885640</td>\n",
       "      <td>0.896184</td>\n",
       "      <td>0.859080</td>\n",
       "      <td>0.898995</td>\n",
       "      <td>0.885013</td>\n",
       "      <td>0.884982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision:</th>\n",
       "      <td>0.420290</td>\n",
       "      <td>0.337079</td>\n",
       "      <td>0.421875</td>\n",
       "      <td>0.422535</td>\n",
       "      <td>0.397260</td>\n",
       "      <td>0.399808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall:</th>\n",
       "      <td>0.783784</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>0.783784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE:</th>\n",
       "      <td>0.014833</td>\n",
       "      <td>0.020396</td>\n",
       "      <td>0.014529</td>\n",
       "      <td>0.014838</td>\n",
       "      <td>0.016074</td>\n",
       "      <td>0.016134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE:</th>\n",
       "      <td>0.014833</td>\n",
       "      <td>0.020396</td>\n",
       "      <td>0.014529</td>\n",
       "      <td>0.014838</td>\n",
       "      <td>0.016074</td>\n",
       "      <td>0.016134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R^2:</th>\n",
       "      <td>-0.312302</td>\n",
       "      <td>-0.804415</td>\n",
       "      <td>-0.284967</td>\n",
       "      <td>-0.312307</td>\n",
       "      <td>-0.421666</td>\n",
       "      <td>-0.427131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc:</th>\n",
       "      <td>0.885640</td>\n",
       "      <td>0.896184</td>\n",
       "      <td>0.859080</td>\n",
       "      <td>0.898995</td>\n",
       "      <td>0.885013</td>\n",
       "      <td>0.884982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc:</th>\n",
       "      <td>0.981240</td>\n",
       "      <td>0.955480</td>\n",
       "      <td>0.986799</td>\n",
       "      <td>0.954816</td>\n",
       "      <td>0.989056</td>\n",
       "      <td>0.973478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1:</th>\n",
       "      <td>0.547170</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.534653</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.527273</td>\n",
       "      <td>0.528168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log-loss:</th>\n",
       "      <td>0.054616</td>\n",
       "      <td>0.078579</td>\n",
       "      <td>0.046260</td>\n",
       "      <td>0.064213</td>\n",
       "      <td>0.055769</td>\n",
       "      <td>0.059887</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4       avg\n",
       "Accuracy:   0.885640  0.896184  0.859080  0.898995  0.885013  0.884982\n",
       "Precision:  0.420290  0.337079  0.421875  0.422535  0.397260  0.399808\n",
       "Recall:     0.783784  0.810811  0.729730  0.810811  0.783784  0.783784\n",
       "MSE:        0.014833  0.020396  0.014529  0.014838  0.016074  0.016134\n",
       "MAE:        0.014833  0.020396  0.014529  0.014838  0.016074  0.016134\n",
       "R^2:       -0.312302 -0.804415 -0.284967 -0.312307 -0.421666 -0.427131\n",
       "auc:        0.885640  0.896184  0.859080  0.898995  0.885013  0.884982\n",
       "roc_auc:    0.981240  0.955480  0.986799  0.954816  0.989056  0.973478\n",
       "F1:         0.547170  0.476190  0.534653  0.555556  0.527273  0.528168\n",
       "log-loss:   0.054616  0.078579  0.046260  0.064213  0.055769  0.059887"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest3_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy:</th>\n",
       "      <td>0.927874</td>\n",
       "      <td>0.877836</td>\n",
       "      <td>0.946869</td>\n",
       "      <td>0.879301</td>\n",
       "      <td>0.929882</td>\n",
       "      <td>0.912352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision:</th>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.674419</td>\n",
       "      <td>0.573770</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.673469</td>\n",
       "      <td>0.620886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall:</th>\n",
       "      <td>0.891892</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.891892</td>\n",
       "      <td>0.864865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE:</th>\n",
       "      <td>0.041121</td>\n",
       "      <td>0.041121</td>\n",
       "      <td>0.052336</td>\n",
       "      <td>0.061682</td>\n",
       "      <td>0.037383</td>\n",
       "      <td>0.046729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE:</th>\n",
       "      <td>0.041121</td>\n",
       "      <td>0.041121</td>\n",
       "      <td>0.052336</td>\n",
       "      <td>0.061682</td>\n",
       "      <td>0.037383</td>\n",
       "      <td>0.046729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R^2:</th>\n",
       "      <td>0.361229</td>\n",
       "      <td>0.361229</td>\n",
       "      <td>0.187018</td>\n",
       "      <td>0.041843</td>\n",
       "      <td>0.419299</td>\n",
       "      <td>0.274124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc:</th>\n",
       "      <td>0.927874</td>\n",
       "      <td>0.877836</td>\n",
       "      <td>0.946869</td>\n",
       "      <td>0.879301</td>\n",
       "      <td>0.929882</td>\n",
       "      <td>0.912352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc:</th>\n",
       "      <td>0.984099</td>\n",
       "      <td>0.960708</td>\n",
       "      <td>0.988549</td>\n",
       "      <td>0.917128</td>\n",
       "      <td>0.988549</td>\n",
       "      <td>0.967806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1:</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.645161</td>\n",
       "      <td>0.767442</td>\n",
       "      <td>0.720378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log-loss:</th>\n",
       "      <td>0.103740</td>\n",
       "      <td>0.151643</td>\n",
       "      <td>0.131280</td>\n",
       "      <td>0.355930</td>\n",
       "      <td>0.108682</td>\n",
       "      <td>0.170255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4       avg\n",
       "Accuracy:   0.927874  0.877836  0.946869  0.879301  0.929882  0.912352\n",
       "Precision:  0.647059  0.674419  0.573770  0.535714  0.673469  0.620886\n",
       "Recall:     0.891892  0.783784  0.945946  0.810811  0.891892  0.864865\n",
       "MSE:        0.041121  0.041121  0.052336  0.061682  0.037383  0.046729\n",
       "MAE:        0.041121  0.041121  0.052336  0.061682  0.037383  0.046729\n",
       "R^2:        0.361229  0.361229  0.187018  0.041843  0.419299  0.274124\n",
       "auc:        0.927874  0.877836  0.946869  0.879301  0.929882  0.912352\n",
       "roc_auc:    0.984099  0.960708  0.988549  0.917128  0.988549  0.967806\n",
       "F1:         0.750000  0.725000  0.714286  0.645161  0.767442  0.720378\n",
       "log-loss:   0.103740  0.151643  0.131280  0.355930  0.108682  0.170255"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest4_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/pandas/core/arraylike.py:397: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/pandas/core/arraylike.py:397: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/pandas/core/arraylike.py:397: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/pandas/core/arraylike.py:397: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#  Lalonde Subsample \n",
    "nswCps_lalonde_ps_FOREST_withRe78 = propensity_score_funct(nswCps_lalonde_subset,grid_search_forest1,nswCps_lalonde,False)\n",
    "nswPsid_lalonde_ps_FOREST_withRe78= propensity_score_funct(nswPsid_lalonde_subset,grid_search_forest2,nswPsid_lalonde,False)\n",
    "#  Dehejia & Wahba sub sample \n",
    "nswCps_dehWab_ps_FOREST_withRe78 = propensity_score_funct(nswCps_dehWab_subset,grid_search_forest3,nswCps_dehWab,True)\n",
    "nswPsid_dehWab_ps_FOREST_withRe78 = propensity_score_funct(nswPsid_dehWab_subset,grid_search_forest4,nswPsid_dehWab,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>treat</th>\n",
       "      <th>age</th>\n",
       "      <th>education*</th>\n",
       "      <th>black</th>\n",
       "      <th>hispanic</th>\n",
       "      <th>married</th>\n",
       "      <th>nodegree</th>\n",
       "      <th>re74</th>\n",
       "      <th>re75</th>\n",
       "      <th>re78</th>\n",
       "      <th>propensity_score</th>\n",
       "      <th>propensity_logit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9930</td>\n",
       "      <td>0.935999</td>\n",
       "      <td>2.682723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3595</td>\n",
       "      <td>0.351583</td>\n",
       "      <td>-0.612088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24909</td>\n",
       "      <td>0.988211</td>\n",
       "      <td>4.428687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>27</td>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7506</td>\n",
       "      <td>0.996804</td>\n",
       "      <td>5.742562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>33</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>289</td>\n",
       "      <td>0.996507</td>\n",
       "      <td>5.653493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16172</th>\n",
       "      <td>0.0</td>\n",
       "      <td>22</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3975</td>\n",
       "      <td>6801</td>\n",
       "      <td>2757</td>\n",
       "      <td>0.624854</td>\n",
       "      <td>0.510203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16173</th>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1445</td>\n",
       "      <td>11832</td>\n",
       "      <td>6895</td>\n",
       "      <td>0.013650</td>\n",
       "      <td>-4.280268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16174</th>\n",
       "      <td>0.0</td>\n",
       "      <td>37</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1733</td>\n",
       "      <td>1559</td>\n",
       "      <td>4221</td>\n",
       "      <td>0.029262</td>\n",
       "      <td>-3.501750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16175</th>\n",
       "      <td>0.0</td>\n",
       "      <td>47</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16914</td>\n",
       "      <td>11384</td>\n",
       "      <td>13671</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16176</th>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13628</td>\n",
       "      <td>13144</td>\n",
       "      <td>7979</td>\n",
       "      <td>0.001638</td>\n",
       "      <td>-6.412519</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16177 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       treat  age  education*  black  hispanic  married  nodegree   re74  \\\n",
       "0        1.0   37          11    1.0       0.0      1.0       1.0      0   \n",
       "1        1.0   22           9    0.0       1.0      0.0       1.0      0   \n",
       "2        1.0   30          12    1.0       0.0      0.0       0.0      0   \n",
       "3        1.0   27          11    1.0       0.0      0.0       1.0      0   \n",
       "4        1.0   33           8    1.0       0.0      0.0       1.0      0   \n",
       "...      ...  ...         ...    ...       ...      ...       ...    ...   \n",
       "16172    0.0   22          12    1.0       0.0      0.0       0.0   3975   \n",
       "16173    0.0   20          12    1.0       0.0      1.0       0.0   1445   \n",
       "16174    0.0   37          12    0.0       0.0      0.0       0.0   1733   \n",
       "16175    0.0   47           9    0.0       0.0      1.0       1.0  16914   \n",
       "16176    0.0   40          10    0.0       0.0      0.0       1.0  13628   \n",
       "\n",
       "        re75   re78  propensity_score  propensity_logit  \n",
       "0          0   9930          0.935999          2.682723  \n",
       "1          0   3595          0.351583         -0.612088  \n",
       "2          0  24909          0.988211          4.428687  \n",
       "3          0   7506          0.996804          5.742562  \n",
       "4          0    289          0.996507          5.653493  \n",
       "...      ...    ...               ...               ...  \n",
       "16172   6801   2757          0.624854          0.510203  \n",
       "16173  11832   6895          0.013650         -4.280268  \n",
       "16174   1559   4221          0.029262         -3.501750  \n",
       "16175  11384  13671          0.000000              -inf  \n",
       "16176  13144   7979          0.001638         -6.412519  \n",
       "\n",
       "[16177 rows x 12 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nswCps_dehWab_ps_FOREST_withRe78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save unmatched logit datasets \n",
    "nswCps_lalonde_ps_FOREST_withRe78.to_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/forest/unmatched/nswCps_lalonde_ps_unmatched_FOREST_FS1.csv')\n",
    "nswPsid_lalonde_ps_FOREST_withRe78.to_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/forest/unmatched/nswPsid_lalonde_ps_unmatched_FOREST_FS1.csv')\n",
    "nswCps_dehWab_ps_FOREST_withRe78.to_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/forest/unmatched/nswCps_dehWab_ps_unmatched_FOREST_FS1.csv')\n",
    "nswPsid_dehWab_ps_FOREST_withRe78.to_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/forest/unmatched/nswPsid_dehWab_ps_unmatched_FOREST_FS1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model (4) ; Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Select covariates for use in BOOST model \"\"\"\n",
    "#sc = StandardScaler()\n",
    "# ======== Lalonde (1986) data ======== #\n",
    "\n",
    "# Apply selection\n",
    "nswCps_lalonde_subset = nswCps_lalonde[markov_blanket_specification_1]\n",
    "nswPsid_lalonde_subset = nswPsid_lalonde[markov_blanket_specification_2]\n",
    "\n",
    "nswCps_lalonde_target = nswCps_lalonde_subset['treat']\n",
    "nswCps_lalonde_features = nswCps_lalonde_subset.drop('treat', axis=1)\n",
    "nswPsid_lalonde_target = nswPsid_lalonde_subset['treat']\n",
    "nswPsid_lalonde_features = nswPsid_lalonde_subset.drop('treat', axis=1)\n",
    "\n",
    "# ======== Dehejia & Wahba (1986) sub sample ======== #\n",
    "\n",
    "# Apply selection\n",
    "nswCps_dehWab_subset = nswCps_dehWab[markov_blanket_specification_3]\n",
    "nswPsid_dehWab_subset = nswPsid_dehWab[markov_blanket_specification_4]\n",
    "\n",
    "nswCps_dehWab_target = nswCps_dehWab_subset['treat']\n",
    "nswCps_dehWab_features = nswCps_dehWab_subset.drop('treat', axis=1)\n",
    "nswPsid_dehWab_target = nswPsid_dehWab_subset['treat']\n",
    "nswPsid_dehWab_features = nswPsid_dehWab_subset.drop('treat', axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframes to store results \n",
    "boost1_metrics = pd.DataFrame()\n",
    "boost2_metrics = pd.DataFrame()\n",
    "boost3_metrics = pd.DataFrame()\n",
    "boost4_metrics = pd.DataFrame()\n",
    "    \n",
    "\n",
    "# models \n",
    "grid_search_boost1 = XGBClassifier(objective= 'binary:logistic',\n",
    "                                    seed=0,\n",
    "                                    booster = 'dart',\n",
    "                                    colsample_bytree = 0.3,\n",
    "                                    gamma = 0.5,\n",
    "                                    learning_rate = 0.05,\n",
    "                                    max_depth = None,\n",
    "                                    min_child_weight = 2,\n",
    "                                    subsample = 0.5,\n",
    "                                    nthread=4) \n",
    "grid_search_boost2 = XGBClassifier(objective= 'binary:logistic',\n",
    "                                    seed=0,\n",
    "                                    booster = 'dart',\n",
    "                                    colsample_bytree = 0.3,\n",
    "                                    gamma = 0.5,\n",
    "                                    learning_rate = 0.05,\n",
    "                                    max_depth = None,\n",
    "                                    min_child_weight = 2,\n",
    "                                    subsample = 0.5,\n",
    "                                    nthread=4) \n",
    "grid_search_boost3 = XGBClassifier(objective= 'binary:logistic',\n",
    "                                    seed=0,\n",
    "                                    booster = 'gbtree',\n",
    "                                    colsample_bytree = 0.3,\n",
    "                                    gamma = 1,\n",
    "                                    learning_rate = 0.05,\n",
    "                                    min_child_weight = 2,\n",
    "                                    subsample = 0.5,\n",
    "                                    nthread=4) \n",
    "grid_search_boost4 = XGBClassifier(objective= 'binary:logistic',\n",
    "                                    seed=0,\n",
    "                                    booster = 'gbtree',\n",
    "                                    colsample_bytree = 0.3,\n",
    "                                    gamma = 0.5,\n",
    "                                    learning_rate = 0.05,\n",
    "                                    min_child_weight = 2,\n",
    "                                    subsample = 0.5,\n",
    "                                    nthread=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_evaluation_SMOTEENN(grid_search_boost1,nswCps_lalonde_features,nswCps_lalonde_target,boost1_metrics) # cps\n",
    "kfold_evaluation_SMOTEENN(grid_search_boost2,nswPsid_lalonde_features,nswPsid_lalonde_target,boost2_metrics) # psid\n",
    "kfold_evaluation_SMOTEENN(grid_search_boost3,nswCps_dehWab_features,nswCps_dehWab_target,boost3_metrics) # cps\n",
    "kfold_evaluation_SMOTEENN(grid_search_boost4,nswPsid_dehWab_features,nswPsid_dehWab_target,boost4_metrics) # psid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost1_metrics['avg'] = np.mean(boost1_metrics,axis=1)\n",
    "boost2_metrics['avg'] = np.mean(boost2_metrics,axis=1)\n",
    "boost3_metrics['avg'] = np.mean(boost3_metrics,axis=1)\n",
    "boost4_metrics['avg'] = np.mean(boost4_metrics,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy:</th>\n",
       "      <td>0.854503</td>\n",
       "      <td>0.885710</td>\n",
       "      <td>0.837939</td>\n",
       "      <td>0.894535</td>\n",
       "      <td>0.930643</td>\n",
       "      <td>0.880666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision:</th>\n",
       "      <td>0.141593</td>\n",
       "      <td>0.201613</td>\n",
       "      <td>0.121693</td>\n",
       "      <td>0.174658</td>\n",
       "      <td>0.166172</td>\n",
       "      <td>0.161146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall:</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.779661</td>\n",
       "      <td>0.864407</td>\n",
       "      <td>0.949153</td>\n",
       "      <td>0.845311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE:</th>\n",
       "      <td>0.093002</td>\n",
       "      <td>0.063843</td>\n",
       "      <td>0.105893</td>\n",
       "      <td>0.076427</td>\n",
       "      <td>0.087197</td>\n",
       "      <td>0.085272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE:</th>\n",
       "      <td>0.093002</td>\n",
       "      <td>0.063843</td>\n",
       "      <td>0.105893</td>\n",
       "      <td>0.076427</td>\n",
       "      <td>0.087197</td>\n",
       "      <td>0.085272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R^2:</th>\n",
       "      <td>-4.144747</td>\n",
       "      <td>-2.531707</td>\n",
       "      <td>-4.955304</td>\n",
       "      <td>-3.298176</td>\n",
       "      <td>-3.902365</td>\n",
       "      <td>-3.766460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc:</th>\n",
       "      <td>0.854503</td>\n",
       "      <td>0.885710</td>\n",
       "      <td>0.837939</td>\n",
       "      <td>0.894535</td>\n",
       "      <td>0.930643</td>\n",
       "      <td>0.880666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc:</th>\n",
       "      <td>0.945708</td>\n",
       "      <td>0.954422</td>\n",
       "      <td>0.939465</td>\n",
       "      <td>0.960425</td>\n",
       "      <td>0.976341</td>\n",
       "      <td>0.955272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1:</th>\n",
       "      <td>0.240602</td>\n",
       "      <td>0.324675</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.290598</td>\n",
       "      <td>0.282828</td>\n",
       "      <td>0.269846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log-loss:</th>\n",
       "      <td>0.220633</td>\n",
       "      <td>0.192769</td>\n",
       "      <td>0.222491</td>\n",
       "      <td>0.210346</td>\n",
       "      <td>0.212000</td>\n",
       "      <td>0.211648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4       avg\n",
       "Accuracy:   0.854503  0.885710  0.837939  0.894535  0.930643  0.880666\n",
       "Precision:  0.141593  0.201613  0.121693  0.174658  0.166172  0.161146\n",
       "Recall:     0.800000  0.833333  0.779661  0.864407  0.949153  0.845311\n",
       "MSE:        0.093002  0.063843  0.105893  0.076427  0.087197  0.085272\n",
       "MAE:        0.093002  0.063843  0.105893  0.076427  0.087197  0.085272\n",
       "R^2:       -4.144747 -2.531707 -4.955304 -3.298176 -3.902365 -3.766460\n",
       "auc:        0.854503  0.885710  0.837939  0.894535  0.930643  0.880666\n",
       "roc_auc:    0.945708  0.954422  0.939465  0.960425  0.976341  0.955272\n",
       "F1:         0.240602  0.324675  0.210526  0.290598  0.282828  0.269846\n",
       "log-loss:   0.220633  0.192769  0.222491  0.210346  0.212000  0.211648"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boost1_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy:</th>\n",
       "      <td>0.880823</td>\n",
       "      <td>0.915562</td>\n",
       "      <td>0.876098</td>\n",
       "      <td>0.897624</td>\n",
       "      <td>0.931404</td>\n",
       "      <td>0.900302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision:</th>\n",
       "      <td>0.536842</td>\n",
       "      <td>0.670886</td>\n",
       "      <td>0.556818</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.565657</td>\n",
       "      <td>0.597620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall:</th>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.847458</td>\n",
       "      <td>0.949153</td>\n",
       "      <td>0.872090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE:</th>\n",
       "      <td>0.094982</td>\n",
       "      <td>0.059140</td>\n",
       "      <td>0.087971</td>\n",
       "      <td>0.062837</td>\n",
       "      <td>0.082585</td>\n",
       "      <td>0.077503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE:</th>\n",
       "      <td>0.094982</td>\n",
       "      <td>0.059140</td>\n",
       "      <td>0.087971</td>\n",
       "      <td>0.062837</td>\n",
       "      <td>0.082585</td>\n",
       "      <td>0.077503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R^2:</th>\n",
       "      <td>0.010241</td>\n",
       "      <td>0.383735</td>\n",
       "      <td>0.071098</td>\n",
       "      <td>0.336499</td>\n",
       "      <td>0.127970</td>\n",
       "      <td>0.185908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc:</th>\n",
       "      <td>0.880823</td>\n",
       "      <td>0.915562</td>\n",
       "      <td>0.876098</td>\n",
       "      <td>0.897624</td>\n",
       "      <td>0.931404</td>\n",
       "      <td>0.900302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc:</th>\n",
       "      <td>0.944478</td>\n",
       "      <td>0.969093</td>\n",
       "      <td>0.936628</td>\n",
       "      <td>0.967701</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.957556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1:</th>\n",
       "      <td>0.658065</td>\n",
       "      <td>0.762590</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.708861</td>\n",
       "      <td>0.707385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log-loss:</th>\n",
       "      <td>0.229203</td>\n",
       "      <td>0.184353</td>\n",
       "      <td>0.241779</td>\n",
       "      <td>0.198972</td>\n",
       "      <td>0.239462</td>\n",
       "      <td>0.218754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4       avg\n",
       "Accuracy:   0.880823  0.915562  0.876098  0.897624  0.931404  0.900302\n",
       "Precision:  0.536842  0.670886  0.556818  0.657895  0.565657  0.597620\n",
       "Recall:     0.850000  0.883333  0.830508  0.847458  0.949153  0.872090\n",
       "MSE:        0.094982  0.059140  0.087971  0.062837  0.082585  0.077503\n",
       "MAE:        0.094982  0.059140  0.087971  0.062837  0.082585  0.077503\n",
       "R^2:        0.010241  0.383735  0.071098  0.336499  0.127970  0.185908\n",
       "auc:        0.880823  0.915562  0.876098  0.897624  0.931404  0.900302\n",
       "roc_auc:    0.944478  0.969093  0.936628  0.967701  0.969880  0.957556\n",
       "F1:         0.658065  0.762590  0.666667  0.740741  0.708861  0.707385\n",
       "log-loss:   0.229203  0.184353  0.241779  0.198972  0.239462  0.218754"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boost2_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy:</th>\n",
       "      <td>0.906259</td>\n",
       "      <td>0.885399</td>\n",
       "      <td>0.881729</td>\n",
       "      <td>0.916485</td>\n",
       "      <td>0.943825</td>\n",
       "      <td>0.906739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision:</th>\n",
       "      <td>0.276786</td>\n",
       "      <td>0.189873</td>\n",
       "      <td>0.308511</td>\n",
       "      <td>0.238806</td>\n",
       "      <td>0.253731</td>\n",
       "      <td>0.253541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall:</th>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.843243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE:</th>\n",
       "      <td>0.026885</td>\n",
       "      <td>0.041718</td>\n",
       "      <td>0.022566</td>\n",
       "      <td>0.033076</td>\n",
       "      <td>0.031839</td>\n",
       "      <td>0.031217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE:</th>\n",
       "      <td>0.026885</td>\n",
       "      <td>0.041718</td>\n",
       "      <td>0.022566</td>\n",
       "      <td>0.033076</td>\n",
       "      <td>0.031839</td>\n",
       "      <td>0.031217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R^2:</th>\n",
       "      <td>-1.378547</td>\n",
       "      <td>-2.690849</td>\n",
       "      <td>-0.995800</td>\n",
       "      <td>-1.925350</td>\n",
       "      <td>-1.815991</td>\n",
       "      <td>-1.761308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc:</th>\n",
       "      <td>0.906259</td>\n",
       "      <td>0.885399</td>\n",
       "      <td>0.881729</td>\n",
       "      <td>0.916485</td>\n",
       "      <td>0.943825</td>\n",
       "      <td>0.906739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc:</th>\n",
       "      <td>0.978317</td>\n",
       "      <td>0.951129</td>\n",
       "      <td>0.982358</td>\n",
       "      <td>0.955707</td>\n",
       "      <td>0.989875</td>\n",
       "      <td>0.971477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1:</th>\n",
       "      <td>0.416107</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.442748</td>\n",
       "      <td>0.374269</td>\n",
       "      <td>0.397661</td>\n",
       "      <td>0.387696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log-loss:</th>\n",
       "      <td>0.088943</td>\n",
       "      <td>0.104351</td>\n",
       "      <td>0.081000</td>\n",
       "      <td>0.100260</td>\n",
       "      <td>0.096278</td>\n",
       "      <td>0.094166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4       avg\n",
       "Accuracy:   0.906259  0.885399  0.881729  0.916485  0.943825  0.906739\n",
       "Precision:  0.276786  0.189873  0.308511  0.238806  0.253731  0.253541\n",
       "Recall:     0.837838  0.810811  0.783784  0.864865  0.918919  0.843243\n",
       "MSE:        0.026885  0.041718  0.022566  0.033076  0.031839  0.031217\n",
       "MAE:        0.026885  0.041718  0.022566  0.033076  0.031839  0.031217\n",
       "R^2:       -1.378547 -2.690849 -0.995800 -1.925350 -1.815991 -1.761308\n",
       "auc:        0.906259  0.885399  0.881729  0.916485  0.943825  0.906739\n",
       "roc_auc:    0.978317  0.951129  0.982358  0.955707  0.989875  0.971477\n",
       "F1:         0.416107  0.307692  0.442748  0.374269  0.397661  0.387696\n",
       "log-loss:   0.088943  0.104351  0.081000  0.100260  0.096278  0.094166"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boost3_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy:</th>\n",
       "      <td>0.934820</td>\n",
       "      <td>0.887794</td>\n",
       "      <td>0.935824</td>\n",
       "      <td>0.873195</td>\n",
       "      <td>0.939840</td>\n",
       "      <td>0.914295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision:</th>\n",
       "      <td>0.479452</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.486111</td>\n",
       "      <td>0.351648</td>\n",
       "      <td>0.514706</td>\n",
       "      <td>0.466383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall:</th>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.908108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE:</th>\n",
       "      <td>0.074766</td>\n",
       "      <td>0.069159</td>\n",
       "      <td>0.072897</td>\n",
       "      <td>0.119626</td>\n",
       "      <td>0.065421</td>\n",
       "      <td>0.080374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE:</th>\n",
       "      <td>0.074766</td>\n",
       "      <td>0.069159</td>\n",
       "      <td>0.072897</td>\n",
       "      <td>0.119626</td>\n",
       "      <td>0.065421</td>\n",
       "      <td>0.080374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R^2:</th>\n",
       "      <td>-0.161402</td>\n",
       "      <td>-0.074297</td>\n",
       "      <td>-0.132367</td>\n",
       "      <td>-0.858244</td>\n",
       "      <td>-0.016227</td>\n",
       "      <td>-0.248508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc:</th>\n",
       "      <td>0.934820</td>\n",
       "      <td>0.887794</td>\n",
       "      <td>0.935824</td>\n",
       "      <td>0.873195</td>\n",
       "      <td>0.939840</td>\n",
       "      <td>0.914295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc:</th>\n",
       "      <td>0.984641</td>\n",
       "      <td>0.959541</td>\n",
       "      <td>0.983773</td>\n",
       "      <td>0.922148</td>\n",
       "      <td>0.983149</td>\n",
       "      <td>0.966650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1:</th>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.626263</td>\n",
       "      <td>0.642202</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.614299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log-loss:</th>\n",
       "      <td>0.176870</td>\n",
       "      <td>0.174284</td>\n",
       "      <td>0.187314</td>\n",
       "      <td>0.259563</td>\n",
       "      <td>0.173311</td>\n",
       "      <td>0.194268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4       avg\n",
       "Accuracy:   0.934820  0.887794  0.935824  0.873195  0.939840  0.914295\n",
       "Precision:  0.479452  0.500000  0.486111  0.351648  0.514706  0.466383\n",
       "Recall:     0.945946  0.837838  0.945946  0.864865  0.945946  0.908108\n",
       "MSE:        0.074766  0.069159  0.072897  0.119626  0.065421  0.080374\n",
       "MAE:        0.074766  0.069159  0.072897  0.119626  0.065421  0.080374\n",
       "R^2:       -0.161402 -0.074297 -0.132367 -0.858244 -0.016227 -0.248508\n",
       "auc:        0.934820  0.887794  0.935824  0.873195  0.939840  0.914295\n",
       "roc_auc:    0.984641  0.959541  0.983773  0.922148  0.983149  0.966650\n",
       "F1:         0.636364  0.626263  0.642202  0.500000  0.666667  0.614299\n",
       "log-loss:   0.176870  0.174284  0.187314  0.259563  0.173311  0.194268"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boost4_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/pandas/core/arraylike.py:397: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/pandas/core/arraylike.py:397: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#  Lalonde Subsample \n",
    "nswCps_lalonde_ps_BOOST_withRe78 = propensity_score_funct(nswCps_lalonde_subset,grid_search_boost1,nswCps_lalonde,False)\n",
    "nswPsid_lalonde_ps_BOOST_withRe78= propensity_score_funct(nswPsid_lalonde_subset,grid_search_boost2,nswPsid_lalonde,False)\n",
    "#  Dehejia & Wahba sub sample \n",
    "nswCps_dehWab_ps_BOOST_withRe78 = propensity_score_funct(nswCps_dehWab_subset,grid_search_forest3,nswCps_dehWab,True)\n",
    "nswPsid_dehWab_ps_BOOST_withRe78 = propensity_score_funct(nswPsid_dehWab_subset,grid_search_forest4,nswPsid_dehWab,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save unmatched boost datasets \n",
    "nswCps_lalonde_ps_BOOST_withRe78.to_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/boost/unmatched/nswCps_lalonde_ps_unmatched_BOOST_FS1.csv')\n",
    "nswPsid_lalonde_ps_BOOST_withRe78.to_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/boost/unmatched/nswPsid_lalonde_ps_unmatched_BOOST_FS1.csv')\n",
    "nswCps_dehWab_ps_BOOST_withRe78.to_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/boost/unmatched/nswCps_dehWab_ps_unmatched_BOOST_FS1.csv')\n",
    "nswPsid_dehWab_ps_BOOST_withRe78.to_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/boost/unmatched/nswPsid_dehWab_ps_unmatched_BOOST_FS1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>treat</th>\n",
       "      <th>age</th>\n",
       "      <th>education*</th>\n",
       "      <th>black</th>\n",
       "      <th>hispanic</th>\n",
       "      <th>married</th>\n",
       "      <th>nodegree</th>\n",
       "      <th>re75</th>\n",
       "      <th>re78</th>\n",
       "      <th>propensity_score</th>\n",
       "      <th>propensity_logit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9930</td>\n",
       "      <td>0.524682</td>\n",
       "      <td>0.098808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3595</td>\n",
       "      <td>0.925366</td>\n",
       "      <td>2.517594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24909</td>\n",
       "      <td>0.649052</td>\n",
       "      <td>0.614874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>27</td>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7506</td>\n",
       "      <td>0.917422</td>\n",
       "      <td>2.407825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>33</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>289</td>\n",
       "      <td>0.751119</td>\n",
       "      <td>1.104587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2782</th>\n",
       "      <td>0.0</td>\n",
       "      <td>47</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33837</td>\n",
       "      <td>38568</td>\n",
       "      <td>0.063609</td>\n",
       "      <td>-2.689278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2783</th>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67137</td>\n",
       "      <td>59109</td>\n",
       "      <td>0.136257</td>\n",
       "      <td>-1.846732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2784</th>\n",
       "      <td>0.0</td>\n",
       "      <td>47</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>47968</td>\n",
       "      <td>55710</td>\n",
       "      <td>0.090488</td>\n",
       "      <td>-2.307686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2785</th>\n",
       "      <td>0.0</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44220</td>\n",
       "      <td>20540</td>\n",
       "      <td>0.034184</td>\n",
       "      <td>-3.341218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2786</th>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55500</td>\n",
       "      <td>53198</td>\n",
       "      <td>0.106333</td>\n",
       "      <td>-2.128757</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2787 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      treat  age  education*  black  hispanic  married  nodegree   re75  \\\n",
       "0       1.0   37          11    1.0       0.0      1.0       1.0      0   \n",
       "1       1.0   22           9    0.0       1.0      0.0       1.0      0   \n",
       "2       1.0   30          12    1.0       0.0      0.0       0.0      0   \n",
       "3       1.0   27          11    1.0       0.0      0.0       1.0      0   \n",
       "4       1.0   33           8    1.0       0.0      0.0       1.0      0   \n",
       "...     ...  ...         ...    ...       ...      ...       ...    ...   \n",
       "2782    0.0   47           8    0.0       0.0      1.0       1.0  33837   \n",
       "2783    0.0   32           8    0.0       0.0      1.0       1.0  67137   \n",
       "2784    0.0   47          10    0.0       0.0      1.0       1.0  47968   \n",
       "2785    0.0   54           0    0.0       1.0      1.0       1.0  44220   \n",
       "2786    0.0   40           8    0.0       0.0      1.0       1.0  55500   \n",
       "\n",
       "       re78  propensity_score  propensity_logit  \n",
       "0      9930          0.524682          0.098808  \n",
       "1      3595          0.925366          2.517594  \n",
       "2     24909          0.649052          0.614874  \n",
       "3      7506          0.917422          2.407825  \n",
       "4       289          0.751119          1.104587  \n",
       "...     ...               ...               ...  \n",
       "2782  38568          0.063609         -2.689278  \n",
       "2783  59109          0.136257         -1.846732  \n",
       "2784  55710          0.090488         -2.307686  \n",
       "2785  20540          0.034184         -3.341218  \n",
       "2786  53198          0.106333         -2.128757  \n",
       "\n",
       "[2787 rows x 11 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nswPsid_lalonde_ps_BOOST_withRe78"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model (5) - artifical neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import  DataLoader,SubsetRandomSampler \n",
    "from sklearn.preprocessing import StandardScaler , MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Import standard libraries \n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import random \n",
    "import numpy.random as rand\n",
    "from random import randrange\n",
    "import seaborn as sns\n",
    "from scipy import stats \n",
    "from psmpy.functions import cohenD\n",
    "from psmpy.plotting import *\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from imblearn.combine import SMOTEENN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "annSpecification_1 = ['treat','ageboxcox','education*','black','married','nodegree','re75'] # CPS Lalonde sample \n",
    "annSpecification_2 = ['treat','ageboxcox','education*','married','nodegree','re75']# PSID Lalonde sample  \n",
    "annSpecification_3 = ['treat','ageboxcox','education*','black','married','re74','re75'] # CPS Dehwab sample\n",
    "annSpecification_4 = ['treat','ageboxcox','education*','married','nodegree','re74','re75'] # PSID Dehwab and wahba sample \n",
    "nsw_continuos_vars1 = ['ageboxcox','re75'] \n",
    "nsw_continuos_vars2 = ['ageboxcox','re75'] \n",
    "nswre74_continuos_vars1 = ['ageboxcox','re75',] \n",
    "nswre74_continuos_vars2 = ['ageboxcox','re74','re75',] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample=SMOTEENN(random_state=0)\n",
    "sm = SMOTE(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load quasi-experimental dataset\n",
    "\n",
    "nswCps_lalonde =  pd.read_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/unmatched data/Quasi_NswCps_lalonde.csv')\n",
    "nswPsid_lalonde = pd.read_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/unmatched data/Quasi_NswPsid_lalonde.csv')\n",
    "nswCps_dehWab =   pd.read_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/unmatched data/Quasi_NswCps_dehWab.csv')\n",
    "nswPsid_dehWab =  pd.read_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/unmatched data/Quasi_NswPsid_dehWab.csv')\n",
    "\n",
    "nswCps_lalonde.columns = nswCps_lalonde.columns.str.strip() \n",
    "nswPsid_lalonde.columns = nswPsid_lalonde.columns.str.strip() \n",
    "nswCps_dehWab.columns = nswCps_dehWab.columns.str.strip() \n",
    "nswPsid_dehWab.columns = nswPsid_dehWab.columns.str.strip() \n",
    "\n",
    "# Store outcome variable prior to modelling\n",
    "nswCps_lalonde_re78 = nswCps_lalonde.re78\n",
    "nswPsid_lalonde_re78 = nswPsid_lalonde.re78\n",
    "nswCps_dehWab_re78 = nswCps_dehWab.re78\n",
    "nswPsid_dehWab_re78 = nswPsid_dehWab.re78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sp/c9zy568j6s94zqs20ynd8pz00000gn/T/ipykernel_30126/3854445176.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  nswCps_lalonde_subset[nsw_continuos_vars1] = sc.fit_transform(nswCps_lalonde_subset[nsw_continuos_vars1])\n",
      "/var/folders/sp/c9zy568j6s94zqs20ynd8pz00000gn/T/ipykernel_30126/3854445176.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  nswCps_dehWab_subset[nswre74_continuos_vars1] = sc.fit_transform(nswCps_dehWab_subset[nswre74_continuos_vars1])\n",
      "/var/folders/sp/c9zy568j6s94zqs20ynd8pz00000gn/T/ipykernel_30126/3854445176.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  nswPsid_dehWab_subset[nswre74_continuos_vars2] = sc.fit_transform(nswPsid_dehWab_subset[nswre74_continuos_vars2])\n"
     ]
    }
   ],
   "source": [
    "# ANN data pre-processing\n",
    "# Select covariates \n",
    "# ======== Lalonde (1986) data ======== #\n",
    "\n",
    "\n",
    "nswCps_lalonde_subset = nswCps_lalonde[annSpecification_1]\n",
    "nswPsid_lalonde_subset = nswPsid_lalonde[annSpecification_2]\n",
    "\n",
    "# Z-score normalise continous variables\n",
    "sc = MinMaxScaler()\n",
    "\n",
    "nswCps_lalonde_subset[nsw_continuos_vars1] = sc.fit_transform(nswCps_lalonde_subset[nsw_continuos_vars1])\n",
    "#nswPsid_lalonde_subset[nsw_continuos_vars2] = sc.fit_transform(nswPsid_lalonde_subset[nsw_continuos_vars2])\n",
    "\n",
    "nswCps_lalonde_subset = nswCps_lalonde_subset.sample(frac=1).reset_index(drop=True)\n",
    "nswPsid_lalonde_subset = nswPsid_lalonde_subset.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Create targets and featues\n",
    "nswCps_lalonde_target = nswCps_lalonde_subset['treat']\n",
    "nswCps_lalonde_features = nswCps_lalonde_subset.drop('treat', axis=1)\n",
    "nswPsid_lalonde_target = nswPsid_lalonde_subset['treat']\n",
    "nswPsid_lalonde_features = nswPsid_lalonde_subset.drop('treat', axis=1)\n",
    "\n",
    "# ======== Dehejia & Wahba (1986) sub sample ======== #\n",
    "\n",
    "\n",
    "nswCps_dehWab_subset = nswCps_dehWab[annSpecification_3]\n",
    "nswPsid_dehWab_subset = nswPsid_dehWab[annSpecification_4]\n",
    "\n",
    "nswCps_dehWab_subset[nswre74_continuos_vars1] = sc.fit_transform(nswCps_dehWab_subset[nswre74_continuos_vars1])\n",
    "nswPsid_dehWab_subset[nswre74_continuos_vars2] = sc.fit_transform(nswPsid_dehWab_subset[nswre74_continuos_vars2])\n",
    "\n",
    "nswCps_dehWab_subset = nswCps_dehWab_subset.sample(frac=1).reset_index(drop=True)\n",
    "nswPsid_dehWab_subset = nswPsid_dehWab_subset.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# create targets and features \n",
    "nswCps_dehWab_target = nswCps_dehWab_subset['treat']\n",
    "nswCps_dehWab_features = nswCps_dehWab_subset.drop('treat', axis=1)\n",
    "nswPsid_dehWab_target = nswPsid_dehWab_subset['treat']\n",
    "nswPsid_dehWab_features = nswPsid_dehWab_subset.drop('treat', axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Neural network </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN model \n",
    "hid_1 = 1000\n",
    "hid_2 = 500\n",
    "\n",
    "class twoLayerNN1(nn.Module):\n",
    "    def __init__(self,num_features = nswCps_lalonde_features.shape[1], num_classes=1, num_hidden_1=hid_1, num_hidden_2=hid_2):\n",
    "       \n",
    "       \n",
    "        super(twoLayerNN1, self).__init__() \n",
    "       \n",
    "        self.input_layer =  nn.Linear(num_features,num_hidden_1)                        \n",
    "        self.hidden_layer1 = nn.Linear(num_hidden_1,num_hidden_2)                          \n",
    "        self.out_layer =  nn.Linear(num_hidden_2, num_classes) \n",
    "\n",
    "        # Activation function\n",
    "        self.relu_activation = nn.ReLU()    \n",
    "        self.output_activation = nn.Sigmoid()\n",
    "        #Drop out and batch normalisation \n",
    "        #self.dropout = nn.Dropout(p=0.35)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(num_hidden_1)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(num_hidden_2)\n",
    "        \n",
    "    # Forward propogation method\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = self.relu_activation(self.input_layer(X))\n",
    "        X = self.batchnorm1(X)\n",
    "        X = self.relu_activation(self.hidden_layer1(X))\n",
    "        X = self.batchnorm2(X)\n",
    "        #X = self.dropout(X)\n",
    "        X = self.output_activation(self.out_layer(X))\n",
    "        return X\n",
    "\n",
    "class twoLayerNN2(nn.Module):\n",
    "    def __init__(self,num_features = nswPsid_lalonde_features.shape[1], num_classes=1, num_hidden_1=hid_1, num_hidden_2=hid_2):\n",
    "       \n",
    "        super(twoLayerNN2, self).__init__() \n",
    "       \n",
    "        self.input_layer =  nn.Linear(num_features,num_hidden_1)                        \n",
    "        self.hidden_layer1 = nn.Linear(num_hidden_1,num_hidden_2)                          \n",
    "        self.out_layer =  nn.Linear(num_hidden_2, num_classes) \n",
    "\n",
    "        # Activation function\n",
    "        self.relu_activation = nn.ReLU()    \n",
    "        self.output_activation = nn.Sigmoid()\n",
    "        #Drop out and batch normalisation \n",
    "        self.dropout = nn.Dropout(p=0.35)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(num_hidden_1)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(num_hidden_2)\n",
    "        \n",
    "    # Forward propogation method\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = self.relu_activation(self.input_layer(X))\n",
    "        X = self.batchnorm1(X)\n",
    "        X = self.relu_activation(self.hidden_layer1(X))\n",
    "        X = self.batchnorm2(X)\n",
    "        #X = self.dropout(X)\n",
    "        X = self.output_activation(self.out_layer(X))\n",
    "        return X       \n",
    "\n",
    "class twoLayerNN3(nn.Module):\n",
    "    def __init__(self,num_features = nswCps_dehWab_features.shape[1], num_classes=1, num_hidden_1=hid_1, num_hidden_2=hid_2):\n",
    "       \n",
    "        super(twoLayerNN3, self).__init__() \n",
    "       \n",
    "        self.input_layer =  nn.Linear(num_features,num_hidden_1)                        \n",
    "        self.hidden_layer1 = nn.Linear(num_hidden_1,num_hidden_2)                          \n",
    "        self.out_layer =  nn.Linear(num_hidden_2, num_classes) \n",
    "\n",
    "        # Activation function\n",
    "        self.relu_activation = nn.ReLU()    \n",
    "        self.output_activation = nn.Sigmoid()\n",
    "        #Drop out and batch normalisation \n",
    "        self.dropout = nn.Dropout(p=0.35)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(num_hidden_1)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(num_hidden_2)\n",
    "        \n",
    "    # Forward propogation method\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = self.relu_activation(self.input_layer(X))\n",
    "        X = self.batchnorm1(X)\n",
    "        X = self.relu_activation(self.hidden_layer1(X))\n",
    "        X = self.batchnorm2(X)\n",
    "       # X = self.dropout(X)\n",
    "        X = self.output_activation(self.out_layer(X))\n",
    "        return X       \n",
    "\n",
    "class twoLayerNN4(nn.Module):\n",
    "    def __init__(self,num_features = nswPsid_dehWab_features.shape[1], num_classes=1, num_hidden_1=hid_1, num_hidden_2=hid_2):\n",
    "        super(twoLayerNN4, self).__init__() \n",
    "       \n",
    "        self.input_layer =  nn.Linear(num_features,num_hidden_1)                        \n",
    "        self.hidden_layer1 = nn.Linear(num_hidden_1,num_hidden_2)                          \n",
    "        self.out_layer =  nn.Linear(num_hidden_2, num_classes) \n",
    "\n",
    "        # Activation function\n",
    "        self.relu_activation = nn.ReLU()    \n",
    "        self.output_activation = nn.Sigmoid()\n",
    "        #Drop out and batch normalisation \n",
    "        self.dropout = nn.Dropout(p=0.35)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(num_hidden_1)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(num_hidden_2)\n",
    "        \n",
    "    # Forward propogation method\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = self.relu_activation(self.input_layer(X))\n",
    "        X = self.batchnorm1(X)\n",
    "        X = self.relu_activation(self.hidden_layer1(X))\n",
    "        X = self.batchnorm2(X)\n",
    "        #X = self.dropout(X)\n",
    "        X = self.output_activation(self.out_layer(X))\n",
    "        return X       \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch import NeuralNetClassifier\n",
    "Epochs = 50\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0018\n",
    "batch_size = 480\n",
    "\n",
    "ANN1 = NeuralNetClassifier(twoLayerNN1,max_epochs=Epochs,lr=learning_rate,optimizer__weight_decay=weight_decay,batch_size =batch_size,optimizer = optim.SGD,criterion = nn.BCELoss,iterator_valid__shuffle=False)\n",
    "ANN2 = NeuralNetClassifier(twoLayerNN2,max_epochs=Epochs,lr=learning_rate,optimizer__weight_decay=weight_decay,batch_size = batch_size,optimizer = optim.SGD,criterion = nn.BCELoss,iterator_valid__shuffle=False )\n",
    "ANN3 = NeuralNetClassifier(twoLayerNN3,max_epochs=Epochs,lr=learning_rate,optimizer__weight_decay=weight_decay,batch_size = batch_size,optimizer = optim.SGD,criterion = nn.BCELoss,iterator_valid__shuffle=False )\n",
    "ANN4 = NeuralNetClassifier(twoLayerNN4,max_epochs=Epochs,lr=learning_rate,optimizer__weight_decay=weight_decay,batch_size = batch_size,optimizer = optim.SGD,criterion = nn.BCELoss,iterator_valid__shuffle=False  )\n",
    "                                                  \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
    "from sklearn import metrics\n",
    "\n",
    "# KFold cross validation \n",
    "def kfold_evaluation_ANN(input_model,features,target,store):\n",
    "\t# prepare the cross-validation procedure\n",
    "\tskf = StratifiedKFold(n_splits=5, shuffle=True,random_state=1)\n",
    "\t\n",
    "\t#Function to get model scores\t\n",
    "\tdef get_score(model,X_train , X_test, y_train , y_test,fold,store):\n",
    "\t\tmodel.fit(X_train,y_train)\n",
    "\t\tprediction = model.predict(X_test)\n",
    "\t\tprediction_proba = model.predict_proba(X_test)\n",
    "\t\tstore.loc['Accuracy:', fold] = metrics.balanced_accuracy_score(y_test, prediction)\n",
    "\t\tstore.loc['Precision:', fold] = metrics.precision_score(y_test, prediction)\n",
    "\t\tstore.loc['Recall:', fold] = metrics.recall_score(y_test, prediction)\n",
    "\t\tstore.loc['MSE:', fold] =  metrics.mean_squared_error(y_test, prediction)\n",
    "\t\tstore.loc['MAE:', fold] =  metrics.mean_absolute_error(y_test, prediction)\n",
    "\t\tstore.loc['R^2:', fold] =  metrics.r2_score(y_test, prediction)\n",
    "\t\tstore.loc['roc_auc:', fold] =  metrics.roc_auc_score(y_test, prediction_proba)\n",
    "\t\tstore.loc['roc_auc:', fold] =  metrics.roc_auc_score(y_test, prediction_proba)\n",
    "\t\tstore.loc['F1:', fold] =  metrics.f1_score(y_test, prediction)\n",
    "\t\tstore.loc['logloss:', fold] =  metrics.log_loss(y_test, prediction_proba)\n",
    "\t\n",
    "\t#Kfold training loop\n",
    "\tfor fold, (train_index , test_index) in enumerate(skf.split(features,target)):\n",
    "\t\t#print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "\n",
    "\t\tX_train , X_test, y_train , y_test = features[train_index],features[test_index],\\\n",
    "\t\t\t\t\t\t\t\t\t\t\t\ttarget[train_index] , target[test_index]\n",
    "\t\t\n",
    "\t\tprint('Xtrain pre augmentation:',X_train.shape)\n",
    "\t\tprint('Ytrain pre augmentation',y_train.shape) \n",
    "\n",
    "\t\tX_train, y_train = resample.fit_resample(X_train, y_train)\t\n",
    "\t\tshuffled = pd.concat([pd.DataFrame(X_train),pd.DataFrame(y_train)],axis=1).sample(frac=1).reset_index(drop=True)\n",
    "        #resplit into test and train \n",
    "\t\tX_train, y_train = shuffled.iloc[:,:-1], shuffled.iloc[:,-1]\n",
    "\t\tX_train = X_train.to_numpy()\n",
    "\t\ty_train = y_train.to_numpy()\n",
    "\t\tX_train = X_train.astype(np.float32)\n",
    "\t\ty_train = y_train.astype(np.float32)\n",
    "\t\ty_train = np.reshape(y_train,(y_train.shape[0],1))\n",
    "\n",
    "\t\tprint('Xtrain:',X_train.shape)\n",
    "\t\tprint('Ytrain',y_train.shape)\t\n",
    "\t\tprint('Xtest',X_test.shape)\t\n",
    "\t\tprint('Ytest',y_test.shape)\t\n",
    "\t\t\t\t\t\t\t\t\t \n",
    "\t\tget_score(input_model,X_train , X_test, y_train , y_test,fold,store)\n",
    "\t\t# return statement here with python dataframe of metrics \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conver features and targets to numpy arrays then floats to feed to ANN\n",
    "X1 = nswCps_lalonde_features.to_numpy()\n",
    "Y1 = nswCps_lalonde_target.to_numpy()\n",
    "X1 = X1.astype(np.float32)\n",
    "Y1 = Y1.astype(np.float32)\n",
    "Y1 = np.reshape(Y1,(Y1.shape[0],1))\n",
    "\n",
    "# ~ ~ ~\n",
    "X2 = nswPsid_lalonde_features.to_numpy()\n",
    "Y2 = nswPsid_lalonde_target.to_numpy()\n",
    "X2 = X2.astype(np.float32)\n",
    "Y2 = Y2.astype(np.float32)\n",
    "Y2 = np.reshape(Y2,(Y2.shape[0],1))\n",
    "# ~ ~ ~\n",
    "X3 = nswCps_dehWab_features.to_numpy()\n",
    "Y3 = nswCps_dehWab_target.to_numpy()\n",
    "X3 = X3.astype(np.float32)\n",
    "Y3 = Y3.astype(np.float32)\n",
    "Y3 = np.reshape(Y3,(Y3.shape[0],1))\n",
    "# ~ ~ ~\n",
    "X4 = nswPsid_dehWab_features.to_numpy()\n",
    "Y4 = nswPsid_dehWab_target.to_numpy()\n",
    "X4 = X4.astype(np.float32)\n",
    "Y4 = Y4.astype(np.float32)\n",
    "Y4 = np.reshape(Y4,(Y4.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain pre augmentation: (13031, 6)\n",
      "Ytrain pre augmentation (13031, 1)\n",
      "Xtrain: (23980, 6)\n",
      "Ytrain (23980, 1)\n",
      "Xtest (3258, 6)\n",
      "Ytest (3258, 1)\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.3612\u001b[0m       \u001b[32m0.5050\u001b[0m        \u001b[35m0.3376\u001b[0m  0.8277\n",
      "      2        \u001b[36m0.2273\u001b[0m       0.5050        \u001b[35m0.2250\u001b[0m  0.6132\n",
      "      3        \u001b[36m0.1944\u001b[0m       0.5050        \u001b[35m0.1933\u001b[0m  0.6792\n",
      "      4        \u001b[36m0.1772\u001b[0m       0.5050        \u001b[35m0.1788\u001b[0m  0.6041\n",
      "      5        \u001b[36m0.1660\u001b[0m       0.5050        \u001b[35m0.1687\u001b[0m  0.6048\n",
      "      6        \u001b[36m0.1576\u001b[0m       0.5050        \u001b[35m0.1606\u001b[0m  0.6078\n",
      "      7        \u001b[36m0.1509\u001b[0m       0.5050        \u001b[35m0.1540\u001b[0m  0.6033\n",
      "      8        \u001b[36m0.1455\u001b[0m       0.5050        \u001b[35m0.1482\u001b[0m  0.6103\n",
      "      9        \u001b[36m0.1409\u001b[0m       0.5050        \u001b[35m0.1435\u001b[0m  0.6328\n",
      "     10        \u001b[36m0.1369\u001b[0m       0.5050        \u001b[35m0.1393\u001b[0m  0.6987\n",
      "     11        \u001b[36m0.1335\u001b[0m       0.5050        \u001b[35m0.1364\u001b[0m  1.2942\n",
      "     12        \u001b[36m0.1305\u001b[0m       0.5050        \u001b[35m0.1335\u001b[0m  1.0697\n",
      "     13        \u001b[36m0.1277\u001b[0m       0.5050        \u001b[35m0.1301\u001b[0m  1.0210\n",
      "     14        \u001b[36m0.1253\u001b[0m       0.5050        \u001b[35m0.1275\u001b[0m  1.1108\n",
      "     15        \u001b[36m0.1232\u001b[0m       0.5050        \u001b[35m0.1240\u001b[0m  0.9268\n",
      "     16        \u001b[36m0.1211\u001b[0m       0.5050        0.1250  0.9911\n",
      "     17        \u001b[36m0.1193\u001b[0m       0.5050        \u001b[35m0.1225\u001b[0m  0.8554\n",
      "     18        \u001b[36m0.1177\u001b[0m       0.5050        \u001b[35m0.1210\u001b[0m  0.8468\n",
      "     19        \u001b[36m0.1162\u001b[0m       0.5050        \u001b[35m0.1170\u001b[0m  0.9050\n",
      "     20        \u001b[36m0.1149\u001b[0m       0.5050        \u001b[35m0.1156\u001b[0m  1.1554\n",
      "     21        \u001b[36m0.1134\u001b[0m       0.5050        \u001b[35m0.1140\u001b[0m  1.0524\n",
      "     22        \u001b[36m0.1123\u001b[0m       0.5050        \u001b[35m0.1127\u001b[0m  0.8903\n",
      "     23        \u001b[36m0.1114\u001b[0m       0.5050        \u001b[35m0.1119\u001b[0m  0.8499\n",
      "     24        \u001b[36m0.1103\u001b[0m       0.5050        \u001b[35m0.1102\u001b[0m  0.8998\n",
      "     25        \u001b[36m0.1095\u001b[0m       0.5050        \u001b[35m0.1093\u001b[0m  0.9435\n",
      "     26        \u001b[36m0.1086\u001b[0m       0.5050        \u001b[35m0.1085\u001b[0m  0.8721\n",
      "     27        \u001b[36m0.1076\u001b[0m       0.5050        \u001b[35m0.1075\u001b[0m  0.8734\n",
      "     28        \u001b[36m0.1068\u001b[0m       0.5050        \u001b[35m0.1067\u001b[0m  0.9351\n",
      "     29        \u001b[36m0.1061\u001b[0m       0.5050        \u001b[35m0.1058\u001b[0m  0.9675\n",
      "     30        \u001b[36m0.1054\u001b[0m       0.5050        \u001b[35m0.1051\u001b[0m  0.9181\n",
      "     31        \u001b[36m0.1047\u001b[0m       0.5050        \u001b[35m0.1048\u001b[0m  0.9028\n",
      "     32        \u001b[36m0.1037\u001b[0m       0.5050        \u001b[35m0.1043\u001b[0m  1.0703\n",
      "     33        \u001b[36m0.1032\u001b[0m       0.5050        \u001b[35m0.1034\u001b[0m  0.9456\n",
      "     34        \u001b[36m0.1026\u001b[0m       0.5050        0.1036  0.9209\n",
      "     35        \u001b[36m0.1020\u001b[0m       0.5050        \u001b[35m0.1027\u001b[0m  0.9863\n",
      "     36        \u001b[36m0.1016\u001b[0m       0.5050        \u001b[35m0.1018\u001b[0m  0.9384\n",
      "     37        0.1016       0.5050        0.1034  0.9223\n",
      "     38        \u001b[36m0.1007\u001b[0m       0.5050        \u001b[35m0.1009\u001b[0m  0.9129\n",
      "     39        \u001b[36m0.1003\u001b[0m       0.5050        0.1074  0.8895\n",
      "     40        \u001b[36m0.0998\u001b[0m       0.5050        0.1059  0.8449\n",
      "     41        \u001b[36m0.0991\u001b[0m       0.5050        \u001b[35m0.1009\u001b[0m  0.8585\n",
      "     42        \u001b[36m0.0989\u001b[0m       0.5050        0.1028  0.9136\n",
      "     43        \u001b[36m0.0981\u001b[0m       0.5050        \u001b[35m0.0991\u001b[0m  0.9019\n",
      "     44        \u001b[36m0.0980\u001b[0m       0.5050        0.1001  1.1876\n",
      "     45        0.0983       0.5050        0.0999  1.3937\n",
      "     46        \u001b[36m0.0980\u001b[0m       0.5050        0.0994  1.5204\n",
      "     47        \u001b[36m0.0967\u001b[0m       0.5050        \u001b[35m0.0974\u001b[0m  1.3386\n",
      "     48        \u001b[36m0.0966\u001b[0m       0.5050        0.0974  1.3132\n",
      "     49        \u001b[36m0.0955\u001b[0m       0.5050        \u001b[35m0.0963\u001b[0m  0.9104\n",
      "     50        \u001b[36m0.0948\u001b[0m       0.5050        0.0966  1.0140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain pre augmentation: (13031, 6)\n",
      "Ytrain pre augmentation (13031, 1)\n",
      "Xtrain: (23860, 6)\n",
      "Ytrain (23860, 1)\n",
      "Xtest (3258, 6)\n",
      "Ytest (3258, 1)\n",
      "Re-initializing module.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.3548\u001b[0m       \u001b[32m0.5069\u001b[0m        \u001b[35m0.3414\u001b[0m  1.1054\n",
      "      2        \u001b[36m0.2370\u001b[0m       0.5069        \u001b[35m0.2270\u001b[0m  0.9144\n",
      "      3        \u001b[36m0.2085\u001b[0m       0.5069        \u001b[35m0.2037\u001b[0m  0.8404\n",
      "      4        \u001b[36m0.1933\u001b[0m       0.5069        \u001b[35m0.1894\u001b[0m  1.1062\n",
      "      5        \u001b[36m0.1826\u001b[0m       0.5069        \u001b[35m0.1806\u001b[0m  0.9932\n",
      "      6        \u001b[36m0.1750\u001b[0m       0.5069        \u001b[35m0.1734\u001b[0m  0.9396\n",
      "      7        \u001b[36m0.1690\u001b[0m       0.5069        \u001b[35m0.1675\u001b[0m  0.9401\n",
      "      8        \u001b[36m0.1640\u001b[0m       0.5069        \u001b[35m0.1622\u001b[0m  0.8723\n",
      "      9        \u001b[36m0.1596\u001b[0m       0.5069        \u001b[35m0.1579\u001b[0m  0.9118\n",
      "     10        \u001b[36m0.1557\u001b[0m       0.5069        \u001b[35m0.1537\u001b[0m  0.7936\n",
      "     11        \u001b[36m0.1522\u001b[0m       0.5069        \u001b[35m0.1497\u001b[0m  0.7879\n",
      "     12        \u001b[36m0.1491\u001b[0m       0.5069        \u001b[35m0.1466\u001b[0m  0.8306\n",
      "     13        \u001b[36m0.1463\u001b[0m       0.5069        \u001b[35m0.1439\u001b[0m  0.7321\n",
      "     14        \u001b[36m0.1437\u001b[0m       0.5069        \u001b[35m0.1415\u001b[0m  0.7998\n",
      "     15        \u001b[36m0.1412\u001b[0m       0.5069        \u001b[35m0.1393\u001b[0m  0.7733\n",
      "     16        \u001b[36m0.1390\u001b[0m       0.5069        \u001b[35m0.1374\u001b[0m  0.7797\n",
      "     17        \u001b[36m0.1373\u001b[0m       0.5069        \u001b[35m0.1372\u001b[0m  0.8752\n",
      "     18        \u001b[36m0.1351\u001b[0m       0.5069        \u001b[35m0.1338\u001b[0m  0.7052\n",
      "     19        \u001b[36m0.1332\u001b[0m       0.5069        0.1348  0.8616\n",
      "     20        \u001b[36m0.1315\u001b[0m       0.5069        \u001b[35m0.1304\u001b[0m  0.9142\n",
      "     21        \u001b[36m0.1299\u001b[0m       0.5069        0.1313  1.0403\n",
      "     22        \u001b[36m0.1290\u001b[0m       0.5069        \u001b[35m0.1267\u001b[0m  0.9824\n",
      "     23        \u001b[36m0.1278\u001b[0m       0.5069        \u001b[35m0.1254\u001b[0m  0.9692\n",
      "     24        \u001b[36m0.1264\u001b[0m       0.5069        0.1259  0.8919\n",
      "     25        \u001b[36m0.1248\u001b[0m       0.5069        0.1270  0.8654\n",
      "     26        \u001b[36m0.1237\u001b[0m       0.5069        \u001b[35m0.1241\u001b[0m  0.8466\n",
      "     27        \u001b[36m0.1226\u001b[0m       0.5069        \u001b[35m0.1213\u001b[0m  0.8416\n",
      "     28        \u001b[36m0.1217\u001b[0m       0.5069        \u001b[35m0.1204\u001b[0m  0.8563\n",
      "     29        \u001b[36m0.1207\u001b[0m       0.5069        0.1218  0.8064\n",
      "     30        \u001b[36m0.1198\u001b[0m       0.5069        0.1208  0.7777\n",
      "     31        \u001b[36m0.1188\u001b[0m       0.5069        \u001b[35m0.1201\u001b[0m  0.7538\n",
      "     32        \u001b[36m0.1179\u001b[0m       0.5069        \u001b[35m0.1191\u001b[0m  0.7983\n",
      "     33        \u001b[36m0.1170\u001b[0m       0.5069        \u001b[35m0.1180\u001b[0m  0.8030\n",
      "     34        \u001b[36m0.1162\u001b[0m       0.5069        \u001b[35m0.1154\u001b[0m  0.8765\n",
      "     35        \u001b[36m0.1154\u001b[0m       0.5069        0.1168  0.8765\n",
      "     36        \u001b[36m0.1147\u001b[0m       0.5069        0.1167  0.9492\n",
      "     37        \u001b[36m0.1141\u001b[0m       0.5069        0.1164  0.7948\n",
      "     38        \u001b[36m0.1135\u001b[0m       0.5069        0.1160  0.7882\n",
      "     39        \u001b[36m0.1126\u001b[0m       0.5069        \u001b[35m0.1122\u001b[0m  1.1353\n",
      "     40        \u001b[36m0.1120\u001b[0m       0.5069        0.1149  0.9212\n",
      "     41        \u001b[36m0.1118\u001b[0m       0.5069        0.1144  0.9126\n",
      "     42        \u001b[36m0.1107\u001b[0m       0.5069        \u001b[35m0.1106\u001b[0m  0.9727\n",
      "     43        \u001b[36m0.1103\u001b[0m       0.5069        0.1138  0.9082\n",
      "     44        \u001b[36m0.1102\u001b[0m       0.5069        0.1135  0.9363\n",
      "     45        \u001b[36m0.1097\u001b[0m       0.5069        0.1134  1.0020\n",
      "     46        \u001b[36m0.1092\u001b[0m       0.5069        0.1131  0.9814\n",
      "     47        \u001b[36m0.1082\u001b[0m       0.5069        \u001b[35m0.1082\u001b[0m  0.8968\n",
      "     48        \u001b[36m0.1082\u001b[0m       0.5069        0.1137  0.9193\n",
      "     49        \u001b[36m0.1078\u001b[0m       0.5069        0.1127  1.0154\n",
      "     50        \u001b[36m0.1071\u001b[0m       0.5069        \u001b[35m0.1073\u001b[0m  1.0421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain pre augmentation: (13031, 6)\n",
      "Ytrain pre augmentation (13031, 1)\n",
      "Xtrain: (23860, 6)\n",
      "Ytrain (23860, 1)\n",
      "Xtest (3258, 6)\n",
      "Ytest (3258, 1)\n",
      "Re-initializing module.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.3956\u001b[0m       \u001b[32m0.5054\u001b[0m        \u001b[35m0.3457\u001b[0m  0.9232\n",
      "      2        \u001b[36m0.2362\u001b[0m       0.5054        \u001b[35m0.2260\u001b[0m  0.9152\n",
      "      3        \u001b[36m0.2055\u001b[0m       0.5054        \u001b[35m0.2040\u001b[0m  0.9278\n",
      "      4        \u001b[36m0.1916\u001b[0m       0.5054        \u001b[35m0.1931\u001b[0m  1.0135\n",
      "      5        \u001b[36m0.1831\u001b[0m       0.5054        \u001b[35m0.1858\u001b[0m  1.2256\n",
      "      6        \u001b[36m0.1772\u001b[0m       0.5054        \u001b[35m0.1803\u001b[0m  1.3238\n",
      "      7        \u001b[36m0.1725\u001b[0m       0.5054        \u001b[35m0.1758\u001b[0m  0.9594\n",
      "      8        \u001b[36m0.1687\u001b[0m       0.5054        \u001b[35m0.1721\u001b[0m  1.4391\n",
      "      9        \u001b[36m0.1654\u001b[0m       0.5054        \u001b[35m0.1686\u001b[0m  1.0410\n",
      "     10        \u001b[36m0.1624\u001b[0m       0.5054        \u001b[35m0.1655\u001b[0m  1.1274\n",
      "     11        \u001b[36m0.1595\u001b[0m       0.5054        \u001b[35m0.1624\u001b[0m  1.0818\n",
      "     12        \u001b[36m0.1570\u001b[0m       0.5054        \u001b[35m0.1597\u001b[0m  1.0127\n",
      "     13        \u001b[36m0.1546\u001b[0m       0.5054        \u001b[35m0.1570\u001b[0m  0.9702\n",
      "     14        \u001b[36m0.1522\u001b[0m       0.5054        \u001b[35m0.1546\u001b[0m  1.0920\n",
      "     15        \u001b[36m0.1502\u001b[0m       0.5054        \u001b[35m0.1525\u001b[0m  0.8608\n",
      "     16        \u001b[36m0.1483\u001b[0m       0.5054        \u001b[35m0.1506\u001b[0m  0.7018\n",
      "     17        \u001b[36m0.1465\u001b[0m       0.5054        \u001b[35m0.1487\u001b[0m  0.6939\n",
      "     18        \u001b[36m0.1447\u001b[0m       0.5054        \u001b[35m0.1470\u001b[0m  0.6830\n",
      "     19        \u001b[36m0.1431\u001b[0m       0.5054        \u001b[35m0.1453\u001b[0m  0.6961\n",
      "     20        \u001b[36m0.1415\u001b[0m       0.5054        \u001b[35m0.1437\u001b[0m  0.6842\n",
      "     21        \u001b[36m0.1399\u001b[0m       0.5054        \u001b[35m0.1422\u001b[0m  0.6883\n",
      "     22        \u001b[36m0.1384\u001b[0m       0.5054        \u001b[35m0.1406\u001b[0m  0.6817\n",
      "     23        \u001b[36m0.1369\u001b[0m       0.5054        \u001b[35m0.1392\u001b[0m  0.7454\n",
      "     24        \u001b[36m0.1355\u001b[0m       0.5054        \u001b[35m0.1376\u001b[0m  0.8625\n",
      "     25        \u001b[36m0.1340\u001b[0m       0.5054        \u001b[35m0.1362\u001b[0m  0.8066\n",
      "     26        \u001b[36m0.1326\u001b[0m       0.5054        \u001b[35m0.1347\u001b[0m  0.6737\n",
      "     27        \u001b[36m0.1312\u001b[0m       0.5054        \u001b[35m0.1334\u001b[0m  0.7009\n",
      "     28        \u001b[36m0.1299\u001b[0m       0.5054        \u001b[35m0.1321\u001b[0m  0.6704\n",
      "     29        \u001b[36m0.1286\u001b[0m       0.5054        \u001b[35m0.1309\u001b[0m  0.6793\n",
      "     30        \u001b[36m0.1274\u001b[0m       0.5054        \u001b[35m0.1297\u001b[0m  0.6916\n",
      "     31        \u001b[36m0.1262\u001b[0m       0.5054        \u001b[35m0.1285\u001b[0m  0.6927\n",
      "     32        \u001b[36m0.1250\u001b[0m       0.5054        \u001b[35m0.1274\u001b[0m  0.6978\n",
      "     33        \u001b[36m0.1239\u001b[0m       0.5054        \u001b[35m0.1263\u001b[0m  0.6907\n",
      "     34        \u001b[36m0.1228\u001b[0m       0.5054        \u001b[35m0.1252\u001b[0m  0.6828\n",
      "     35        \u001b[36m0.1217\u001b[0m       0.5054        \u001b[35m0.1241\u001b[0m  0.6944\n",
      "     36        \u001b[36m0.1207\u001b[0m       0.5054        \u001b[35m0.1231\u001b[0m  0.6802\n",
      "     37        \u001b[36m0.1196\u001b[0m       0.5054        \u001b[35m0.1221\u001b[0m  0.7643\n",
      "     38        \u001b[36m0.1186\u001b[0m       0.5054        \u001b[35m0.1211\u001b[0m  0.6653\n",
      "     39        \u001b[36m0.1176\u001b[0m       0.5054        \u001b[35m0.1202\u001b[0m  0.6551\n",
      "     40        \u001b[36m0.1166\u001b[0m       0.5054        \u001b[35m0.1192\u001b[0m  0.6766\n",
      "     41        \u001b[36m0.1157\u001b[0m       0.5054        \u001b[35m0.1183\u001b[0m  0.6919\n",
      "     42        \u001b[36m0.1147\u001b[0m       0.5054        \u001b[35m0.1175\u001b[0m  0.8163\n",
      "     43        \u001b[36m0.1138\u001b[0m       0.5054        \u001b[35m0.1166\u001b[0m  0.8345\n",
      "     44        \u001b[36m0.1129\u001b[0m       0.5054        \u001b[35m0.1157\u001b[0m  0.9224\n",
      "     45        \u001b[36m0.1121\u001b[0m       0.5054        \u001b[35m0.1149\u001b[0m  1.0811\n",
      "     46        \u001b[36m0.1112\u001b[0m       0.5054        \u001b[35m0.1142\u001b[0m  1.3279\n",
      "     47        \u001b[36m0.1104\u001b[0m       0.5054        \u001b[35m0.1133\u001b[0m  0.9387\n",
      "     48        \u001b[36m0.1096\u001b[0m       0.5054        \u001b[35m0.1125\u001b[0m  0.9447\n",
      "     49        \u001b[36m0.1088\u001b[0m       0.5054        \u001b[35m0.1117\u001b[0m  0.8971\n",
      "     50        \u001b[36m0.1079\u001b[0m       0.5054        \u001b[35m0.1109\u001b[0m  0.9163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain pre augmentation: (13031, 6)\n",
      "Ytrain pre augmentation (13031, 1)\n",
      "Xtrain: (24177, 6)\n",
      "Ytrain (24177, 1)\n",
      "Xtest (3258, 6)\n",
      "Ytest (3258, 1)\n",
      "Re-initializing module.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.3956\u001b[0m       \u001b[32m0.5031\u001b[0m        \u001b[35m0.3337\u001b[0m  0.8841\n",
      "      2        \u001b[36m0.2524\u001b[0m       0.5031        \u001b[35m0.2260\u001b[0m  0.8298\n",
      "      3        \u001b[36m0.2234\u001b[0m       0.5031        \u001b[35m0.2056\u001b[0m  1.0478\n",
      "      4        \u001b[36m0.2098\u001b[0m       0.5031        \u001b[35m0.1942\u001b[0m  0.9738\n",
      "      5        \u001b[36m0.2012\u001b[0m       0.5031        \u001b[35m0.1863\u001b[0m  0.8467\n",
      "      6        \u001b[36m0.1949\u001b[0m       0.5031        \u001b[35m0.1804\u001b[0m  0.8143\n",
      "      7        \u001b[36m0.1898\u001b[0m       0.5031        \u001b[35m0.1756\u001b[0m  0.6917\n",
      "      8        \u001b[36m0.1854\u001b[0m       0.5031        \u001b[35m0.1715\u001b[0m  0.6868\n",
      "      9        \u001b[36m0.1815\u001b[0m       0.5031        \u001b[35m0.1677\u001b[0m  0.7497\n",
      "     10        \u001b[36m0.1779\u001b[0m       0.5031        \u001b[35m0.1642\u001b[0m  0.9441\n",
      "     11        \u001b[36m0.1746\u001b[0m       0.5031        \u001b[35m0.1611\u001b[0m  0.9625\n",
      "     12        \u001b[36m0.1716\u001b[0m       0.5031        \u001b[35m0.1583\u001b[0m  0.9944\n",
      "     13        \u001b[36m0.1688\u001b[0m       0.5031        \u001b[35m0.1557\u001b[0m  0.9586\n",
      "     14        \u001b[36m0.1662\u001b[0m       0.5031        \u001b[35m0.1532\u001b[0m  0.9438\n",
      "     15        \u001b[36m0.1637\u001b[0m       0.5031        \u001b[35m0.1507\u001b[0m  0.9514\n",
      "     16        \u001b[36m0.1612\u001b[0m       0.5031        \u001b[35m0.1484\u001b[0m  0.9367\n",
      "     17        \u001b[36m0.1589\u001b[0m       0.5031        \u001b[35m0.1462\u001b[0m  0.9705\n",
      "     18        \u001b[36m0.1567\u001b[0m       0.5031        \u001b[35m0.1442\u001b[0m  0.8589\n",
      "     19        \u001b[36m0.1547\u001b[0m       0.5031        \u001b[35m0.1423\u001b[0m  0.8032\n",
      "     20        \u001b[36m0.1529\u001b[0m       0.5031        \u001b[35m0.1406\u001b[0m  0.7317\n",
      "     21        \u001b[36m0.1511\u001b[0m       0.5031        \u001b[35m0.1389\u001b[0m  0.7969\n",
      "     22        \u001b[36m0.1494\u001b[0m       0.5031        \u001b[35m0.1372\u001b[0m  1.0411\n",
      "     23        \u001b[36m0.1477\u001b[0m       0.5031        \u001b[35m0.1357\u001b[0m  0.8952\n",
      "     24        \u001b[36m0.1461\u001b[0m       0.5031        \u001b[35m0.1342\u001b[0m  1.6014\n",
      "     25        \u001b[36m0.1445\u001b[0m       0.5031        \u001b[35m0.1326\u001b[0m  1.0666\n",
      "     26        \u001b[36m0.1430\u001b[0m       0.5031        \u001b[35m0.1312\u001b[0m  1.0063\n",
      "     27        \u001b[36m0.1414\u001b[0m       0.5031        \u001b[35m0.1297\u001b[0m  1.4607\n",
      "     28        \u001b[36m0.1399\u001b[0m       0.5031        \u001b[35m0.1283\u001b[0m  0.9260\n",
      "     29        \u001b[36m0.1385\u001b[0m       0.5031        \u001b[35m0.1270\u001b[0m  0.9460\n",
      "     30        \u001b[36m0.1372\u001b[0m       0.5031        \u001b[35m0.1258\u001b[0m  1.1197\n",
      "     31        \u001b[36m0.1358\u001b[0m       0.5031        \u001b[35m0.1245\u001b[0m  2.0086\n",
      "     32        \u001b[36m0.1346\u001b[0m       0.5031        \u001b[35m0.1234\u001b[0m  1.0066\n",
      "     33        \u001b[36m0.1334\u001b[0m       0.5031        \u001b[35m0.1222\u001b[0m  0.9163\n",
      "     34        \u001b[36m0.1322\u001b[0m       0.5031        \u001b[35m0.1211\u001b[0m  0.9272\n",
      "     35        \u001b[36m0.1310\u001b[0m       0.5031        \u001b[35m0.1200\u001b[0m  0.9759\n",
      "     36        \u001b[36m0.1299\u001b[0m       0.5031        \u001b[35m0.1190\u001b[0m  1.0210\n",
      "     37        \u001b[36m0.1287\u001b[0m       0.5031        \u001b[35m0.1180\u001b[0m  0.9857\n",
      "     38        \u001b[36m0.1276\u001b[0m       0.5031        \u001b[35m0.1170\u001b[0m  1.0027\n",
      "     39        \u001b[36m0.1265\u001b[0m       0.5031        \u001b[35m0.1159\u001b[0m  0.9697\n",
      "     40        \u001b[36m0.1255\u001b[0m       0.5031        \u001b[35m0.1150\u001b[0m  0.9209\n",
      "     41        \u001b[36m0.1245\u001b[0m       0.5031        \u001b[35m0.1141\u001b[0m  0.9307\n",
      "     42        \u001b[36m0.1236\u001b[0m       0.5031        \u001b[35m0.1132\u001b[0m  1.0608\n",
      "     43        \u001b[36m0.1226\u001b[0m       0.5031        \u001b[35m0.1123\u001b[0m  1.0244\n",
      "     44        \u001b[36m0.1217\u001b[0m       0.5031        \u001b[35m0.1114\u001b[0m  0.9655\n",
      "     45        \u001b[36m0.1208\u001b[0m       0.5031        \u001b[35m0.1106\u001b[0m  0.9743\n",
      "     46        \u001b[36m0.1200\u001b[0m       0.5031        \u001b[35m0.1097\u001b[0m  0.9387\n",
      "     47        \u001b[36m0.1191\u001b[0m       0.5031        \u001b[35m0.1089\u001b[0m  0.9422\n",
      "     48        \u001b[36m0.1183\u001b[0m       0.5031        \u001b[35m0.1082\u001b[0m  0.9486\n",
      "     49        \u001b[36m0.1174\u001b[0m       0.5031        \u001b[35m0.1075\u001b[0m  1.2694\n",
      "     50        \u001b[36m0.1166\u001b[0m       0.5031        \u001b[35m0.1067\u001b[0m  1.1572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain pre augmentation: (13032, 6)\n",
      "Ytrain pre augmentation (13032, 1)\n",
      "Xtrain: (24023, 6)\n",
      "Ytrain (24023, 1)\n",
      "Xtest (3257, 6)\n",
      "Ytest (3257, 1)\n",
      "Re-initializing module.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.4156\u001b[0m       \u001b[32m0.5028\u001b[0m        \u001b[35m0.3297\u001b[0m  1.0213\n",
      "      2        \u001b[36m0.2335\u001b[0m       0.5028        \u001b[35m0.2152\u001b[0m  0.9057\n",
      "      3        \u001b[36m0.2030\u001b[0m       0.5028        \u001b[35m0.1962\u001b[0m  0.9398\n",
      "      4        \u001b[36m0.1895\u001b[0m       0.5028        \u001b[35m0.1853\u001b[0m  0.9634\n",
      "      5        \u001b[36m0.1809\u001b[0m       0.5028        \u001b[35m0.1777\u001b[0m  0.9113\n",
      "      6        \u001b[36m0.1748\u001b[0m       0.5028        \u001b[35m0.1721\u001b[0m  0.8614\n",
      "      7        \u001b[36m0.1699\u001b[0m       0.5028        \u001b[35m0.1674\u001b[0m  0.8652\n",
      "      8        \u001b[36m0.1655\u001b[0m       0.5028        \u001b[35m0.1632\u001b[0m  0.8277\n",
      "      9        \u001b[36m0.1613\u001b[0m       0.5028        \u001b[35m0.1592\u001b[0m  0.7610\n",
      "     10        \u001b[36m0.1578\u001b[0m       0.5028        \u001b[35m0.1555\u001b[0m  0.6861\n",
      "     11        \u001b[36m0.1546\u001b[0m       0.5028        \u001b[35m0.1526\u001b[0m  0.7520\n",
      "     12        \u001b[36m0.1516\u001b[0m       0.5028        \u001b[35m0.1501\u001b[0m  0.6708\n",
      "     13        \u001b[36m0.1490\u001b[0m       0.5028        \u001b[35m0.1465\u001b[0m  0.7183\n",
      "     14        \u001b[36m0.1465\u001b[0m       0.5028        \u001b[35m0.1441\u001b[0m  0.7466\n",
      "     15        \u001b[36m0.1441\u001b[0m       0.5028        \u001b[35m0.1412\u001b[0m  0.7045\n",
      "     16        \u001b[36m0.1418\u001b[0m       0.5028        \u001b[35m0.1391\u001b[0m  0.6779\n",
      "     17        \u001b[36m0.1397\u001b[0m       0.5028        \u001b[35m0.1368\u001b[0m  0.7300\n",
      "     18        \u001b[36m0.1377\u001b[0m       0.5028        \u001b[35m0.1347\u001b[0m  0.7102\n",
      "     19        \u001b[36m0.1358\u001b[0m       0.5028        \u001b[35m0.1327\u001b[0m  0.7429\n",
      "     20        \u001b[36m0.1339\u001b[0m       0.5028        \u001b[35m0.1307\u001b[0m  0.6810\n",
      "     21        \u001b[36m0.1321\u001b[0m       0.5028        \u001b[35m0.1288\u001b[0m  0.8406\n",
      "     22        \u001b[36m0.1304\u001b[0m       0.5028        \u001b[35m0.1270\u001b[0m  0.8525\n",
      "     23        \u001b[36m0.1288\u001b[0m       0.5028        \u001b[35m0.1251\u001b[0m  0.9317\n",
      "     24        \u001b[36m0.1272\u001b[0m       0.5028        \u001b[35m0.1236\u001b[0m  0.8693\n",
      "     25        \u001b[36m0.1257\u001b[0m       0.5028        \u001b[35m0.1220\u001b[0m  0.8768\n",
      "     26        \u001b[36m0.1243\u001b[0m       0.5028        \u001b[35m0.1207\u001b[0m  0.8308\n",
      "     27        \u001b[36m0.1231\u001b[0m       0.5028        \u001b[35m0.1193\u001b[0m  0.8271\n",
      "     28        \u001b[36m0.1218\u001b[0m       0.5028        \u001b[35m0.1181\u001b[0m  0.8184\n",
      "     29        \u001b[36m0.1207\u001b[0m       0.5028        \u001b[35m0.1169\u001b[0m  0.9370\n",
      "     30        \u001b[36m0.1196\u001b[0m       0.5028        \u001b[35m0.1158\u001b[0m  0.8452\n",
      "     31        \u001b[36m0.1185\u001b[0m       0.5028        \u001b[35m0.1147\u001b[0m  0.9086\n",
      "     32        \u001b[36m0.1176\u001b[0m       0.5028        \u001b[35m0.1138\u001b[0m  0.8073\n",
      "     33        \u001b[36m0.1166\u001b[0m       0.5028        \u001b[35m0.1131\u001b[0m  0.7934\n",
      "     34        \u001b[36m0.1157\u001b[0m       0.5028        \u001b[35m0.1118\u001b[0m  0.9224\n",
      "     35        \u001b[36m0.1148\u001b[0m       0.5028        \u001b[35m0.1114\u001b[0m  0.8945\n",
      "     36        \u001b[36m0.1139\u001b[0m       0.5028        \u001b[35m0.1102\u001b[0m  0.7528\n",
      "     37        \u001b[36m0.1130\u001b[0m       0.5028        \u001b[35m0.1100\u001b[0m  0.6600\n",
      "     38        \u001b[36m0.1122\u001b[0m       0.5028        \u001b[35m0.1091\u001b[0m  0.6907\n",
      "     39        \u001b[36m0.1116\u001b[0m       0.5028        \u001b[35m0.1077\u001b[0m  0.7011\n",
      "     40        \u001b[36m0.1108\u001b[0m       0.5028        \u001b[35m0.1073\u001b[0m  0.7186\n",
      "     41        \u001b[36m0.1100\u001b[0m       0.5028        \u001b[35m0.1063\u001b[0m  0.6480\n",
      "     42        \u001b[36m0.1095\u001b[0m       0.5028        \u001b[35m0.1054\u001b[0m  0.7121\n",
      "     43        \u001b[36m0.1090\u001b[0m       0.5028        \u001b[35m0.1048\u001b[0m  0.7110\n",
      "     44        \u001b[36m0.1086\u001b[0m       0.5028        0.1049  0.7302\n",
      "     45        \u001b[36m0.1081\u001b[0m       0.5028        \u001b[35m0.1046\u001b[0m  0.6863\n",
      "     46        \u001b[36m0.1076\u001b[0m       0.5028        \u001b[35m0.1044\u001b[0m  0.7112\n",
      "     47        \u001b[36m0.1068\u001b[0m       0.5028        \u001b[35m0.1026\u001b[0m  0.7438\n",
      "     48        \u001b[36m0.1063\u001b[0m       0.5028        \u001b[35m0.1022\u001b[0m  0.7118\n",
      "     49        \u001b[36m0.1061\u001b[0m       0.5028        \u001b[35m0.1021\u001b[0m  0.7296\n",
      "     50        \u001b[36m0.1057\u001b[0m       0.5028        0.1032  0.8114\n",
      "Xtrain pre augmentation: (2229, 5)\n",
      "Ytrain pre augmentation (2229, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain: (3218, 5)\n",
      "Ytrain (3218, 1)\n",
      "Xtest (558, 5)\n",
      "Ytest (558, 1)\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6568\u001b[0m       \u001b[32m0.5248\u001b[0m        \u001b[35m0.6121\u001b[0m  0.1748\n",
      "      2        \u001b[36m0.3888\u001b[0m       0.5248        \u001b[35m0.5524\u001b[0m  0.1436\n",
      "      3        \u001b[36m0.3218\u001b[0m       0.5248        \u001b[35m0.4902\u001b[0m  0.1447\n",
      "      4        \u001b[36m0.2889\u001b[0m       0.5248        \u001b[35m0.4297\u001b[0m  0.1408\n",
      "      5        \u001b[36m0.2688\u001b[0m       0.5248        \u001b[35m0.3739\u001b[0m  0.1420\n",
      "      6        \u001b[36m0.2543\u001b[0m       0.5248        \u001b[35m0.3270\u001b[0m  0.1457\n",
      "      7        \u001b[36m0.2429\u001b[0m       0.5248        \u001b[35m0.2906\u001b[0m  0.1359\n",
      "      8        \u001b[36m0.2337\u001b[0m       0.5248        \u001b[35m0.2635\u001b[0m  0.1390\n",
      "      9        \u001b[36m0.2259\u001b[0m       0.5248        \u001b[35m0.2434\u001b[0m  0.1393\n",
      "     10        \u001b[36m0.2191\u001b[0m       0.5248        \u001b[35m0.2285\u001b[0m  0.1415\n",
      "     11        \u001b[36m0.2133\u001b[0m       0.5248        \u001b[35m0.2175\u001b[0m  0.1391\n",
      "     12        \u001b[36m0.2083\u001b[0m       0.5248        \u001b[35m0.2092\u001b[0m  0.1543\n",
      "     13        \u001b[36m0.2038\u001b[0m       0.5248        \u001b[35m0.2028\u001b[0m  0.1335\n",
      "     14        \u001b[36m0.1998\u001b[0m       0.5248        \u001b[35m0.1976\u001b[0m  0.1306\n",
      "     15        \u001b[36m0.1962\u001b[0m       0.5248        \u001b[35m0.1933\u001b[0m  0.1294\n",
      "     16        \u001b[36m0.1930\u001b[0m       0.5248        \u001b[35m0.1896\u001b[0m  0.1292\n",
      "     17        \u001b[36m0.1900\u001b[0m       0.5248        \u001b[35m0.1864\u001b[0m  0.1319\n",
      "     18        \u001b[36m0.1873\u001b[0m       0.5248        \u001b[35m0.1835\u001b[0m  0.1280\n",
      "     19        \u001b[36m0.1847\u001b[0m       0.5248        \u001b[35m0.1809\u001b[0m  0.1255\n",
      "     20        \u001b[36m0.1824\u001b[0m       0.5248        \u001b[35m0.1785\u001b[0m  0.1258\n",
      "     21        \u001b[36m0.1802\u001b[0m       0.5248        \u001b[35m0.1763\u001b[0m  0.1247\n",
      "     22        \u001b[36m0.1780\u001b[0m       0.5248        \u001b[35m0.1742\u001b[0m  0.1416\n",
      "     23        \u001b[36m0.1760\u001b[0m       0.5248        \u001b[35m0.1722\u001b[0m  0.1247\n",
      "     24        \u001b[36m0.1741\u001b[0m       0.5248        \u001b[35m0.1703\u001b[0m  0.1337\n",
      "     25        \u001b[36m0.1725\u001b[0m       0.5248        \u001b[35m0.1686\u001b[0m  0.1329\n",
      "     26        \u001b[36m0.1710\u001b[0m       0.5248        \u001b[35m0.1671\u001b[0m  0.1293\n",
      "     27        \u001b[36m0.1696\u001b[0m       0.5248        \u001b[35m0.1656\u001b[0m  0.1340\n",
      "     28        \u001b[36m0.1682\u001b[0m       0.5248        \u001b[35m0.1643\u001b[0m  0.1324\n",
      "     29        \u001b[36m0.1670\u001b[0m       0.5248        \u001b[35m0.1630\u001b[0m  0.1289\n",
      "     30        \u001b[36m0.1658\u001b[0m       0.5248        \u001b[35m0.1618\u001b[0m  0.1353\n",
      "     31        \u001b[36m0.1646\u001b[0m       0.5248        \u001b[35m0.1607\u001b[0m  0.1244\n",
      "     32        \u001b[36m0.1636\u001b[0m       0.5248        \u001b[35m0.1597\u001b[0m  0.1315\n",
      "     33        \u001b[36m0.1626\u001b[0m       0.5248        \u001b[35m0.1587\u001b[0m  0.1300\n",
      "     34        \u001b[36m0.1617\u001b[0m       0.5248        \u001b[35m0.1578\u001b[0m  0.2678\n",
      "     35        \u001b[36m0.1608\u001b[0m       0.5248        \u001b[35m0.1570\u001b[0m  0.1330\n",
      "     36        \u001b[36m0.1600\u001b[0m       0.5248        \u001b[35m0.1562\u001b[0m  0.1309\n",
      "     37        \u001b[36m0.1592\u001b[0m       0.5248        \u001b[35m0.1554\u001b[0m  0.1306\n",
      "     38        \u001b[36m0.1585\u001b[0m       0.5248        \u001b[35m0.1547\u001b[0m  0.1298\n",
      "     39        \u001b[36m0.1577\u001b[0m       0.5248        \u001b[35m0.1540\u001b[0m  0.1509\n",
      "     40        \u001b[36m0.1570\u001b[0m       0.5248        \u001b[35m0.1534\u001b[0m  0.1449\n",
      "     41        \u001b[36m0.1564\u001b[0m       0.5248        \u001b[35m0.1527\u001b[0m  0.1281\n",
      "     42        \u001b[36m0.1558\u001b[0m       0.5248        \u001b[35m0.1522\u001b[0m  0.1212\n",
      "     43        \u001b[36m0.1552\u001b[0m       0.5248        \u001b[35m0.1516\u001b[0m  0.1262\n",
      "     44        \u001b[36m0.1547\u001b[0m       0.5248        \u001b[35m0.1511\u001b[0m  0.1277\n",
      "     45        \u001b[36m0.1541\u001b[0m       0.5248        \u001b[35m0.1506\u001b[0m  0.1192\n",
      "     46        \u001b[36m0.1536\u001b[0m       0.5248        \u001b[35m0.1501\u001b[0m  0.1332\n",
      "     47        \u001b[36m0.1531\u001b[0m       0.5248        \u001b[35m0.1497\u001b[0m  0.1206\n",
      "     48        \u001b[36m0.1527\u001b[0m       0.5248        \u001b[35m0.1492\u001b[0m  0.1284\n",
      "     49        \u001b[36m0.1522\u001b[0m       0.5248        \u001b[35m0.1488\u001b[0m  0.1238\n",
      "     50        \u001b[36m0.1518\u001b[0m       0.5248        \u001b[35m0.1484\u001b[0m  0.1219\n",
      "Xtrain pre augmentation: (2229, 5)\n",
      "Ytrain pre augmentation (2229, 1)\n",
      "Xtrain: (3171, 5)\n",
      "Ytrain (3171, 1)\n",
      "Xtest (558, 5)\n",
      "Ytest (558, 1)\n",
      "Re-initializing module.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.5611\u001b[0m       \u001b[32m0.5228\u001b[0m        \u001b[35m0.5483\u001b[0m  0.1225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2        \u001b[36m0.3810\u001b[0m       0.5228        \u001b[35m0.4948\u001b[0m  0.1303\n",
      "      3        \u001b[36m0.3306\u001b[0m       0.5228        \u001b[35m0.4407\u001b[0m  0.1285\n",
      "      4        \u001b[36m0.2990\u001b[0m       0.5228        \u001b[35m0.3902\u001b[0m  0.1160\n",
      "      5        \u001b[36m0.2782\u001b[0m       0.5228        \u001b[35m0.3480\u001b[0m  0.1151\n",
      "      6        \u001b[36m0.2630\u001b[0m       0.5228        \u001b[35m0.3154\u001b[0m  0.1190\n",
      "      7        \u001b[36m0.2515\u001b[0m       0.5228        \u001b[35m0.2903\u001b[0m  0.1197\n",
      "      8        \u001b[36m0.2417\u001b[0m       0.5228        \u001b[35m0.2876\u001b[0m  0.1190\n",
      "      9        \u001b[36m0.2359\u001b[0m       0.5228        \u001b[35m0.2557\u001b[0m  0.1204\n",
      "     10        \u001b[36m0.2286\u001b[0m       0.5228        \u001b[35m0.2433\u001b[0m  0.1190\n",
      "     11        \u001b[36m0.2227\u001b[0m       0.5228        \u001b[35m0.2325\u001b[0m  0.1202\n",
      "     12        \u001b[36m0.2173\u001b[0m       0.5228        \u001b[35m0.2228\u001b[0m  0.1059\n",
      "     13        \u001b[36m0.2125\u001b[0m       0.5228        \u001b[35m0.2139\u001b[0m  0.1019\n",
      "     14        \u001b[36m0.2081\u001b[0m       0.5228        \u001b[35m0.2062\u001b[0m  0.0959\n",
      "     15        \u001b[36m0.2041\u001b[0m       0.5228        \u001b[35m0.1995\u001b[0m  0.1023\n",
      "     16        \u001b[36m0.2004\u001b[0m       0.5228        \u001b[35m0.1938\u001b[0m  0.0990\n",
      "     17        \u001b[36m0.1969\u001b[0m       0.5228        \u001b[35m0.1887\u001b[0m  0.0960\n",
      "     18        \u001b[36m0.1935\u001b[0m       0.5228        \u001b[35m0.1848\u001b[0m  0.0994\n",
      "     19        \u001b[36m0.1907\u001b[0m       0.5228        \u001b[35m0.1815\u001b[0m  0.1062\n",
      "     20        \u001b[36m0.1879\u001b[0m       0.5228        \u001b[35m0.1791\u001b[0m  0.1107\n",
      "     21        \u001b[36m0.1855\u001b[0m       0.5228        \u001b[35m0.1754\u001b[0m  0.1248\n",
      "     22        \u001b[36m0.1833\u001b[0m       0.5228        \u001b[35m0.1733\u001b[0m  0.1175\n",
      "     23        \u001b[36m0.1816\u001b[0m       0.5228        \u001b[35m0.1705\u001b[0m  0.1181\n",
      "     24        \u001b[36m0.1792\u001b[0m       0.5228        \u001b[35m0.1687\u001b[0m  0.1203\n",
      "     25        \u001b[36m0.1773\u001b[0m       0.5228        \u001b[35m0.1671\u001b[0m  0.1213\n",
      "     26        \u001b[36m0.1763\u001b[0m       0.5228        \u001b[35m0.1647\u001b[0m  0.1234\n",
      "     27        \u001b[36m0.1744\u001b[0m       0.5228        \u001b[35m0.1627\u001b[0m  0.1281\n",
      "     28        \u001b[36m0.1742\u001b[0m       0.5228        \u001b[35m0.1605\u001b[0m  0.1149\n",
      "     29        \u001b[36m0.1723\u001b[0m       0.5228        \u001b[35m0.1597\u001b[0m  0.1191\n",
      "     30        \u001b[36m0.1709\u001b[0m       0.5228        \u001b[35m0.1581\u001b[0m  0.1240\n",
      "     31        \u001b[36m0.1694\u001b[0m       0.5228        \u001b[35m0.1565\u001b[0m  0.1377\n",
      "     32        \u001b[36m0.1682\u001b[0m       0.5228        \u001b[35m0.1553\u001b[0m  0.1308\n",
      "     33        \u001b[36m0.1673\u001b[0m       0.5228        \u001b[35m0.1547\u001b[0m  0.1130\n",
      "     34        \u001b[36m0.1663\u001b[0m       0.5228        \u001b[35m0.1537\u001b[0m  0.1145\n",
      "     35        \u001b[36m0.1657\u001b[0m       0.5228        \u001b[35m0.1526\u001b[0m  0.1231\n",
      "     36        \u001b[36m0.1648\u001b[0m       0.5228        \u001b[35m0.1515\u001b[0m  0.1256\n",
      "     37        \u001b[36m0.1639\u001b[0m       0.5228        \u001b[35m0.1505\u001b[0m  0.1173\n",
      "     38        \u001b[36m0.1630\u001b[0m       0.5228        \u001b[35m0.1495\u001b[0m  0.1042\n",
      "     39        \u001b[36m0.1622\u001b[0m       0.5228        \u001b[35m0.1486\u001b[0m  0.2184\n",
      "     40        \u001b[36m0.1614\u001b[0m       0.5228        \u001b[35m0.1477\u001b[0m  0.1208\n",
      "     41        \u001b[36m0.1607\u001b[0m       0.5228        \u001b[35m0.1471\u001b[0m  0.1052\n",
      "     42        \u001b[36m0.1600\u001b[0m       0.5228        \u001b[35m0.1464\u001b[0m  0.1126\n",
      "     43        \u001b[36m0.1593\u001b[0m       0.5228        \u001b[35m0.1457\u001b[0m  0.1367\n",
      "     44        \u001b[36m0.1587\u001b[0m       0.5228        \u001b[35m0.1451\u001b[0m  0.1087\n",
      "     45        \u001b[36m0.1580\u001b[0m       0.5228        \u001b[35m0.1441\u001b[0m  0.1178\n",
      "     46        \u001b[36m0.1574\u001b[0m       0.5228        \u001b[35m0.1435\u001b[0m  0.1206\n",
      "     47        \u001b[36m0.1567\u001b[0m       0.5228        \u001b[35m0.1425\u001b[0m  0.1204\n",
      "     48        \u001b[36m0.1561\u001b[0m       0.5228        \u001b[35m0.1421\u001b[0m  0.1185\n",
      "     49        \u001b[36m0.1554\u001b[0m       0.5228        \u001b[35m0.1412\u001b[0m  0.1203\n",
      "     50        \u001b[36m0.1548\u001b[0m       0.5228        \u001b[35m0.1407\u001b[0m  0.1151\n",
      "Xtrain pre augmentation: (2230, 5)\n",
      "Ytrain pre augmentation (2230, 1)\n",
      "Xtrain: (3230, 5)\n",
      "Ytrain (3230, 1)\n",
      "Xtest (557, 5)\n",
      "Ytest (557, 1)\n",
      "Re-initializing module.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.4684\u001b[0m       \u001b[32m0.5186\u001b[0m        \u001b[35m0.5574\u001b[0m  0.1198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2        \u001b[36m0.3444\u001b[0m       0.5186        \u001b[35m0.5059\u001b[0m  0.1356\n",
      "      3        \u001b[36m0.3009\u001b[0m       0.5186        \u001b[35m0.4494\u001b[0m  0.1235\n",
      "      4        \u001b[36m0.2749\u001b[0m       0.5186        \u001b[35m0.3948\u001b[0m  0.1201\n",
      "      5        \u001b[36m0.2565\u001b[0m       0.5186        \u001b[35m0.3459\u001b[0m  0.1197\n",
      "      6        \u001b[36m0.2428\u001b[0m       0.5186        \u001b[35m0.3056\u001b[0m  0.1203\n",
      "      7        \u001b[36m0.2334\u001b[0m       0.5186        \u001b[35m0.2742\u001b[0m  0.1165\n",
      "      8        \u001b[36m0.2245\u001b[0m       0.5186        \u001b[35m0.2497\u001b[0m  0.1130\n",
      "      9        \u001b[36m0.2167\u001b[0m       0.5186        \u001b[35m0.2316\u001b[0m  0.1122\n",
      "     10        \u001b[36m0.2103\u001b[0m       0.5186        \u001b[35m0.2180\u001b[0m  0.1166\n",
      "     11        \u001b[36m0.2050\u001b[0m       0.5186        \u001b[35m0.2074\u001b[0m  0.1061\n",
      "     12        \u001b[36m0.2002\u001b[0m       0.5186        \u001b[35m0.1990\u001b[0m  0.1278\n",
      "     13        \u001b[36m0.1960\u001b[0m       0.5186        \u001b[35m0.1924\u001b[0m  0.1259\n",
      "     14        \u001b[36m0.1923\u001b[0m       0.5186        \u001b[35m0.1871\u001b[0m  0.1081\n",
      "     15        \u001b[36m0.1888\u001b[0m       0.5186        \u001b[35m0.1829\u001b[0m  0.1050\n",
      "     16        \u001b[36m0.1857\u001b[0m       0.5186        \u001b[35m0.1795\u001b[0m  0.1060\n",
      "     17        \u001b[36m0.1829\u001b[0m       0.5186        \u001b[35m0.1767\u001b[0m  0.1058\n",
      "     18        \u001b[36m0.1805\u001b[0m       0.5186        \u001b[35m0.1742\u001b[0m  0.1043\n",
      "     19        \u001b[36m0.1782\u001b[0m       0.5186        \u001b[35m0.1721\u001b[0m  0.1095\n",
      "     20        \u001b[36m0.1762\u001b[0m       0.5186        \u001b[35m0.1702\u001b[0m  0.1016\n",
      "     21        \u001b[36m0.1743\u001b[0m       0.5186        \u001b[35m0.1686\u001b[0m  0.1080\n",
      "     22        \u001b[36m0.1726\u001b[0m       0.5186        \u001b[35m0.1669\u001b[0m  0.1030\n",
      "     23        \u001b[36m0.1709\u001b[0m       0.5186        \u001b[35m0.1654\u001b[0m  0.0973\n",
      "     24        \u001b[36m0.1694\u001b[0m       0.5186        \u001b[35m0.1639\u001b[0m  0.0977\n",
      "     25        \u001b[36m0.1679\u001b[0m       0.5186        \u001b[35m0.1625\u001b[0m  0.0955\n",
      "     26        \u001b[36m0.1665\u001b[0m       0.5186        \u001b[35m0.1611\u001b[0m  0.0979\n",
      "     27        \u001b[36m0.1653\u001b[0m       0.5186        \u001b[35m0.1598\u001b[0m  0.0984\n",
      "     28        \u001b[36m0.1641\u001b[0m       0.5186        \u001b[35m0.1586\u001b[0m  0.0969\n",
      "     29        \u001b[36m0.1630\u001b[0m       0.5186        \u001b[35m0.1575\u001b[0m  0.0983\n",
      "     30        \u001b[36m0.1620\u001b[0m       0.5186        \u001b[35m0.1565\u001b[0m  0.0965\n",
      "     31        \u001b[36m0.1611\u001b[0m       0.5186        \u001b[35m0.1555\u001b[0m  0.0973\n",
      "     32        \u001b[36m0.1601\u001b[0m       0.5186        \u001b[35m0.1546\u001b[0m  0.0969\n",
      "     33        \u001b[36m0.1593\u001b[0m       0.5186        \u001b[35m0.1537\u001b[0m  0.1051\n",
      "     34        \u001b[36m0.1585\u001b[0m       0.5186        \u001b[35m0.1529\u001b[0m  0.0976\n",
      "     35        \u001b[36m0.1577\u001b[0m       0.5186        \u001b[35m0.1521\u001b[0m  0.0978\n",
      "     36        \u001b[36m0.1569\u001b[0m       0.5186        \u001b[35m0.1513\u001b[0m  0.0953\n",
      "     37        \u001b[36m0.1562\u001b[0m       0.5186        \u001b[35m0.1506\u001b[0m  0.0971\n",
      "     38        \u001b[36m0.1555\u001b[0m       0.5186        \u001b[35m0.1499\u001b[0m  0.0940\n",
      "     39        \u001b[36m0.1548\u001b[0m       0.5186        \u001b[35m0.1492\u001b[0m  0.1081\n",
      "     40        \u001b[36m0.1542\u001b[0m       0.5186        \u001b[35m0.1486\u001b[0m  0.0969\n",
      "     41        \u001b[36m0.1535\u001b[0m       0.5186        \u001b[35m0.1479\u001b[0m  0.0951\n",
      "     42        \u001b[36m0.1530\u001b[0m       0.5186        \u001b[35m0.1473\u001b[0m  0.1033\n",
      "     43        \u001b[36m0.1524\u001b[0m       0.5186        \u001b[35m0.1468\u001b[0m  0.0979\n",
      "     44        \u001b[36m0.1519\u001b[0m       0.5186        \u001b[35m0.1462\u001b[0m  0.1047\n",
      "     45        \u001b[36m0.1514\u001b[0m       0.5186        \u001b[35m0.1457\u001b[0m  0.0995\n",
      "     46        \u001b[36m0.1508\u001b[0m       0.5186        \u001b[35m0.1452\u001b[0m  0.1966\n",
      "     47        \u001b[36m0.1504\u001b[0m       0.5186        \u001b[35m0.1447\u001b[0m  0.1008\n",
      "     48        \u001b[36m0.1499\u001b[0m       0.5186        \u001b[35m0.1442\u001b[0m  0.1002\n",
      "     49        \u001b[36m0.1494\u001b[0m       0.5186        \u001b[35m0.1438\u001b[0m  0.1001\n",
      "     50        \u001b[36m0.1490\u001b[0m       0.5186        \u001b[35m0.1433\u001b[0m  0.1013\n",
      "Xtrain pre augmentation: (2230, 5)\n",
      "Ytrain pre augmentation (2230, 1)\n",
      "Xtrain: (3244, 5)\n",
      "Ytrain (3244, 1)\n",
      "Xtest (557, 5)\n",
      "Ytest (557, 1)\n",
      "Re-initializing module.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.5099\u001b[0m       \u001b[32m0.5208\u001b[0m        \u001b[35m0.5290\u001b[0m  0.1022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2        \u001b[36m0.3513\u001b[0m       0.5208        \u001b[35m0.4888\u001b[0m  0.1210\n",
      "      3        \u001b[36m0.3035\u001b[0m       0.5208        \u001b[35m0.4419\u001b[0m  0.1004\n",
      "      4        \u001b[36m0.2785\u001b[0m       0.5208        \u001b[35m0.4047\u001b[0m  0.0999\n",
      "      5        \u001b[36m0.2614\u001b[0m       0.5208        \u001b[35m0.3582\u001b[0m  0.0966\n",
      "      6        \u001b[36m0.2484\u001b[0m       0.5208        \u001b[35m0.3214\u001b[0m  0.0978\n",
      "      7        \u001b[36m0.2382\u001b[0m       0.5208        \u001b[35m0.2921\u001b[0m  0.1163\n",
      "      8        \u001b[36m0.2297\u001b[0m       0.5208        \u001b[35m0.2687\u001b[0m  0.0969\n",
      "      9        \u001b[36m0.2225\u001b[0m       0.5208        \u001b[35m0.2500\u001b[0m  0.1086\n",
      "     10        \u001b[36m0.2162\u001b[0m       0.5208        \u001b[35m0.2353\u001b[0m  0.1528\n",
      "     11        \u001b[36m0.2107\u001b[0m       0.5208        \u001b[35m0.2240\u001b[0m  0.2449\n",
      "     12        \u001b[36m0.2059\u001b[0m       0.5208        \u001b[35m0.2152\u001b[0m  0.1052\n",
      "     13        \u001b[36m0.2015\u001b[0m       0.5208        \u001b[35m0.2082\u001b[0m  0.0990\n",
      "     14        \u001b[36m0.1974\u001b[0m       0.5208        \u001b[35m0.2024\u001b[0m  0.1076\n",
      "     15        \u001b[36m0.1937\u001b[0m       0.5208        \u001b[35m0.1975\u001b[0m  0.1106\n",
      "     16        \u001b[36m0.1903\u001b[0m       0.5208        \u001b[35m0.1932\u001b[0m  0.1148\n",
      "     17        \u001b[36m0.1871\u001b[0m       0.5208        \u001b[35m0.1893\u001b[0m  0.1137\n",
      "     18        \u001b[36m0.1841\u001b[0m       0.5208        \u001b[35m0.1857\u001b[0m  0.1126\n",
      "     19        \u001b[36m0.1813\u001b[0m       0.5208        \u001b[35m0.1828\u001b[0m  0.2072\n",
      "     20        \u001b[36m0.1788\u001b[0m       0.5208        \u001b[35m0.1800\u001b[0m  0.1137\n",
      "     21        \u001b[36m0.1765\u001b[0m       0.5208        \u001b[35m0.1776\u001b[0m  0.1120\n",
      "     22        \u001b[36m0.1743\u001b[0m       0.5208        \u001b[35m0.1753\u001b[0m  0.1199\n",
      "     23        \u001b[36m0.1723\u001b[0m       0.5208        \u001b[35m0.1733\u001b[0m  0.1126\n",
      "     24        \u001b[36m0.1705\u001b[0m       0.5208        \u001b[35m0.1713\u001b[0m  0.1233\n",
      "     25        \u001b[36m0.1688\u001b[0m       0.5208        \u001b[35m0.1695\u001b[0m  0.1126\n",
      "     26        \u001b[36m0.1672\u001b[0m       0.5208        \u001b[35m0.1677\u001b[0m  0.1113\n",
      "     27        \u001b[36m0.1656\u001b[0m       0.5208        \u001b[35m0.1660\u001b[0m  0.1268\n",
      "     28        \u001b[36m0.1641\u001b[0m       0.5208        \u001b[35m0.1645\u001b[0m  0.1278\n",
      "     29        \u001b[36m0.1628\u001b[0m       0.5208        \u001b[35m0.1631\u001b[0m  0.1276\n",
      "     30        \u001b[36m0.1615\u001b[0m       0.5208        \u001b[35m0.1618\u001b[0m  0.1356\n",
      "     31        \u001b[36m0.1604\u001b[0m       0.5208        \u001b[35m0.1606\u001b[0m  0.1201\n",
      "     32        \u001b[36m0.1593\u001b[0m       0.5208        \u001b[35m0.1594\u001b[0m  0.1250\n",
      "     33        \u001b[36m0.1583\u001b[0m       0.5208        \u001b[35m0.1583\u001b[0m  0.1331\n",
      "     34        \u001b[36m0.1573\u001b[0m       0.5208        \u001b[35m0.1572\u001b[0m  0.1343\n",
      "     35        \u001b[36m0.1564\u001b[0m       0.5208        \u001b[35m0.1561\u001b[0m  0.1324\n",
      "     36        \u001b[36m0.1555\u001b[0m       0.5208        \u001b[35m0.1551\u001b[0m  0.1464\n",
      "     37        \u001b[36m0.1546\u001b[0m       0.5208        \u001b[35m0.1541\u001b[0m  0.1284\n",
      "     38        \u001b[36m0.1538\u001b[0m       0.5208        \u001b[35m0.1532\u001b[0m  0.1260\n",
      "     39        \u001b[36m0.1530\u001b[0m       0.5208        \u001b[35m0.1523\u001b[0m  0.1119\n",
      "     40        \u001b[36m0.1523\u001b[0m       0.5208        \u001b[35m0.1515\u001b[0m  0.1115\n",
      "     41        \u001b[36m0.1516\u001b[0m       0.5208        \u001b[35m0.1507\u001b[0m  0.0982\n",
      "     42        \u001b[36m0.1509\u001b[0m       0.5208        \u001b[35m0.1498\u001b[0m  0.0976\n",
      "     43        \u001b[36m0.1502\u001b[0m       0.5208        \u001b[35m0.1491\u001b[0m  0.0977\n",
      "     44        \u001b[36m0.1497\u001b[0m       0.5208        \u001b[35m0.1484\u001b[0m  0.0969\n",
      "     45        \u001b[36m0.1491\u001b[0m       0.5208        \u001b[35m0.1477\u001b[0m  0.0991\n",
      "     46        \u001b[36m0.1486\u001b[0m       0.5208        \u001b[35m0.1471\u001b[0m  0.1987\n",
      "     47        \u001b[36m0.1481\u001b[0m       0.5208        \u001b[35m0.1465\u001b[0m  0.1006\n",
      "     48        \u001b[36m0.1476\u001b[0m       0.5208        \u001b[35m0.1459\u001b[0m  0.1086\n",
      "     49        \u001b[36m0.1472\u001b[0m       0.5208        \u001b[35m0.1454\u001b[0m  0.0969\n",
      "     50        \u001b[36m0.1467\u001b[0m       0.5208        \u001b[35m0.1449\u001b[0m  0.1022\n",
      "Xtrain pre augmentation: (2230, 5)\n",
      "Ytrain pre augmentation (2230, 1)\n",
      "Xtrain: (3204, 5)\n",
      "Ytrain (3204, 1)\n",
      "Xtest (557, 5)\n",
      "Ytest (557, 1)\n",
      "Re-initializing module.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6125\u001b[0m       \u001b[32m0.5179\u001b[0m        \u001b[35m0.5624\u001b[0m  0.0987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2        \u001b[36m0.3802\u001b[0m       0.5179        \u001b[35m0.5181\u001b[0m  0.1079\n",
      "      3        \u001b[36m0.3262\u001b[0m       0.5179        \u001b[35m0.4695\u001b[0m  0.1037\n",
      "      4        \u001b[36m0.2965\u001b[0m       0.5179        \u001b[35m0.4219\u001b[0m  0.0978\n",
      "      5        \u001b[36m0.2756\u001b[0m       0.5179        \u001b[35m0.3795\u001b[0m  0.0993\n",
      "      6        \u001b[36m0.2595\u001b[0m       0.5179        \u001b[35m0.3448\u001b[0m  0.0964\n",
      "      7        \u001b[36m0.2475\u001b[0m       0.5179        \u001b[35m0.3166\u001b[0m  0.1045\n",
      "      8        \u001b[36m0.2373\u001b[0m       0.5179        \u001b[35m0.2937\u001b[0m  0.0989\n",
      "      9        \u001b[36m0.2286\u001b[0m       0.5179        \u001b[35m0.2745\u001b[0m  0.0960\n",
      "     10        \u001b[36m0.2212\u001b[0m       0.5179        \u001b[35m0.2587\u001b[0m  0.0954\n",
      "     11        \u001b[36m0.2149\u001b[0m       0.5179        \u001b[35m0.2458\u001b[0m  0.0965\n",
      "     12        \u001b[36m0.2098\u001b[0m       0.5179        \u001b[35m0.2359\u001b[0m  0.0971\n",
      "     13        \u001b[36m0.2053\u001b[0m       0.5179        \u001b[35m0.2282\u001b[0m  0.0995\n",
      "     14        \u001b[36m0.2014\u001b[0m       0.5179        \u001b[35m0.2222\u001b[0m  0.0950\n",
      "     15        \u001b[36m0.1978\u001b[0m       0.5179        \u001b[35m0.2173\u001b[0m  0.1040\n",
      "     16        \u001b[36m0.1945\u001b[0m       0.5179        \u001b[35m0.2132\u001b[0m  0.0976\n",
      "     17        \u001b[36m0.1915\u001b[0m       0.5179        \u001b[35m0.2097\u001b[0m  0.1007\n",
      "     18        \u001b[36m0.1888\u001b[0m       0.5179        \u001b[35m0.2066\u001b[0m  0.0972\n",
      "     19        \u001b[36m0.1862\u001b[0m       0.5179        \u001b[35m0.2038\u001b[0m  0.0958\n",
      "     20        \u001b[36m0.1838\u001b[0m       0.5179        \u001b[35m0.2012\u001b[0m  0.0969\n",
      "     21        \u001b[36m0.1816\u001b[0m       0.5179        \u001b[35m0.1988\u001b[0m  0.0944\n",
      "     22        \u001b[36m0.1795\u001b[0m       0.5179        \u001b[35m0.1965\u001b[0m  0.0953\n",
      "     23        \u001b[36m0.1776\u001b[0m       0.5179        \u001b[35m0.1945\u001b[0m  0.0972\n",
      "     24        \u001b[36m0.1757\u001b[0m       0.5179        \u001b[35m0.1926\u001b[0m  0.0946\n",
      "     25        \u001b[36m0.1740\u001b[0m       0.5179        \u001b[35m0.1908\u001b[0m  0.1024\n",
      "     26        \u001b[36m0.1724\u001b[0m       0.5179        \u001b[35m0.1891\u001b[0m  0.1123\n",
      "     27        \u001b[36m0.1709\u001b[0m       0.5179        \u001b[35m0.1875\u001b[0m  0.1000\n",
      "     28        \u001b[36m0.1694\u001b[0m       0.5179        \u001b[35m0.1859\u001b[0m  0.1129\n",
      "     29        \u001b[36m0.1681\u001b[0m       0.5179        \u001b[35m0.1844\u001b[0m  0.0964\n",
      "     30        \u001b[36m0.1668\u001b[0m       0.5179        \u001b[35m0.1830\u001b[0m  0.1116\n",
      "     31        \u001b[36m0.1655\u001b[0m       0.5179        \u001b[35m0.1816\u001b[0m  0.0973\n",
      "     32        \u001b[36m0.1643\u001b[0m       0.5179        \u001b[35m0.1803\u001b[0m  0.0974\n",
      "     33        \u001b[36m0.1632\u001b[0m       0.5179        \u001b[35m0.1790\u001b[0m  0.0960\n",
      "     34        \u001b[36m0.1621\u001b[0m       0.5179        \u001b[35m0.1778\u001b[0m  0.0971\n",
      "     35        \u001b[36m0.1611\u001b[0m       0.5179        \u001b[35m0.1766\u001b[0m  0.0992\n",
      "     36        \u001b[36m0.1601\u001b[0m       0.5179        \u001b[35m0.1755\u001b[0m  0.0982\n",
      "     37        \u001b[36m0.1591\u001b[0m       0.5179        \u001b[35m0.1744\u001b[0m  0.0955\n",
      "     38        \u001b[36m0.1582\u001b[0m       0.5179        \u001b[35m0.1734\u001b[0m  0.1026\n",
      "     39        \u001b[36m0.1574\u001b[0m       0.5179        \u001b[35m0.1724\u001b[0m  0.0980\n",
      "     40        \u001b[36m0.1566\u001b[0m       0.5179        \u001b[35m0.1715\u001b[0m  0.0948\n",
      "     41        \u001b[36m0.1558\u001b[0m       0.5179        \u001b[35m0.1707\u001b[0m  0.0982\n",
      "     42        \u001b[36m0.1551\u001b[0m       0.5179        \u001b[35m0.1699\u001b[0m  0.0960\n",
      "     43        \u001b[36m0.1544\u001b[0m       0.5179        \u001b[35m0.1690\u001b[0m  0.0963\n",
      "     44        \u001b[36m0.1537\u001b[0m       0.5179        \u001b[35m0.1683\u001b[0m  0.0962\n",
      "     45        \u001b[36m0.1531\u001b[0m       0.5179        \u001b[35m0.1677\u001b[0m  0.1001\n",
      "     46        \u001b[36m0.1525\u001b[0m       0.5179        \u001b[35m0.1671\u001b[0m  0.0967\n",
      "     47        \u001b[36m0.1519\u001b[0m       0.5179        \u001b[35m0.1664\u001b[0m  0.1038\n",
      "     48        \u001b[36m0.1514\u001b[0m       0.5179        \u001b[35m0.1658\u001b[0m  0.1980\n",
      "     49        \u001b[36m0.1508\u001b[0m       0.5179        \u001b[35m0.1653\u001b[0m  0.1044\n",
      "     50        \u001b[36m0.1503\u001b[0m       0.5179        \u001b[35m0.1647\u001b[0m  0.0975\n",
      "Xtrain pre augmentation: (12941, 6)\n",
      "Ytrain pre augmentation (12941, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain: (19119, 6)\n",
      "Ytrain (19119, 1)\n",
      "Xtest (3236, 6)\n",
      "Ytest (3236, 1)\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.3045\u001b[0m       \u001b[32m0.4725\u001b[0m        \u001b[35m0.3345\u001b[0m  0.5508\n",
      "      2        \u001b[36m0.2295\u001b[0m       0.4725        \u001b[35m0.2307\u001b[0m  0.5659\n",
      "      3        \u001b[36m0.2077\u001b[0m       0.4725        \u001b[35m0.2071\u001b[0m  0.5822\n",
      "      4        \u001b[36m0.1965\u001b[0m       0.4725        \u001b[35m0.1946\u001b[0m  0.5950\n",
      "      5        \u001b[36m0.1874\u001b[0m       0.4725        \u001b[35m0.1871\u001b[0m  0.5685\n",
      "      6        \u001b[36m0.1808\u001b[0m       0.4725        \u001b[35m0.1819\u001b[0m  0.5750\n",
      "      7        \u001b[36m0.1758\u001b[0m       0.4725        \u001b[35m0.1773\u001b[0m  0.5488\n",
      "      8        \u001b[36m0.1710\u001b[0m       0.4725        \u001b[35m0.1731\u001b[0m  0.5485\n",
      "      9        \u001b[36m0.1682\u001b[0m       0.4725        \u001b[35m0.1702\u001b[0m  0.5525\n",
      "     10        \u001b[36m0.1650\u001b[0m       0.4725        \u001b[35m0.1678\u001b[0m  0.5591\n",
      "     11        \u001b[36m0.1619\u001b[0m       0.4725        \u001b[35m0.1643\u001b[0m  0.5504\n",
      "     12        \u001b[36m0.1604\u001b[0m       0.4725        0.1647  0.5397\n",
      "     13        \u001b[36m0.1585\u001b[0m       0.4725        \u001b[35m0.1619\u001b[0m  0.5503\n",
      "     14        \u001b[36m0.1575\u001b[0m       0.4725        \u001b[35m0.1608\u001b[0m  0.5502\n",
      "     15        \u001b[36m0.1561\u001b[0m       0.4725        \u001b[35m0.1594\u001b[0m  0.5534\n",
      "     16        \u001b[36m0.1545\u001b[0m       0.4725        \u001b[35m0.1580\u001b[0m  0.6035\n",
      "     17        \u001b[36m0.1531\u001b[0m       0.4725        \u001b[35m0.1566\u001b[0m  0.7209\n",
      "     18        \u001b[36m0.1516\u001b[0m       0.4725        \u001b[35m0.1546\u001b[0m  0.7376\n",
      "     19        \u001b[36m0.1513\u001b[0m       0.4725        0.9613  0.7447\n",
      "     20        0.1520       0.4725        0.1553  0.6734\n",
      "     21        \u001b[36m0.1500\u001b[0m       0.4725        \u001b[35m0.1539\u001b[0m  0.6924\n",
      "     22        \u001b[36m0.1486\u001b[0m       0.4725        \u001b[35m0.1526\u001b[0m  0.6799\n",
      "     23        \u001b[36m0.1480\u001b[0m       0.4725        \u001b[35m0.1513\u001b[0m  0.6779\n",
      "     24        \u001b[36m0.1477\u001b[0m       0.4725        \u001b[35m0.1508\u001b[0m  0.6846\n",
      "     25        \u001b[36m0.1461\u001b[0m       0.4725        \u001b[35m0.1492\u001b[0m  0.6979\n",
      "     26        \u001b[36m0.1451\u001b[0m       0.4725        \u001b[35m0.1486\u001b[0m  0.6289\n",
      "     27        \u001b[36m0.1443\u001b[0m       0.4725        \u001b[35m0.1476\u001b[0m  0.6865\n",
      "     28        \u001b[36m0.1432\u001b[0m       0.4725        \u001b[35m0.1464\u001b[0m  0.6788\n",
      "     29        \u001b[36m0.1424\u001b[0m       0.4725        0.2019  0.6848\n",
      "     30        \u001b[36m0.1420\u001b[0m       0.4725        0.1469  0.6241\n",
      "     31        \u001b[36m0.1416\u001b[0m       0.4725        0.1480  0.6227\n",
      "     32        \u001b[36m0.1406\u001b[0m       0.4725        \u001b[35m0.1442\u001b[0m  0.5654\n",
      "     33        0.1412       0.4725        \u001b[35m0.1441\u001b[0m  0.5640\n",
      "     34        \u001b[36m0.1398\u001b[0m       0.4725        \u001b[35m0.1432\u001b[0m  0.5512\n",
      "     35        \u001b[36m0.1389\u001b[0m       0.4725        \u001b[35m0.1428\u001b[0m  0.5653\n",
      "     36        \u001b[36m0.1387\u001b[0m       0.4725        0.1464  0.5525\n",
      "     37        \u001b[36m0.1379\u001b[0m       0.4725        0.1530  0.5448\n",
      "     38        \u001b[36m0.1374\u001b[0m       0.4725        0.3731  0.5553\n",
      "     39        0.1385       0.4725        0.1432  0.5523\n",
      "     40        0.1374       0.4725        0.1471  0.5533\n",
      "     41        \u001b[36m0.1367\u001b[0m       0.4725        \u001b[35m0.1405\u001b[0m  0.5544\n",
      "     42        \u001b[36m0.1366\u001b[0m       0.4725        0.1407  0.6619\n",
      "     43        \u001b[36m0.1347\u001b[0m       0.4725        \u001b[35m0.1393\u001b[0m  0.5506\n",
      "     44        \u001b[36m0.1347\u001b[0m       0.4725        \u001b[35m0.1375\u001b[0m  0.5405\n",
      "     45        \u001b[36m0.1335\u001b[0m       0.4725        \u001b[35m0.1360\u001b[0m  0.5627\n",
      "     46        \u001b[36m0.1323\u001b[0m       0.4725        \u001b[35m0.1344\u001b[0m  0.5592\n",
      "     47        \u001b[36m0.1316\u001b[0m       0.4725        0.1344  0.5457\n",
      "     48        \u001b[36m0.1306\u001b[0m       0.4725        \u001b[35m0.1336\u001b[0m  0.5606\n",
      "     49        0.1315       0.4725        \u001b[35m0.1330\u001b[0m  0.5675\n",
      "     50        \u001b[36m0.1299\u001b[0m       0.4725        \u001b[35m0.1329\u001b[0m  0.5499\n",
      "Xtrain pre augmentation: (12941, 6)\n",
      "Ytrain pre augmentation (12941, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain: (19612, 6)\n",
      "Ytrain (19612, 1)\n",
      "Xtest (3236, 6)\n",
      "Ytest (3236, 1)\n",
      "Re-initializing module.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.2844\u001b[0m       \u001b[32m0.4741\u001b[0m        \u001b[35m0.3335\u001b[0m  0.5480\n",
      "      2        \u001b[36m0.2156\u001b[0m       0.4741        \u001b[35m0.2321\u001b[0m  0.5439\n",
      "      3        \u001b[36m0.1989\u001b[0m       0.4741        \u001b[35m0.2097\u001b[0m  0.5626\n",
      "      4        \u001b[36m0.1902\u001b[0m       0.4741        \u001b[35m0.1870\u001b[0m  0.5828\n",
      "      5        \u001b[36m0.1829\u001b[0m       0.4741        0.1904  0.6046\n",
      "      6        \u001b[36m0.1813\u001b[0m       0.4741        0.1947  0.5462\n",
      "      7        \u001b[36m0.1762\u001b[0m       0.4741        0.1874  0.5636\n",
      "      8        0.1775       0.4741        \u001b[35m0.1792\u001b[0m  0.8202\n",
      "      9        \u001b[36m0.1711\u001b[0m       0.4741        0.1856  0.6367\n",
      "     10        \u001b[36m0.1685\u001b[0m       0.4741        \u001b[35m0.1705\u001b[0m  0.6166\n",
      "     11        \u001b[36m0.1679\u001b[0m       0.4741        0.1870  0.6081\n",
      "     12        \u001b[36m0.1679\u001b[0m       0.4741        0.1844  0.6244\n",
      "     13        \u001b[36m0.1650\u001b[0m       0.4741        \u001b[35m0.1671\u001b[0m  0.6084\n",
      "     14        \u001b[36m0.1621\u001b[0m       0.4741        0.1805  0.7244\n",
      "     15        \u001b[36m0.1600\u001b[0m       0.4741        \u001b[35m0.1653\u001b[0m  0.6153\n",
      "     16        \u001b[36m0.1564\u001b[0m       0.4741        \u001b[35m0.1639\u001b[0m  0.6284\n",
      "     17        \u001b[36m0.1550\u001b[0m       0.4741        0.1701  0.6093\n",
      "     18        0.1580       0.4741        0.1825  0.6133\n",
      "     19        0.1573       0.4741        \u001b[35m0.1613\u001b[0m  0.6115\n",
      "     20        0.1577       0.4741        0.1835  0.5997\n",
      "     21        0.1624       0.4741        0.1666  0.6255\n",
      "     22        0.1591       0.4741        0.1639  0.6153\n",
      "     23        0.1570       0.4741        0.1617  0.6390\n",
      "     24        0.1554       0.4741        \u001b[35m0.1602\u001b[0m  0.6457\n",
      "     25        \u001b[36m0.1533\u001b[0m       0.4741        \u001b[35m0.1578\u001b[0m  0.7154\n",
      "     26        \u001b[36m0.1519\u001b[0m       0.4741        0.1580  0.5960\n",
      "     27        \u001b[36m0.1504\u001b[0m       0.4741        \u001b[35m0.1546\u001b[0m  0.6104\n",
      "     28        \u001b[36m0.1486\u001b[0m       0.4741        \u001b[35m0.1534\u001b[0m  0.6170\n",
      "     29        \u001b[36m0.1483\u001b[0m       0.4741        0.1544  0.6144\n",
      "     30        \u001b[36m0.1473\u001b[0m       0.4741        \u001b[35m0.1517\u001b[0m  0.6237\n",
      "     31        \u001b[36m0.1464\u001b[0m       0.4741        0.1525  0.5940\n",
      "     32        \u001b[36m0.1460\u001b[0m       0.4741        \u001b[35m0.1502\u001b[0m  0.6095\n",
      "     33        0.1471       0.4741        0.1517  0.6057\n",
      "     34        0.1463       0.4741        0.1511  0.8400\n",
      "     35        \u001b[36m0.1453\u001b[0m       0.4741        0.1505  0.8083\n",
      "     36        \u001b[36m0.1447\u001b[0m       0.4741        \u001b[35m0.1494\u001b[0m  0.6147\n",
      "     37        \u001b[36m0.1439\u001b[0m       0.4741        \u001b[35m0.1478\u001b[0m  0.6166\n",
      "     38        \u001b[36m0.1420\u001b[0m       0.4741        0.1479  0.6279\n",
      "     39        0.1425       0.4741        \u001b[35m0.1469\u001b[0m  0.6039\n",
      "     40        \u001b[36m0.1416\u001b[0m       0.4741        \u001b[35m0.1458\u001b[0m  0.6161\n",
      "     41        \u001b[36m0.1401\u001b[0m       0.4741        \u001b[35m0.1443\u001b[0m  0.6068\n",
      "     42        \u001b[36m0.1395\u001b[0m       0.4741        \u001b[35m0.1438\u001b[0m  0.6043\n",
      "     43        0.1414       0.4741        0.1458  0.6220\n",
      "     44        \u001b[36m0.1387\u001b[0m       0.4741        0.1440  0.6285\n",
      "     45        0.1388       0.4741        \u001b[35m0.1430\u001b[0m  0.5979\n",
      "     46        \u001b[36m0.1381\u001b[0m       0.4741        0.1444  0.6984\n",
      "     47        \u001b[36m0.1368\u001b[0m       0.4741        0.1488  0.6083\n",
      "     48        0.1383       0.4741        0.1434  0.6150\n",
      "     49        0.1377       0.4741        \u001b[35m0.1420\u001b[0m  0.6073\n",
      "     50        0.1374       0.4741        0.1423  0.6416\n",
      "Xtrain pre augmentation: (12942, 6)\n",
      "Ytrain pre augmentation (12942, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain: (19603, 6)\n",
      "Ytrain (19603, 1)\n",
      "Xtest (3235, 6)\n",
      "Ytest (3235, 1)\n",
      "Re-initializing module.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.3246\u001b[0m       \u001b[32m0.4698\u001b[0m        \u001b[35m0.3500\u001b[0m  0.6275\n",
      "      2        \u001b[36m0.2553\u001b[0m       0.4698        \u001b[35m0.2496\u001b[0m  0.6104\n",
      "      3        \u001b[36m0.2358\u001b[0m       0.4698        \u001b[35m0.2239\u001b[0m  0.6199\n",
      "      4        \u001b[36m0.2232\u001b[0m       0.4698        \u001b[35m0.2123\u001b[0m  0.6010\n",
      "      5        \u001b[36m0.2153\u001b[0m       0.4698        \u001b[35m0.2043\u001b[0m  0.6186\n",
      "      6        \u001b[36m0.2104\u001b[0m       0.4698        \u001b[35m0.1998\u001b[0m  0.6114\n",
      "      7        \u001b[36m0.2084\u001b[0m       0.4698        0.2000  0.6066\n",
      "      8        \u001b[36m0.2055\u001b[0m       0.4698        \u001b[35m0.1951\u001b[0m  0.6262\n",
      "      9        \u001b[36m0.2018\u001b[0m       0.4698        \u001b[35m0.1924\u001b[0m  0.6295\n",
      "     10        \u001b[36m0.2009\u001b[0m       0.4698        \u001b[35m0.1923\u001b[0m  0.6062\n",
      "     11        \u001b[36m0.1997\u001b[0m       0.4698        \u001b[35m0.1918\u001b[0m  0.6152\n",
      "     12        0.2007       0.4698        \u001b[35m0.1912\u001b[0m  0.6069\n",
      "     13        0.1998       0.4698        \u001b[35m0.1900\u001b[0m  0.6128\n",
      "     14        \u001b[36m0.1977\u001b[0m       0.4698        0.1908  0.6187\n",
      "     15        \u001b[36m0.1977\u001b[0m       0.4698        \u001b[35m0.1889\u001b[0m  0.6076\n",
      "     16        \u001b[36m0.1965\u001b[0m       0.4698        \u001b[35m0.1873\u001b[0m  0.6073\n",
      "     17        \u001b[36m0.1956\u001b[0m       0.4698        \u001b[35m0.1861\u001b[0m  0.7393\n",
      "     18        \u001b[36m0.1946\u001b[0m       0.4698        \u001b[35m0.1848\u001b[0m  0.6099\n",
      "     19        \u001b[36m0.1933\u001b[0m       0.4698        \u001b[35m0.1835\u001b[0m  0.6130\n",
      "     20        \u001b[36m0.1922\u001b[0m       0.4698        \u001b[35m0.1826\u001b[0m  0.6081\n",
      "     21        \u001b[36m0.1914\u001b[0m       0.4698        \u001b[35m0.1818\u001b[0m  0.6155\n",
      "     22        \u001b[36m0.1905\u001b[0m       0.4698        \u001b[35m0.1810\u001b[0m  0.6063\n",
      "     23        \u001b[36m0.1892\u001b[0m       0.4698        \u001b[35m0.1791\u001b[0m  0.6005\n",
      "     24        0.1893       0.4698        0.1801  0.6029\n",
      "     25        \u001b[36m0.1889\u001b[0m       0.4698        0.1796  0.6355\n",
      "     26        \u001b[36m0.1882\u001b[0m       0.4698        \u001b[35m0.1789\u001b[0m  0.6221\n",
      "     27        \u001b[36m0.1876\u001b[0m       0.4698        \u001b[35m0.1784\u001b[0m  0.6189\n",
      "     28        \u001b[36m0.1869\u001b[0m       0.4698        \u001b[35m0.1777\u001b[0m  0.7893\n",
      "     29        \u001b[36m0.1861\u001b[0m       0.4698        \u001b[35m0.1770\u001b[0m  0.6238\n",
      "     30        \u001b[36m0.1853\u001b[0m       0.4698        \u001b[35m0.1761\u001b[0m  0.6142\n",
      "     31        \u001b[36m0.1833\u001b[0m       0.4698        \u001b[35m0.1727\u001b[0m  0.6150\n",
      "     32        \u001b[36m0.1808\u001b[0m       0.4698        0.1752  0.6063\n",
      "     33        0.1811       0.4698        0.1762  0.6233\n",
      "     34        0.1830       0.4698        0.1754  0.6099\n",
      "     35        0.1824       0.4698        0.1739  0.6013\n",
      "     36        0.1812       0.4698        0.1736  0.6239\n",
      "     37        0.1817       0.4698        0.1741  0.6125\n",
      "     38        \u001b[36m0.1803\u001b[0m       0.4698        \u001b[35m0.1717\u001b[0m  0.6109\n",
      "     39        \u001b[36m0.1790\u001b[0m       0.4698        \u001b[35m0.1710\u001b[0m  0.6089\n",
      "     40        \u001b[36m0.1781\u001b[0m       0.4698        \u001b[35m0.1706\u001b[0m  0.6123\n",
      "     41        0.1809       0.4698        0.1734  0.6116\n",
      "     42        \u001b[36m0.1773\u001b[0m       0.4698        0.1720  0.6054\n",
      "     43        \u001b[36m0.1766\u001b[0m       0.4698        \u001b[35m0.1695\u001b[0m  3.0621\n",
      "     44        \u001b[36m0.1764\u001b[0m       0.4698        \u001b[35m0.1694\u001b[0m  0.9469\n",
      "     45        \u001b[36m0.1758\u001b[0m       0.4698        0.1701  0.7926\n",
      "     46        0.1783       0.4698        0.1731  0.7053\n",
      "     47        0.1769       0.4698        0.1711  0.6856\n",
      "     48        0.1766       0.4698        0.1760  0.7205\n",
      "     49        0.1808       0.4698        0.1730  0.6858\n",
      "     50        0.1790       0.4698        0.1722  0.7486\n",
      "Xtrain pre augmentation: (12942, 6)\n",
      "Ytrain pre augmentation (12942, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain: (20206, 6)\n",
      "Ytrain (20206, 1)\n",
      "Xtest (3235, 6)\n",
      "Ytest (3235, 1)\n",
      "Re-initializing module.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.3442\u001b[0m       \u001b[32m0.4770\u001b[0m        \u001b[35m0.3414\u001b[0m  0.7208\n",
      "      2        \u001b[36m0.2294\u001b[0m       0.4770        \u001b[35m0.2286\u001b[0m  0.6607\n",
      "      3        \u001b[36m0.2014\u001b[0m       0.4770        \u001b[35m0.2026\u001b[0m  0.6288\n",
      "      4        \u001b[36m0.1865\u001b[0m       0.4770        \u001b[35m0.1886\u001b[0m  0.6378\n",
      "      5        \u001b[36m0.1798\u001b[0m       0.4770        \u001b[35m0.1827\u001b[0m  0.6369\n",
      "      6        \u001b[36m0.1741\u001b[0m       0.4770        \u001b[35m0.1782\u001b[0m  0.6732\n",
      "      7        \u001b[36m0.1698\u001b[0m       0.4770        \u001b[35m0.1775\u001b[0m  0.6483\n",
      "      8        \u001b[36m0.1681\u001b[0m       0.4770        \u001b[35m0.1729\u001b[0m  0.6574\n",
      "      9        \u001b[36m0.1658\u001b[0m       0.4770        \u001b[35m0.1712\u001b[0m  0.6555\n",
      "     10        \u001b[36m0.1633\u001b[0m       0.4770        \u001b[35m0.1702\u001b[0m  0.6560\n",
      "     11        \u001b[36m0.1614\u001b[0m       0.4770        \u001b[35m0.1697\u001b[0m  0.6487\n",
      "     12        \u001b[36m0.1603\u001b[0m       0.4770        \u001b[35m0.1682\u001b[0m  0.6433\n",
      "     13        \u001b[36m0.1596\u001b[0m       0.4770        0.1693  0.6355\n",
      "     14        \u001b[36m0.1589\u001b[0m       0.4770        \u001b[35m0.1675\u001b[0m  0.6302\n",
      "     15        \u001b[36m0.1573\u001b[0m       0.4770        \u001b[35m0.1667\u001b[0m  0.6288\n",
      "     16        \u001b[36m0.1559\u001b[0m       0.4770        \u001b[35m0.1642\u001b[0m  0.6333\n",
      "     17        \u001b[36m0.1553\u001b[0m       0.4770        0.1702  0.6319\n",
      "     18        0.1560       0.4770        \u001b[35m0.1640\u001b[0m  0.6456\n",
      "     19        \u001b[36m0.1538\u001b[0m       0.4770        0.2304  0.7006\n",
      "     20        \u001b[36m0.1526\u001b[0m       0.4770        \u001b[35m0.1628\u001b[0m  0.6636\n",
      "     21        \u001b[36m0.1517\u001b[0m       0.4770        \u001b[35m0.1595\u001b[0m  0.6424\n",
      "     22        \u001b[36m0.1495\u001b[0m       0.4770        \u001b[35m0.1582\u001b[0m  0.6231\n",
      "     23        0.1511       0.4770        1.2410  0.6234\n",
      "     24        0.1526       0.4770        0.1619  0.6424\n",
      "     25        0.1508       0.4770        0.1682  0.6331\n",
      "     26        0.1510       0.4770        0.1651  0.6282\n",
      "     27        0.1502       0.4770        0.1609  0.7146\n",
      "     28        0.1504       0.4770        0.1600  0.8333\n",
      "     29        \u001b[36m0.1495\u001b[0m       0.4770        0.1592  0.8034\n",
      "     30        \u001b[36m0.1483\u001b[0m       0.4770        \u001b[35m0.1576\u001b[0m  0.7752\n",
      "     31        \u001b[36m0.1478\u001b[0m       0.4770        \u001b[35m0.1567\u001b[0m  0.7787\n",
      "     32        \u001b[36m0.1467\u001b[0m       0.4770        \u001b[35m0.1555\u001b[0m  0.8817\n",
      "     33        \u001b[36m0.1462\u001b[0m       0.4770        \u001b[35m0.1549\u001b[0m  0.8325\n",
      "     34        \u001b[36m0.1452\u001b[0m       0.4770        0.2975  0.8007\n",
      "     35        0.1479       0.4770        0.1578  0.9506\n",
      "     36        0.1467       0.4770        0.1589  0.8109\n",
      "     37        0.1470       0.4770        0.1564  0.7818\n",
      "     38        0.1454       0.4770        0.1560  0.7913\n",
      "     39        \u001b[36m0.1448\u001b[0m       0.4770        \u001b[35m0.1541\u001b[0m  0.8342\n",
      "     40        0.1452       0.4770        0.1549  0.8404\n",
      "     41        0.1451       0.4770        0.1548  0.8036\n",
      "     42        \u001b[36m0.1442\u001b[0m       0.4770        0.1588  0.8238\n",
      "     43        \u001b[36m0.1442\u001b[0m       0.4770        \u001b[35m0.1537\u001b[0m  1.0111\n",
      "     44        \u001b[36m0.1434\u001b[0m       0.4770        \u001b[35m0.1532\u001b[0m  0.7560\n",
      "     45        \u001b[36m0.1426\u001b[0m       0.4770        \u001b[35m0.1522\u001b[0m  0.6882\n",
      "     46        \u001b[36m0.1423\u001b[0m       0.4770        \u001b[35m0.1518\u001b[0m  0.7684\n",
      "     47        \u001b[36m0.1409\u001b[0m       0.4770        \u001b[35m0.1505\u001b[0m  0.8099\n",
      "     48        \u001b[36m0.1405\u001b[0m       0.4770        \u001b[35m0.1494\u001b[0m  0.7735\n",
      "     49        \u001b[36m0.1399\u001b[0m       0.4770        0.1662  0.7788\n",
      "     50        0.1401       0.4770        \u001b[35m0.1478\u001b[0m  0.7762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain pre augmentation: (12942, 6)\n",
      "Ytrain pre augmentation (12942, 1)\n",
      "Xtrain: (18871, 6)\n",
      "Ytrain (18871, 1)\n",
      "Xtest (3235, 6)\n",
      "Ytest (3235, 1)\n",
      "Re-initializing module.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.3557\u001b[0m       \u001b[32m0.4710\u001b[0m        \u001b[35m0.3703\u001b[0m  0.8280\n",
      "      2        \u001b[36m0.2511\u001b[0m       0.4710        \u001b[35m0.2422\u001b[0m  0.6886\n",
      "      3        \u001b[36m0.2301\u001b[0m       0.4710        \u001b[35m0.2119\u001b[0m  0.6125\n",
      "      4        \u001b[36m0.2191\u001b[0m       0.4710        \u001b[35m0.2004\u001b[0m  0.7218\n",
      "      5        \u001b[36m0.2110\u001b[0m       0.4710        \u001b[35m0.1924\u001b[0m  0.6618\n",
      "      6        \u001b[36m0.2043\u001b[0m       0.4710        \u001b[35m0.1859\u001b[0m  0.6479\n",
      "      7        \u001b[36m0.1995\u001b[0m       0.4710        \u001b[35m0.1806\u001b[0m  0.6559\n",
      "      8        \u001b[36m0.1953\u001b[0m       0.4710        \u001b[35m0.1762\u001b[0m  0.6195\n",
      "      9        \u001b[36m0.1920\u001b[0m       0.4710        \u001b[35m0.1733\u001b[0m  0.6305\n",
      "     10        \u001b[36m0.1892\u001b[0m       0.4710        \u001b[35m0.1710\u001b[0m  0.6161\n",
      "     11        \u001b[36m0.1867\u001b[0m       0.4710        \u001b[35m0.1683\u001b[0m  0.6060\n",
      "     12        \u001b[36m0.1844\u001b[0m       0.4710        \u001b[35m0.1662\u001b[0m  0.6288\n",
      "     13        \u001b[36m0.1827\u001b[0m       0.4710        \u001b[35m0.1641\u001b[0m  0.7351\n",
      "     14        \u001b[36m0.1809\u001b[0m       0.4710        \u001b[35m0.1627\u001b[0m  0.7169\n",
      "     15        \u001b[36m0.1795\u001b[0m       0.4710        \u001b[35m0.1611\u001b[0m  0.7592\n",
      "     16        \u001b[36m0.1781\u001b[0m       0.4710        \u001b[35m0.1598\u001b[0m  0.7673\n",
      "     17        \u001b[36m0.1769\u001b[0m       0.4710        \u001b[35m0.1586\u001b[0m  0.7668\n",
      "     18        \u001b[36m0.1762\u001b[0m       0.4710        \u001b[35m0.1586\u001b[0m  0.7512\n",
      "     19        \u001b[36m0.1753\u001b[0m       0.4710        \u001b[35m0.1572\u001b[0m  0.7690\n",
      "     20        \u001b[36m0.1741\u001b[0m       0.4710        \u001b[35m0.1563\u001b[0m  0.7992\n",
      "     21        \u001b[36m0.1730\u001b[0m       0.4710        \u001b[35m0.1552\u001b[0m  0.7966\n",
      "     22        \u001b[36m0.1720\u001b[0m       0.4710        \u001b[35m0.1542\u001b[0m  0.7691\n",
      "     23        \u001b[36m0.1714\u001b[0m       0.4710        \u001b[35m0.1539\u001b[0m  0.7447\n",
      "     24        \u001b[36m0.1706\u001b[0m       0.4710        \u001b[35m0.1529\u001b[0m  0.7096\n",
      "     25        \u001b[36m0.1698\u001b[0m       0.4710        \u001b[35m0.1521\u001b[0m  0.6886\n",
      "     26        \u001b[36m0.1695\u001b[0m       0.4710        \u001b[35m0.1518\u001b[0m  0.6695\n",
      "     27        0.1704       0.4710        0.1541  0.6701\n",
      "     28        \u001b[36m0.1695\u001b[0m       0.4710        0.1519  0.6833\n",
      "     29        \u001b[36m0.1683\u001b[0m       0.4710        \u001b[35m0.1504\u001b[0m  0.7721\n",
      "     30        \u001b[36m0.1675\u001b[0m       0.4710        \u001b[35m0.1497\u001b[0m  0.6649\n",
      "     31        \u001b[36m0.1668\u001b[0m       0.4710        \u001b[35m0.1491\u001b[0m  0.6702\n",
      "     32        \u001b[36m0.1662\u001b[0m       0.4710        \u001b[35m0.1485\u001b[0m  0.6703\n",
      "     33        \u001b[36m0.1657\u001b[0m       0.4710        \u001b[35m0.1481\u001b[0m  0.6703\n",
      "     34        \u001b[36m0.1652\u001b[0m       0.4710        \u001b[35m0.1476\u001b[0m  0.6836\n",
      "     35        \u001b[36m0.1649\u001b[0m       0.4710        \u001b[35m0.1474\u001b[0m  0.6612\n",
      "     36        \u001b[36m0.1643\u001b[0m       0.4710        \u001b[35m0.1469\u001b[0m  0.6663\n",
      "     37        \u001b[36m0.1639\u001b[0m       0.4710        \u001b[35m0.1464\u001b[0m  0.6852\n",
      "     38        \u001b[36m0.1636\u001b[0m       0.4710        \u001b[35m0.1461\u001b[0m  0.6717\n",
      "     39        \u001b[36m0.1632\u001b[0m       0.4710        \u001b[35m0.1458\u001b[0m  0.6657\n",
      "     40        \u001b[36m0.1628\u001b[0m       0.4710        \u001b[35m0.1455\u001b[0m  0.6780\n",
      "     41        \u001b[36m0.1626\u001b[0m       0.4710        \u001b[35m0.1451\u001b[0m  0.6747\n",
      "     42        \u001b[36m0.1622\u001b[0m       0.4710        \u001b[35m0.1448\u001b[0m  0.6714\n",
      "     43        \u001b[36m0.1619\u001b[0m       0.4710        \u001b[35m0.1445\u001b[0m  0.6937\n",
      "     44        \u001b[36m0.1615\u001b[0m       0.4710        \u001b[35m0.1441\u001b[0m  0.6686\n",
      "     45        \u001b[36m0.1613\u001b[0m       0.4710        \u001b[35m0.1439\u001b[0m  0.6608\n",
      "     46        \u001b[36m0.1609\u001b[0m       0.4710        0.1444  0.6752\n",
      "     47        0.1615       0.4710        \u001b[35m0.1436\u001b[0m  0.6715\n",
      "     48        \u001b[36m0.1605\u001b[0m       0.4710        \u001b[35m0.1432\u001b[0m  0.7921\n",
      "     49        \u001b[36m0.1601\u001b[0m       0.4710        \u001b[35m0.1429\u001b[0m  0.6663\n",
      "     50        \u001b[36m0.1598\u001b[0m       0.4710        \u001b[35m0.1425\u001b[0m  0.6775\n",
      "Xtrain pre augmentation: (2140, 6)\n",
      "Ytrain pre augmentation (2140, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain: (3709, 6)\n",
      "Ytrain (3709, 1)\n",
      "Xtest (535, 6)\n",
      "Ytest (535, 1)\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7037\u001b[0m       \u001b[32m0.4960\u001b[0m        \u001b[35m0.7182\u001b[0m  0.1331\n",
      "      2        \u001b[36m0.4729\u001b[0m       0.4960        \u001b[35m0.6615\u001b[0m  0.1665\n",
      "      3        \u001b[36m0.3859\u001b[0m       0.4960        \u001b[35m0.5862\u001b[0m  0.1536\n",
      "      4        \u001b[36m0.3394\u001b[0m       0.4960        \u001b[35m0.5026\u001b[0m  0.1723\n",
      "      5        \u001b[36m0.3106\u001b[0m       0.4960        \u001b[35m0.4216\u001b[0m  0.1486\n",
      "      6        \u001b[36m0.2905\u001b[0m       0.4960        \u001b[35m0.3530\u001b[0m  0.1466\n",
      "      7        \u001b[36m0.2761\u001b[0m       0.4960        \u001b[35m0.3039\u001b[0m  0.1450\n",
      "      8        \u001b[36m0.2654\u001b[0m       0.4960        \u001b[35m0.2723\u001b[0m  0.1527\n",
      "      9        \u001b[36m0.2569\u001b[0m       0.4960        \u001b[35m0.2524\u001b[0m  0.1461\n",
      "     10        \u001b[36m0.2499\u001b[0m       0.4960        \u001b[35m0.2394\u001b[0m  0.1512\n",
      "     11        \u001b[36m0.2439\u001b[0m       0.4960        \u001b[35m0.2295\u001b[0m  0.1820\n",
      "     12        \u001b[36m0.2387\u001b[0m       0.4960        \u001b[35m0.2219\u001b[0m  0.1500\n",
      "     13        \u001b[36m0.2340\u001b[0m       0.4960        \u001b[35m0.2151\u001b[0m  0.1464\n",
      "     14        \u001b[36m0.2298\u001b[0m       0.4960        \u001b[35m0.2104\u001b[0m  0.1563\n",
      "     15        \u001b[36m0.2260\u001b[0m       0.4960        \u001b[35m0.2058\u001b[0m  0.1444\n",
      "     16        \u001b[36m0.2225\u001b[0m       0.4960        \u001b[35m0.2014\u001b[0m  0.1561\n",
      "     17        \u001b[36m0.2186\u001b[0m       0.4960        \u001b[35m0.1977\u001b[0m  0.1603\n",
      "     18        \u001b[36m0.2154\u001b[0m       0.4960        \u001b[35m0.1953\u001b[0m  0.1545\n",
      "     19        \u001b[36m0.2128\u001b[0m       0.4960        \u001b[35m0.1925\u001b[0m  0.1745\n",
      "     20        \u001b[36m0.2103\u001b[0m       0.4960        \u001b[35m0.1902\u001b[0m  0.1535\n",
      "     21        \u001b[36m0.2077\u001b[0m       0.4960        \u001b[35m0.1879\u001b[0m  0.1550\n",
      "     22        \u001b[36m0.2053\u001b[0m       0.4960        \u001b[35m0.1853\u001b[0m  0.1573\n",
      "     23        \u001b[36m0.2030\u001b[0m       0.4960        \u001b[35m0.1833\u001b[0m  0.1591\n",
      "     24        \u001b[36m0.2007\u001b[0m       0.4960        \u001b[35m0.1814\u001b[0m  0.1531\n",
      "     25        \u001b[36m0.1986\u001b[0m       0.4960        \u001b[35m0.1801\u001b[0m  0.1513\n",
      "     26        \u001b[36m0.1966\u001b[0m       0.4960        \u001b[35m0.1777\u001b[0m  0.1416\n",
      "     27        \u001b[36m0.1945\u001b[0m       0.4960        \u001b[35m0.1762\u001b[0m  0.1414\n",
      "     28        \u001b[36m0.1923\u001b[0m       0.4960        0.1763  0.1512\n",
      "     29        \u001b[36m0.1907\u001b[0m       0.4960        \u001b[35m0.1744\u001b[0m  0.1625\n",
      "     30        \u001b[36m0.1889\u001b[0m       0.4960        \u001b[35m0.1725\u001b[0m  0.1492\n",
      "     31        \u001b[36m0.1870\u001b[0m       0.4960        \u001b[35m0.1705\u001b[0m  0.2533\n",
      "     32        \u001b[36m0.1848\u001b[0m       0.4960        \u001b[35m0.1678\u001b[0m  0.1497\n",
      "     33        \u001b[36m0.1832\u001b[0m       0.4960        \u001b[35m0.1664\u001b[0m  0.1423\n",
      "     34        \u001b[36m0.1811\u001b[0m       0.4960        \u001b[35m0.1647\u001b[0m  0.1440\n",
      "     35        \u001b[36m0.1802\u001b[0m       0.4960        \u001b[35m0.1633\u001b[0m  0.1654\n",
      "     36        \u001b[36m0.1784\u001b[0m       0.4960        \u001b[35m0.1627\u001b[0m  0.1500\n",
      "     37        \u001b[36m0.1768\u001b[0m       0.4960        0.1628  0.1476\n",
      "     38        \u001b[36m0.1740\u001b[0m       0.4960        \u001b[35m0.1605\u001b[0m  0.1504\n",
      "     39        \u001b[36m0.1732\u001b[0m       0.4960        \u001b[35m0.1587\u001b[0m  0.1446\n",
      "     40        \u001b[36m0.1714\u001b[0m       0.4960        \u001b[35m0.1578\u001b[0m  0.1454\n",
      "     41        \u001b[36m0.1699\u001b[0m       0.4960        \u001b[35m0.1557\u001b[0m  0.1583\n",
      "     42        \u001b[36m0.1686\u001b[0m       0.4960        0.1564  0.1451\n",
      "     43        \u001b[36m0.1674\u001b[0m       0.4960        \u001b[35m0.1542\u001b[0m  0.1518\n",
      "     44        \u001b[36m0.1657\u001b[0m       0.4960        \u001b[35m0.1529\u001b[0m  0.1475\n",
      "     45        \u001b[36m0.1641\u001b[0m       0.4960        \u001b[35m0.1524\u001b[0m  0.1416\n",
      "     46        \u001b[36m0.1626\u001b[0m       0.4960        \u001b[35m0.1500\u001b[0m  0.1453\n",
      "     47        \u001b[36m0.1610\u001b[0m       0.4960        \u001b[35m0.1490\u001b[0m  0.1446\n",
      "     48        \u001b[36m0.1594\u001b[0m       0.4960        \u001b[35m0.1468\u001b[0m  0.1473\n",
      "     49        \u001b[36m0.1577\u001b[0m       0.4960        \u001b[35m0.1462\u001b[0m  0.1481\n",
      "     50        \u001b[36m0.1559\u001b[0m       0.4960        \u001b[35m0.1437\u001b[0m  0.1422\n",
      "Xtrain pre augmentation: (2140, 6)\n",
      "Ytrain pre augmentation (2140, 1)\n",
      "Xtrain: (3623, 6)\n",
      "Ytrain (3623, 1)\n",
      "Xtest (535, 6)\n",
      "Ytest (535, 1)\n",
      "Re-initializing module.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6702\u001b[0m       \u001b[32m0.5021\u001b[0m        \u001b[35m0.6714\u001b[0m  0.1444\n",
      "      2        \u001b[36m0.4252\u001b[0m       0.5021        \u001b[35m0.6288\u001b[0m  0.1673\n",
      "      3        \u001b[36m0.3526\u001b[0m       0.5021        \u001b[35m0.5675\u001b[0m  0.1474\n",
      "      4        \u001b[36m0.3129\u001b[0m       0.5021        \u001b[35m0.4941\u001b[0m  0.1503\n",
      "      5        \u001b[36m0.2882\u001b[0m       0.5021        \u001b[35m0.4213\u001b[0m  0.1487\n",
      "      6        \u001b[36m0.2733\u001b[0m       0.5021        \u001b[35m0.3632\u001b[0m  0.1420\n",
      "      7        \u001b[36m0.2621\u001b[0m       0.5021        \u001b[35m0.3242\u001b[0m  0.1409\n",
      "      8        \u001b[36m0.2538\u001b[0m       0.5021        \u001b[35m0.3009\u001b[0m  0.1379\n",
      "      9        \u001b[36m0.2472\u001b[0m       0.5021        \u001b[35m0.2866\u001b[0m  0.1488\n",
      "     10        \u001b[36m0.2416\u001b[0m       0.5021        \u001b[35m0.2768\u001b[0m  0.2257\n",
      "     11        \u001b[36m0.2371\u001b[0m       0.5021        \u001b[35m0.2703\u001b[0m  0.2799\n",
      "     12        \u001b[36m0.2333\u001b[0m       0.5021        \u001b[35m0.2644\u001b[0m  0.1389\n",
      "     13        \u001b[36m0.2291\u001b[0m       0.5021        \u001b[35m0.2614\u001b[0m  0.1409\n",
      "     14        \u001b[36m0.2265\u001b[0m       0.5021        0.2633  0.1631\n",
      "     15        \u001b[36m0.2240\u001b[0m       0.5021        \u001b[35m0.2544\u001b[0m  0.1374\n",
      "     16        \u001b[36m0.2202\u001b[0m       0.5021        \u001b[35m0.2525\u001b[0m  0.1522\n",
      "     17        \u001b[36m0.2178\u001b[0m       0.5021        0.2531  0.1406\n",
      "     18        \u001b[36m0.2163\u001b[0m       0.5021        \u001b[35m0.2504\u001b[0m  0.1538\n",
      "     19        \u001b[36m0.2138\u001b[0m       0.5021        0.2533  0.1313\n",
      "     20        \u001b[36m0.2119\u001b[0m       0.5021        \u001b[35m0.2446\u001b[0m  0.1392\n",
      "     21        \u001b[36m0.2087\u001b[0m       0.5021        \u001b[35m0.2432\u001b[0m  0.1406\n",
      "     22        \u001b[36m0.2077\u001b[0m       0.5021        \u001b[35m0.2416\u001b[0m  0.1305\n",
      "     23        \u001b[36m0.2059\u001b[0m       0.5021        \u001b[35m0.2406\u001b[0m  0.1368\n",
      "     24        \u001b[36m0.2052\u001b[0m       0.5021        \u001b[35m0.2394\u001b[0m  0.1317\n",
      "     25        \u001b[36m0.2026\u001b[0m       0.5021        \u001b[35m0.2364\u001b[0m  0.1324\n",
      "     26        \u001b[36m0.2005\u001b[0m       0.5021        0.2396  0.1440\n",
      "     27        \u001b[36m0.1988\u001b[0m       0.5021        \u001b[35m0.2319\u001b[0m  0.1301\n",
      "     28        \u001b[36m0.1971\u001b[0m       0.5021        0.2348  0.1234\n",
      "     29        \u001b[36m0.1958\u001b[0m       0.5021        0.2449  0.1224\n",
      "     30        \u001b[36m0.1956\u001b[0m       0.5021        0.2328  0.1247\n",
      "     31        \u001b[36m0.1930\u001b[0m       0.5021        \u001b[35m0.2294\u001b[0m  0.1316\n",
      "     32        \u001b[36m0.1918\u001b[0m       0.5021        \u001b[35m0.2259\u001b[0m  0.1214\n",
      "     33        \u001b[36m0.1911\u001b[0m       0.5021        \u001b[35m0.2215\u001b[0m  0.1213\n",
      "     34        \u001b[36m0.1883\u001b[0m       0.5021        0.2243  0.1276\n",
      "     35        0.1883       0.5021        \u001b[35m0.2198\u001b[0m  0.1219\n",
      "     36        \u001b[36m0.1858\u001b[0m       0.5021        0.2233  0.1219\n",
      "     37        \u001b[36m0.1847\u001b[0m       0.5021        0.2302  0.1298\n",
      "     38        \u001b[36m0.1841\u001b[0m       0.5021        \u001b[35m0.2174\u001b[0m  0.1227\n",
      "     39        \u001b[36m0.1823\u001b[0m       0.5021        0.2407  0.1216\n",
      "     40        0.1847       0.5021        \u001b[35m0.2130\u001b[0m  0.1251\n",
      "     41        \u001b[36m0.1800\u001b[0m       0.5021        0.2152  0.1229\n",
      "     42        \u001b[36m0.1785\u001b[0m       0.5021        0.2254  0.1222\n",
      "     43        0.1788       0.5021        0.2182  0.1209\n",
      "     44        \u001b[36m0.1768\u001b[0m       0.5021        \u001b[35m0.2104\u001b[0m  0.1221\n",
      "     45        \u001b[36m0.1753\u001b[0m       0.5021        0.2297  0.1307\n",
      "     46        0.1773       0.5021        \u001b[35m0.2059\u001b[0m  0.1334\n",
      "     47        \u001b[36m0.1726\u001b[0m       0.5021        0.2316  0.1240\n",
      "     48        0.1762       0.5021        0.2070  0.1200\n",
      "     49        \u001b[36m0.1720\u001b[0m       0.5021        0.2084  0.1189\n",
      "     50        \u001b[36m0.1707\u001b[0m       0.5021        \u001b[35m0.2035\u001b[0m  0.1197\n",
      "Xtrain pre augmentation: (2140, 6)\n",
      "Ytrain pre augmentation (2140, 1)\n",
      "Xtrain: (3633, 6)\n",
      "Ytrain (3633, 1)\n",
      "Xtest (535, 6)\n",
      "Ytest (535, 1)\n",
      "Re-initializing module.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6069\u001b[0m       \u001b[32m0.4979\u001b[0m        \u001b[35m0.6842\u001b[0m  0.1264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2        \u001b[36m0.4212\u001b[0m       0.4979        \u001b[35m0.6428\u001b[0m  0.1534\n",
      "      3        \u001b[36m0.3504\u001b[0m       0.4979        \u001b[35m0.5776\u001b[0m  0.1430\n",
      "      4        \u001b[36m0.3118\u001b[0m       0.4979        \u001b[35m0.4954\u001b[0m  0.1430\n",
      "      5        \u001b[36m0.2878\u001b[0m       0.4979        \u001b[35m0.4148\u001b[0m  0.1364\n",
      "      6        \u001b[36m0.2711\u001b[0m       0.4979        \u001b[35m0.3475\u001b[0m  0.1287\n",
      "      7        \u001b[36m0.2586\u001b[0m       0.4979        \u001b[35m0.3002\u001b[0m  0.1349\n",
      "      8        \u001b[36m0.2486\u001b[0m       0.4979        \u001b[35m0.2701\u001b[0m  0.1252\n",
      "      9        \u001b[36m0.2406\u001b[0m       0.4979        \u001b[35m0.2514\u001b[0m  0.1312\n",
      "     10        \u001b[36m0.2338\u001b[0m       0.4979        \u001b[35m0.2391\u001b[0m  0.1282\n",
      "     11        \u001b[36m0.2280\u001b[0m       0.4979        \u001b[35m0.2297\u001b[0m  0.1383\n",
      "     12        \u001b[36m0.2226\u001b[0m       0.4979        \u001b[35m0.2221\u001b[0m  0.1255\n",
      "     13        \u001b[36m0.2178\u001b[0m       0.4979        \u001b[35m0.2159\u001b[0m  0.1247\n",
      "     14        \u001b[36m0.2135\u001b[0m       0.4979        \u001b[35m0.2104\u001b[0m  0.1447\n",
      "     15        \u001b[36m0.2095\u001b[0m       0.4979        \u001b[35m0.2062\u001b[0m  0.1320\n",
      "     16        \u001b[36m0.2059\u001b[0m       0.4979        \u001b[35m0.2023\u001b[0m  0.1355\n",
      "     17        \u001b[36m0.2025\u001b[0m       0.4979        \u001b[35m0.1991\u001b[0m  0.1261\n",
      "     18        \u001b[36m0.1992\u001b[0m       0.4979        \u001b[35m0.1961\u001b[0m  0.1297\n",
      "     19        \u001b[36m0.1961\u001b[0m       0.4979        \u001b[35m0.1934\u001b[0m  0.1331\n",
      "     20        \u001b[36m0.1933\u001b[0m       0.4979        \u001b[35m0.1909\u001b[0m  0.1324\n",
      "     21        \u001b[36m0.1904\u001b[0m       0.4979        \u001b[35m0.1882\u001b[0m  0.1232\n",
      "     22        \u001b[36m0.1877\u001b[0m       0.4979        \u001b[35m0.1858\u001b[0m  0.1256\n",
      "     23        \u001b[36m0.1851\u001b[0m       0.4979        \u001b[35m0.1833\u001b[0m  0.1343\n",
      "     24        \u001b[36m0.1825\u001b[0m       0.4979        \u001b[35m0.1810\u001b[0m  0.1293\n",
      "     25        \u001b[36m0.1799\u001b[0m       0.4979        \u001b[35m0.1790\u001b[0m  0.2445\n",
      "     26        \u001b[36m0.1775\u001b[0m       0.4979        \u001b[35m0.1770\u001b[0m  0.1291\n",
      "     27        \u001b[36m0.1752\u001b[0m       0.4979        \u001b[35m0.1750\u001b[0m  0.1258\n",
      "     28        \u001b[36m0.1730\u001b[0m       0.4979        \u001b[35m0.1732\u001b[0m  0.1262\n",
      "     29        \u001b[36m0.1709\u001b[0m       0.4979        \u001b[35m0.1713\u001b[0m  0.1253\n",
      "     30        \u001b[36m0.1688\u001b[0m       0.4979        \u001b[35m0.1694\u001b[0m  0.1253\n",
      "     31        \u001b[36m0.1668\u001b[0m       0.4979        \u001b[35m0.1676\u001b[0m  0.1373\n",
      "     32        \u001b[36m0.1647\u001b[0m       0.4979        \u001b[35m0.1661\u001b[0m  0.1295\n",
      "     33        \u001b[36m0.1627\u001b[0m       0.4979        \u001b[35m0.1642\u001b[0m  0.1252\n",
      "     34        \u001b[36m0.1609\u001b[0m       0.4979        \u001b[35m0.1626\u001b[0m  0.1189\n",
      "     35        \u001b[36m0.1592\u001b[0m       0.4979        \u001b[35m0.1604\u001b[0m  0.1254\n",
      "     36        \u001b[36m0.1575\u001b[0m       0.4979        \u001b[35m0.1590\u001b[0m  0.1272\n",
      "     37        \u001b[36m0.1559\u001b[0m       0.4979        \u001b[35m0.1570\u001b[0m  0.1543\n",
      "     38        \u001b[36m0.1543\u001b[0m       0.4979        \u001b[35m0.1556\u001b[0m  0.1238\n",
      "     39        \u001b[36m0.1528\u001b[0m       0.4979        \u001b[35m0.1543\u001b[0m  0.1274\n",
      "     40        \u001b[36m0.1514\u001b[0m       0.4979        \u001b[35m0.1530\u001b[0m  0.1426\n",
      "     41        \u001b[36m0.1500\u001b[0m       0.4979        \u001b[35m0.1515\u001b[0m  0.1229\n",
      "     42        \u001b[36m0.1485\u001b[0m       0.4979        \u001b[35m0.1506\u001b[0m  0.1291\n",
      "     43        \u001b[36m0.1473\u001b[0m       0.4979        \u001b[35m0.1492\u001b[0m  0.1382\n",
      "     44        \u001b[36m0.1459\u001b[0m       0.4979        \u001b[35m0.1480\u001b[0m  0.1215\n",
      "     45        \u001b[36m0.1447\u001b[0m       0.4979        \u001b[35m0.1463\u001b[0m  0.1295\n",
      "     46        \u001b[36m0.1434\u001b[0m       0.4979        \u001b[35m0.1457\u001b[0m  0.1367\n",
      "     47        \u001b[36m0.1422\u001b[0m       0.4979        \u001b[35m0.1445\u001b[0m  0.1300\n",
      "     48        \u001b[36m0.1409\u001b[0m       0.4979        \u001b[35m0.1434\u001b[0m  0.1218\n",
      "     49        \u001b[36m0.1396\u001b[0m       0.4979        \u001b[35m0.1421\u001b[0m  0.1271\n",
      "     50        \u001b[36m0.1384\u001b[0m       0.4979        \u001b[35m0.1415\u001b[0m  0.1294\n",
      "Xtrain pre augmentation: (2140, 6)\n",
      "Ytrain pre augmentation (2140, 1)\n",
      "Xtrain: (3716, 6)\n",
      "Ytrain (3716, 1)\n",
      "Xtest (535, 6)\n",
      "Ytest (535, 1)\n",
      "Re-initializing module.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.5455\u001b[0m       \u001b[32m0.4892\u001b[0m        \u001b[35m0.6714\u001b[0m  0.1234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2        \u001b[36m0.4017\u001b[0m       0.4892        \u001b[35m0.6218\u001b[0m  0.1602\n",
      "      3        \u001b[36m0.3410\u001b[0m       0.4892        \u001b[35m0.5525\u001b[0m  0.1360\n",
      "      4        \u001b[36m0.3077\u001b[0m       0.4892        \u001b[35m0.4704\u001b[0m  0.1373\n",
      "      5        \u001b[36m0.2862\u001b[0m       0.4892        \u001b[35m0.3901\u001b[0m  0.1291\n",
      "      6        \u001b[36m0.2709\u001b[0m       0.4892        \u001b[35m0.3262\u001b[0m  0.1328\n",
      "      7        \u001b[36m0.2594\u001b[0m       0.4892        \u001b[35m0.2818\u001b[0m  0.1301\n",
      "      8        \u001b[36m0.2503\u001b[0m       0.4892        \u001b[35m0.2532\u001b[0m  0.1414\n",
      "      9        \u001b[36m0.2428\u001b[0m       0.4892        \u001b[35m0.2349\u001b[0m  0.1311\n",
      "     10        \u001b[36m0.2365\u001b[0m       0.4892        \u001b[35m0.2223\u001b[0m  0.1398\n",
      "     11        \u001b[36m0.2310\u001b[0m       0.4892        \u001b[35m0.2134\u001b[0m  0.1436\n",
      "     12        \u001b[36m0.2262\u001b[0m       0.4892        \u001b[35m0.2067\u001b[0m  0.1367\n",
      "     13        \u001b[36m0.2218\u001b[0m       0.4892        \u001b[35m0.2012\u001b[0m  0.1315\n",
      "     14        \u001b[36m0.2179\u001b[0m       0.4892        \u001b[35m0.1962\u001b[0m  0.1288\n",
      "     15        \u001b[36m0.2142\u001b[0m       0.4892        \u001b[35m0.1920\u001b[0m  0.1255\n",
      "     16        \u001b[36m0.2108\u001b[0m       0.4892        \u001b[35m0.1883\u001b[0m  0.1292\n",
      "     17        \u001b[36m0.2077\u001b[0m       0.4892        \u001b[35m0.1851\u001b[0m  0.1364\n",
      "     18        \u001b[36m0.2048\u001b[0m       0.4892        \u001b[35m0.1822\u001b[0m  0.1393\n",
      "     19        \u001b[36m0.2020\u001b[0m       0.4892        \u001b[35m0.1796\u001b[0m  0.1398\n",
      "     20        \u001b[36m0.1994\u001b[0m       0.4892        \u001b[35m0.1772\u001b[0m  0.1376\n",
      "     21        \u001b[36m0.1968\u001b[0m       0.4892        \u001b[35m0.1748\u001b[0m  0.1309\n",
      "     22        \u001b[36m0.1942\u001b[0m       0.4892        \u001b[35m0.1726\u001b[0m  0.1261\n",
      "     23        \u001b[36m0.1918\u001b[0m       0.4892        \u001b[35m0.1705\u001b[0m  0.1309\n",
      "     24        \u001b[36m0.1894\u001b[0m       0.4892        \u001b[35m0.1684\u001b[0m  0.1378\n",
      "     25        \u001b[36m0.1872\u001b[0m       0.4892        \u001b[35m0.1663\u001b[0m  0.1358\n",
      "     26        \u001b[36m0.1850\u001b[0m       0.4892        \u001b[35m0.1644\u001b[0m  0.1347\n",
      "     27        \u001b[36m0.1829\u001b[0m       0.4892        \u001b[35m0.1625\u001b[0m  0.1308\n",
      "     28        \u001b[36m0.1808\u001b[0m       0.4892        \u001b[35m0.1606\u001b[0m  0.1344\n",
      "     29        \u001b[36m0.1789\u001b[0m       0.4892        \u001b[35m0.1585\u001b[0m  0.1307\n",
      "     30        \u001b[36m0.1769\u001b[0m       0.4892        \u001b[35m0.1568\u001b[0m  0.1300\n",
      "     31        \u001b[36m0.1751\u001b[0m       0.4892        \u001b[35m0.1553\u001b[0m  0.1372\n",
      "     32        \u001b[36m0.1733\u001b[0m       0.4892        \u001b[35m0.1541\u001b[0m  0.1306\n",
      "     33        \u001b[36m0.1715\u001b[0m       0.4892        \u001b[35m0.1523\u001b[0m  0.1245\n",
      "     34        \u001b[36m0.1699\u001b[0m       0.4892        \u001b[35m0.1503\u001b[0m  0.1349\n",
      "     35        \u001b[36m0.1682\u001b[0m       0.4892        \u001b[35m0.1493\u001b[0m  0.1300\n",
      "     36        \u001b[36m0.1666\u001b[0m       0.4892        \u001b[35m0.1471\u001b[0m  0.1244\n",
      "     37        \u001b[36m0.1649\u001b[0m       0.4892        \u001b[35m0.1463\u001b[0m  0.1474\n",
      "     38        \u001b[36m0.1633\u001b[0m       0.4892        \u001b[35m0.1444\u001b[0m  0.2522\n",
      "     39        \u001b[36m0.1616\u001b[0m       0.4892        \u001b[35m0.1435\u001b[0m  0.1539\n",
      "     40        \u001b[36m0.1602\u001b[0m       0.4892        \u001b[35m0.1424\u001b[0m  0.1420\n",
      "     41        \u001b[36m0.1585\u001b[0m       0.4892        \u001b[35m0.1410\u001b[0m  0.1309\n",
      "     42        \u001b[36m0.1568\u001b[0m       0.4892        \u001b[35m0.1398\u001b[0m  0.1252\n",
      "     43        \u001b[36m0.1555\u001b[0m       0.4892        \u001b[35m0.1381\u001b[0m  0.1329\n",
      "     44        \u001b[36m0.1540\u001b[0m       0.4892        \u001b[35m0.1371\u001b[0m  0.1291\n",
      "     45        \u001b[36m0.1529\u001b[0m       0.4892        \u001b[35m0.1355\u001b[0m  0.1215\n",
      "     46        \u001b[36m0.1514\u001b[0m       0.4892        \u001b[35m0.1351\u001b[0m  0.1385\n",
      "     47        \u001b[36m0.1504\u001b[0m       0.4892        \u001b[35m0.1333\u001b[0m  0.1349\n",
      "     48        \u001b[36m0.1490\u001b[0m       0.4892        \u001b[35m0.1330\u001b[0m  0.1334\n",
      "     49        \u001b[36m0.1480\u001b[0m       0.4892        \u001b[35m0.1312\u001b[0m  0.1232\n",
      "     50        \u001b[36m0.1467\u001b[0m       0.4892        \u001b[35m0.1311\u001b[0m  0.1315\n",
      "Xtrain pre augmentation: (2140, 6)\n",
      "Ytrain pre augmentation (2140, 1)\n",
      "Xtrain: (3629, 6)\n",
      "Ytrain (3629, 1)\n",
      "Xtest (535, 6)\n",
      "Ytest (535, 1)\n",
      "Re-initializing module.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6471\u001b[0m       \u001b[32m0.4917\u001b[0m        \u001b[35m0.6871\u001b[0m  0.2114\n",
      "      2        \u001b[36m0.4541\u001b[0m       0.4917        \u001b[35m0.6437\u001b[0m  0.1468\n",
      "      3        \u001b[36m0.3800\u001b[0m       0.4917        \u001b[35m0.5736\u001b[0m  0.1390\n",
      "      4        \u001b[36m0.3398\u001b[0m       0.4917        \u001b[35m0.4916\u001b[0m  0.1351\n",
      "      5        \u001b[36m0.3142\u001b[0m       0.4917        \u001b[35m0.4110\u001b[0m  0.1261\n",
      "      6        \u001b[36m0.2963\u001b[0m       0.4917        \u001b[35m0.3472\u001b[0m  0.1291\n",
      "      7        \u001b[36m0.2829\u001b[0m       0.4917        \u001b[35m0.3033\u001b[0m  0.1282\n",
      "      8        \u001b[36m0.2726\u001b[0m       0.4917        \u001b[35m0.2755\u001b[0m  0.1266\n",
      "      9        \u001b[36m0.2644\u001b[0m       0.4917        \u001b[35m0.2576\u001b[0m  0.1376\n",
      "     10        \u001b[36m0.2571\u001b[0m       0.4917        \u001b[35m0.2456\u001b[0m  0.1417\n",
      "     11        \u001b[36m0.2509\u001b[0m       0.4917        \u001b[35m0.2368\u001b[0m  0.1368\n",
      "     12        \u001b[36m0.2456\u001b[0m       0.4917        \u001b[35m0.2300\u001b[0m  0.1338\n",
      "     13        \u001b[36m0.2408\u001b[0m       0.4917        \u001b[35m0.2245\u001b[0m  0.1317\n",
      "     14        \u001b[36m0.2366\u001b[0m       0.4917        \u001b[35m0.2200\u001b[0m  0.1299\n",
      "     15        \u001b[36m0.2327\u001b[0m       0.4917        \u001b[35m0.2164\u001b[0m  0.1249\n",
      "     16        \u001b[36m0.2291\u001b[0m       0.4917        \u001b[35m0.2132\u001b[0m  0.1282\n",
      "     17        \u001b[36m0.2258\u001b[0m       0.4917        \u001b[35m0.2102\u001b[0m  0.1318\n",
      "     18        \u001b[36m0.2226\u001b[0m       0.4917        \u001b[35m0.2074\u001b[0m  0.1307\n",
      "     19        \u001b[36m0.2196\u001b[0m       0.4917        \u001b[35m0.2048\u001b[0m  0.1327\n",
      "     20        \u001b[36m0.2168\u001b[0m       0.4917        \u001b[35m0.2023\u001b[0m  0.1266\n",
      "     21        \u001b[36m0.2141\u001b[0m       0.4917        \u001b[35m0.2000\u001b[0m  0.1228\n",
      "     22        \u001b[36m0.2114\u001b[0m       0.4917        \u001b[35m0.1978\u001b[0m  0.1358\n",
      "     23        \u001b[36m0.2089\u001b[0m       0.4917        \u001b[35m0.1957\u001b[0m  0.1319\n",
      "     24        \u001b[36m0.2064\u001b[0m       0.4917        \u001b[35m0.1937\u001b[0m  0.1334\n",
      "     25        \u001b[36m0.2040\u001b[0m       0.4917        \u001b[35m0.1917\u001b[0m  0.1255\n",
      "     26        \u001b[36m0.2016\u001b[0m       0.4917        \u001b[35m0.1899\u001b[0m  0.1295\n",
      "     27        \u001b[36m0.1993\u001b[0m       0.4917        \u001b[35m0.1881\u001b[0m  0.1347\n",
      "     28        \u001b[36m0.1971\u001b[0m       0.4917        \u001b[35m0.1862\u001b[0m  0.1336\n",
      "     29        \u001b[36m0.1950\u001b[0m       0.4917        \u001b[35m0.1845\u001b[0m  0.1695\n",
      "     30        \u001b[36m0.1928\u001b[0m       0.4917        \u001b[35m0.1827\u001b[0m  0.1353\n",
      "     31        \u001b[36m0.1907\u001b[0m       0.4917        \u001b[35m0.1810\u001b[0m  0.1274\n",
      "     32        \u001b[36m0.1887\u001b[0m       0.4917        \u001b[35m0.1794\u001b[0m  0.1391\n",
      "     33        \u001b[36m0.1867\u001b[0m       0.4917        \u001b[35m0.1776\u001b[0m  0.1323\n",
      "     34        \u001b[36m0.1846\u001b[0m       0.4917        \u001b[35m0.1760\u001b[0m  0.1280\n",
      "     35        \u001b[36m0.1825\u001b[0m       0.4917        \u001b[35m0.1744\u001b[0m  0.1218\n",
      "     36        \u001b[36m0.1805\u001b[0m       0.4917        \u001b[35m0.1729\u001b[0m  0.1276\n",
      "     37        \u001b[36m0.1786\u001b[0m       0.4917        \u001b[35m0.1713\u001b[0m  0.1263\n",
      "     38        \u001b[36m0.1767\u001b[0m       0.4917        \u001b[35m0.1697\u001b[0m  0.1208\n",
      "     39        \u001b[36m0.1750\u001b[0m       0.4917        \u001b[35m0.1681\u001b[0m  0.1260\n",
      "     40        \u001b[36m0.1733\u001b[0m       0.4917        \u001b[35m0.1667\u001b[0m  0.1305\n",
      "     41        \u001b[36m0.1716\u001b[0m       0.4917        \u001b[35m0.1652\u001b[0m  0.1416\n",
      "     42        \u001b[36m0.1700\u001b[0m       0.4917        \u001b[35m0.1639\u001b[0m  0.1244\n",
      "     43        \u001b[36m0.1683\u001b[0m       0.4917        \u001b[35m0.1625\u001b[0m  0.1206\n",
      "     44        \u001b[36m0.1666\u001b[0m       0.4917        \u001b[35m0.1612\u001b[0m  0.1225\n",
      "     45        \u001b[36m0.1651\u001b[0m       0.4917        \u001b[35m0.1598\u001b[0m  0.1275\n",
      "     46        \u001b[36m0.1636\u001b[0m       0.4917        \u001b[35m0.1586\u001b[0m  0.2350\n",
      "     47        \u001b[36m0.1620\u001b[0m       0.4917        \u001b[35m0.1574\u001b[0m  0.1245\n",
      "     48        \u001b[36m0.1607\u001b[0m       0.4917        \u001b[35m0.1563\u001b[0m  0.1251\n",
      "     49        \u001b[36m0.1592\u001b[0m       0.4917        \u001b[35m0.1551\u001b[0m  0.1296\n",
      "     50        \u001b[36m0.1579\u001b[0m       0.4917        \u001b[35m0.1541\u001b[0m  0.1273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# ANN 1  - Lalonde CPS \n",
    "ANN1_metrics = pd.DataFrame()\n",
    "ANN2_metrics = pd.DataFrame()\n",
    "ANN3_metrics = pd.DataFrame()\n",
    "ANN4_metrics = pd.DataFrame()\n",
    "\n",
    "kfold_evaluation_ANN(ANN1,X1,Y1,ANN1_metrics)\n",
    "kfold_evaluation_ANN(ANN2,X2,Y2,ANN2_metrics)\n",
    "kfold_evaluation_ANN(ANN3,X3,Y3,ANN3_metrics)\n",
    "kfold_evaluation_ANN(ANN4,X4,Y4,ANN4_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN1_metrics['avg'] = np.mean(ANN1_metrics,axis=1)\n",
    "ANN2_metrics['avg'] = np.mean(ANN2_metrics,axis=1)\n",
    "ANN3_metrics['avg'] = np.mean(ANN3_metrics,axis=1)\n",
    "ANN4_metrics['avg'] = np.mean(ANN4_metrics,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy:</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision:</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall:</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE:</th>\n",
       "      <td>0.018109</td>\n",
       "      <td>0.018109</td>\n",
       "      <td>0.018416</td>\n",
       "      <td>0.018416</td>\n",
       "      <td>0.018115</td>\n",
       "      <td>0.018233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE:</th>\n",
       "      <td>0.018109</td>\n",
       "      <td>0.018109</td>\n",
       "      <td>0.018416</td>\n",
       "      <td>0.018416</td>\n",
       "      <td>0.018115</td>\n",
       "      <td>0.018233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R^2:</th>\n",
       "      <td>-0.018443</td>\n",
       "      <td>-0.018443</td>\n",
       "      <td>-0.018762</td>\n",
       "      <td>-0.018762</td>\n",
       "      <td>-0.018449</td>\n",
       "      <td>-0.018572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc:</th>\n",
       "      <td>0.969495</td>\n",
       "      <td>0.970465</td>\n",
       "      <td>0.954842</td>\n",
       "      <td>0.953067</td>\n",
       "      <td>0.961846</td>\n",
       "      <td>0.961943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1:</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logloss:</th>\n",
       "      <td>0.252304</td>\n",
       "      <td>0.244009</td>\n",
       "      <td>0.245483</td>\n",
       "      <td>0.215336</td>\n",
       "      <td>0.214754</td>\n",
       "      <td>0.234377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4       avg\n",
       "Accuracy:   0.500000  0.500000  0.500000  0.500000  0.500000  0.500000\n",
       "Precision:  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "Recall:     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "MSE:        0.018109  0.018109  0.018416  0.018416  0.018115  0.018233\n",
       "MAE:        0.018109  0.018109  0.018416  0.018416  0.018115  0.018233\n",
       "R^2:       -0.018443 -0.018443 -0.018762 -0.018762 -0.018449 -0.018572\n",
       "roc_auc:    0.969495  0.970465  0.954842  0.953067  0.961846  0.961943\n",
       "F1:         0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "logloss:    0.252304  0.244009  0.245483  0.215336  0.214754  0.234377"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ANN1_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdc635352b0>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from turtle import color\n",
    "history = ANN1.history\n",
    "tloss = history[:, 'train_loss']\n",
    "vloss = history[:, 'valid_loss']\n",
    "plt.plot(tloss, '-',color='b',label='training')\n",
    "plt.plot(vloss, '-',color='r', label='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy:</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision:</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall:</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE:</th>\n",
       "      <td>0.107527</td>\n",
       "      <td>0.107527</td>\n",
       "      <td>0.105925</td>\n",
       "      <td>0.105925</td>\n",
       "      <td>0.105925</td>\n",
       "      <td>0.106566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE:</th>\n",
       "      <td>0.107527</td>\n",
       "      <td>0.107527</td>\n",
       "      <td>0.105925</td>\n",
       "      <td>0.105925</td>\n",
       "      <td>0.105925</td>\n",
       "      <td>0.106566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R^2:</th>\n",
       "      <td>-0.120482</td>\n",
       "      <td>-0.120482</td>\n",
       "      <td>-0.118474</td>\n",
       "      <td>-0.118474</td>\n",
       "      <td>-0.118474</td>\n",
       "      <td>-0.119277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc:</th>\n",
       "      <td>0.926439</td>\n",
       "      <td>0.942687</td>\n",
       "      <td>0.916752</td>\n",
       "      <td>0.920785</td>\n",
       "      <td>0.952165</td>\n",
       "      <td>0.931766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1:</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logloss:</th>\n",
       "      <td>0.345867</td>\n",
       "      <td>0.313366</td>\n",
       "      <td>0.400458</td>\n",
       "      <td>0.373793</td>\n",
       "      <td>0.347314</td>\n",
       "      <td>0.356159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4       avg\n",
       "Accuracy:   0.500000  0.500000  0.500000  0.500000  0.500000  0.500000\n",
       "Precision:  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "Recall:     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "MSE:        0.107527  0.107527  0.105925  0.105925  0.105925  0.106566\n",
       "MAE:        0.107527  0.107527  0.105925  0.105925  0.105925  0.106566\n",
       "R^2:       -0.120482 -0.120482 -0.118474 -0.118474 -0.118474 -0.119277\n",
       "roc_auc:    0.926439  0.942687  0.916752  0.920785  0.952165  0.931766\n",
       "F1:         0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "logloss:    0.345867  0.313366  0.400458  0.373793  0.347314  0.356159"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ANN2_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdc6391d3a0>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from turtle import color\n",
    "history = ANN2.history\n",
    "tloss = history[:, 'train_loss']\n",
    "vloss = history[:, 'valid_loss']\n",
    "plt.plot(tloss, '-',color='b',label='training')\n",
    "plt.plot(vloss, '-',color='r', label='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy:</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision:</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall:</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE:</th>\n",
       "      <td>0.011434</td>\n",
       "      <td>0.011434</td>\n",
       "      <td>0.011437</td>\n",
       "      <td>0.011437</td>\n",
       "      <td>0.011437</td>\n",
       "      <td>0.011436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE:</th>\n",
       "      <td>0.011434</td>\n",
       "      <td>0.011434</td>\n",
       "      <td>0.011437</td>\n",
       "      <td>0.011437</td>\n",
       "      <td>0.011437</td>\n",
       "      <td>0.011436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R^2:</th>\n",
       "      <td>-0.011566</td>\n",
       "      <td>-0.011566</td>\n",
       "      <td>-0.011570</td>\n",
       "      <td>-0.011570</td>\n",
       "      <td>-0.011570</td>\n",
       "      <td>-0.011568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc:</th>\n",
       "      <td>0.912143</td>\n",
       "      <td>0.893962</td>\n",
       "      <td>0.918285</td>\n",
       "      <td>0.885114</td>\n",
       "      <td>0.962464</td>\n",
       "      <td>0.914394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1:</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logloss:</th>\n",
       "      <td>0.242658</td>\n",
       "      <td>0.226241</td>\n",
       "      <td>0.214563</td>\n",
       "      <td>0.245935</td>\n",
       "      <td>0.219753</td>\n",
       "      <td>0.229830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4       avg\n",
       "Accuracy:   0.500000  0.500000  0.500000  0.500000  0.500000  0.500000\n",
       "Precision:  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "Recall:     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "MSE:        0.011434  0.011434  0.011437  0.011437  0.011437  0.011436\n",
       "MAE:        0.011434  0.011434  0.011437  0.011437  0.011437  0.011436\n",
       "R^2:       -0.011566 -0.011566 -0.011570 -0.011570 -0.011570 -0.011568\n",
       "roc_auc:    0.912143  0.893962  0.918285  0.885114  0.962464  0.914394\n",
       "F1:         0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "logloss:    0.242658  0.226241  0.214563  0.245935  0.219753  0.229830"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ANN3_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdc64177a60>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from turtle import color\n",
    "history = ANN3.history\n",
    "tloss = history[:, 'train_loss']\n",
    "vloss = history[:, 'valid_loss']\n",
    "plt.plot(tloss, '-',color='b',label='training')\n",
    "plt.plot(vloss, '-',color='r', label='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy:</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision:</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall:</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE:</th>\n",
       "      <td>0.069159</td>\n",
       "      <td>0.069159</td>\n",
       "      <td>0.069159</td>\n",
       "      <td>0.069159</td>\n",
       "      <td>0.069159</td>\n",
       "      <td>0.069159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE:</th>\n",
       "      <td>0.069159</td>\n",
       "      <td>0.069159</td>\n",
       "      <td>0.069159</td>\n",
       "      <td>0.069159</td>\n",
       "      <td>0.069159</td>\n",
       "      <td>0.069159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R^2:</th>\n",
       "      <td>-0.074297</td>\n",
       "      <td>-0.074297</td>\n",
       "      <td>-0.074297</td>\n",
       "      <td>-0.074297</td>\n",
       "      <td>-0.074297</td>\n",
       "      <td>-0.074297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc:</th>\n",
       "      <td>0.935255</td>\n",
       "      <td>0.948252</td>\n",
       "      <td>0.941007</td>\n",
       "      <td>0.956936</td>\n",
       "      <td>0.969255</td>\n",
       "      <td>0.950141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1:</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logloss:</th>\n",
       "      <td>0.301638</td>\n",
       "      <td>0.275192</td>\n",
       "      <td>0.282770</td>\n",
       "      <td>0.297883</td>\n",
       "      <td>0.237991</td>\n",
       "      <td>0.279095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4       avg\n",
       "Accuracy:   0.500000  0.500000  0.500000  0.500000  0.500000  0.500000\n",
       "Precision:  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "Recall:     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "MSE:        0.069159  0.069159  0.069159  0.069159  0.069159  0.069159\n",
       "MAE:        0.069159  0.069159  0.069159  0.069159  0.069159  0.069159\n",
       "R^2:       -0.074297 -0.074297 -0.074297 -0.074297 -0.074297 -0.074297\n",
       "roc_auc:    0.935255  0.948252  0.941007  0.956936  0.969255  0.950141\n",
       "F1:         0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "logloss:    0.301638  0.275192  0.282770  0.297883  0.237991  0.279095"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ANN4_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdc64b605b0>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from turtle import color\n",
    "history = ANN4.history\n",
    "tloss = history[:, 'train_loss']\n",
    "vloss = history[:, 'valid_loss']\n",
    "plt.plot(tloss, '-',color='b',label='training')\n",
    "plt.plot(vloss, '-',color='r', label='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propensity_score_functANN(dataset,model,dataset2,dehwab):\n",
    "    \n",
    "    dataset2.columns = dataset2.columns.str.strip() \n",
    "    \n",
    "    if dehwab == True: \n",
    "        dataset2 = dataset2[['treat','age','education*','black','hispanic','married','nodegree','re74','re75','re78']]\n",
    "    else:\n",
    "        dataset2 = dataset2[['treat','age','education*','black','hispanic','married','nodegree','re75','re78']]\n",
    "\n",
    "    treat =  dataset.iloc[:,0]\n",
    "    dataset =  dataset.iloc[:,1:len(dataset)]\n",
    "    columns = dataset.columns\n",
    "    dataset = dataset.to_numpy()\n",
    "    dataset = dataset.astype(np.float32)\n",
    "    #Y1 = np.reshape(Y1,(Y1.shape[0],1))\n",
    "    # Generate propensity score prediction  \n",
    "    probabilities = model.predict_proba(dataset)\n",
    "    probabilities = pd.DataFrame(probabilities)\n",
    "    ps = probabilities # propensity score \n",
    "    # merge prediction and existing dataset \n",
    "    dataset = pd.DataFrame(dataset)\n",
    "    dataset.columns = columns\n",
    "    #dataset_proba = pd.merge(dataset, ps, left_index=True, right_index=True)\n",
    "    dataset_proba = pd.merge(dataset2, ps, left_index=True, right_index=True)\n",
    "    dataset_proba.rename(index=int, columns={0:'propensity_score'}, inplace = True) # rename column\n",
    "    dataset_proba['propensity_logit'] = np.log(dataset_proba['propensity_score'] / (1-dataset_proba['propensity_score']))\n",
    "    return dataset_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict get scores on NSW , CPS and PSID \n",
    "# ============ Lalonde Subsample ============ # \n",
    "# predict propensity scores\n",
    "nswCps_lalonde_ps_ANN = propensity_score_functANN(nswCps_lalonde_subset,ANN1,nswCps_lalonde,False)\n",
    "nswPsid_lalonde_ps_ANN= propensity_score_functANN(nswPsid_lalonde_subset,ANN2,nswPsid_lalonde,False)\n",
    "# ============ Dehejia & Wahba sub sample ============ # \n",
    "# predict propensity scores\n",
    "nswCps_dehWab_ps_ANN = propensity_score_functANN(nswCps_dehWab_subset,ANN3,nswCps_dehWab,True)\n",
    "nswPsid_dehWab_ps_ANN = propensity_score_functANN(nswPsid_dehWab_subset,ANN4,nswPsid_dehWab,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>treat</th>\n",
       "      <th>age</th>\n",
       "      <th>education*</th>\n",
       "      <th>black</th>\n",
       "      <th>hispanic</th>\n",
       "      <th>married</th>\n",
       "      <th>nodegree</th>\n",
       "      <th>re75</th>\n",
       "      <th>re78</th>\n",
       "      <th>propensity_score</th>\n",
       "      <th>propensity_logit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9930</td>\n",
       "      <td>0.159121</td>\n",
       "      <td>-1.664782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3595</td>\n",
       "      <td>0.982575</td>\n",
       "      <td>4.032281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24909</td>\n",
       "      <td>0.120149</td>\n",
       "      <td>-1.991023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>27</td>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7506</td>\n",
       "      <td>0.018375</td>\n",
       "      <td>-3.978224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>33</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>289</td>\n",
       "      <td>0.025632</td>\n",
       "      <td>-3.637934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2782</th>\n",
       "      <td>0.0</td>\n",
       "      <td>47</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33837</td>\n",
       "      <td>38568</td>\n",
       "      <td>0.030807</td>\n",
       "      <td>-3.448728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2783</th>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67137</td>\n",
       "      <td>59109</td>\n",
       "      <td>0.967974</td>\n",
       "      <td>3.408644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2784</th>\n",
       "      <td>0.0</td>\n",
       "      <td>47</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>47968</td>\n",
       "      <td>55710</td>\n",
       "      <td>0.061125</td>\n",
       "      <td>-2.731761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2785</th>\n",
       "      <td>0.0</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44220</td>\n",
       "      <td>20540</td>\n",
       "      <td>0.160278</td>\n",
       "      <td>-1.656164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2786</th>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55500</td>\n",
       "      <td>53198</td>\n",
       "      <td>0.022137</td>\n",
       "      <td>-3.788136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2787 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      treat  age  education*  black  hispanic  married  nodegree   re75  \\\n",
       "0       1.0   37          11    1.0       0.0      1.0       1.0      0   \n",
       "1       1.0   22           9    0.0       1.0      0.0       1.0      0   \n",
       "2       1.0   30          12    1.0       0.0      0.0       0.0      0   \n",
       "3       1.0   27          11    1.0       0.0      0.0       1.0      0   \n",
       "4       1.0   33           8    1.0       0.0      0.0       1.0      0   \n",
       "...     ...  ...         ...    ...       ...      ...       ...    ...   \n",
       "2782    0.0   47           8    0.0       0.0      1.0       1.0  33837   \n",
       "2783    0.0   32           8    0.0       0.0      1.0       1.0  67137   \n",
       "2784    0.0   47          10    0.0       0.0      1.0       1.0  47968   \n",
       "2785    0.0   54           0    0.0       1.0      1.0       1.0  44220   \n",
       "2786    0.0   40           8    0.0       0.0      1.0       1.0  55500   \n",
       "\n",
       "       re78  propensity_score  propensity_logit  \n",
       "0      9930          0.159121         -1.664782  \n",
       "1      3595          0.982575          4.032281  \n",
       "2     24909          0.120149         -1.991023  \n",
       "3      7506          0.018375         -3.978224  \n",
       "4       289          0.025632         -3.637934  \n",
       "...     ...               ...               ...  \n",
       "2782  38568          0.030807         -3.448728  \n",
       "2783  59109          0.967974          3.408644  \n",
       "2784  55710          0.061125         -2.731761  \n",
       "2785  20540          0.160278         -1.656164  \n",
       "2786  53198          0.022137         -3.788136  \n",
       "\n",
       "[2787 rows x 11 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nswPsid_lalonde_ps_ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save unmatched boost datasets \n",
    "nswCps_lalonde_ps_ANN.to_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/ann/unmatched/nswCps_lalonde_ps_unmatched_ANN_FS1.csv')\n",
    "nswPsid_lalonde_ps_ANN.to_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/ann/unmatched/nswPsid_lalonde_ps_unmatched_ANN_FS1.csv')\n",
    "nswCps_dehWab_ps_ANN.to_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/ann/unmatched/nswCps_dehWab_ps_unmatched_ANN_FS1.csv')\n",
    "nswPsid_dehWab_ps_ANN.to_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/ann/unmatched/nswPsid_dehWab_ps_unmatched_ANN_FS1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bar plots of model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'boost1_metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/python-script/propensity score estimation with DAG feature selection (best params).ipynb Cell 120\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/python-script/propensity%20score%20estimation%20with%20DAG%20feature%20selection%20%28best%20params%29.ipynb#ch0000116?line=0'>1</a>\u001b[0m \u001b[39m# Average accuracy\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/python-script/propensity%20score%20estimation%20with%20DAG%20feature%20selection%20%28best%20params%29.ipynb#ch0000116?line=1'>2</a>\u001b[0m nsw_cps_lalonde_acc \u001b[39m=\u001b[39m [ANN1_metrics\u001b[39m.\u001b[39mloc[\u001b[39m'\u001b[39m\u001b[39mAccuracy:\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mavg\u001b[39m\u001b[39m'\u001b[39m],boost1_metrics\u001b[39m.\u001b[39mloc[\u001b[39m'\u001b[39m\u001b[39mAccuracy:\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mavg\u001b[39m\u001b[39m'\u001b[39m],forest1_metrics\u001b[39m.\u001b[39mloc[\u001b[39m'\u001b[39m\u001b[39mAccuracy:\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mavg\u001b[39m\u001b[39m'\u001b[39m],cart1_metrics\u001b[39m.\u001b[39mloc[\u001b[39m'\u001b[39m\u001b[39mAccuracy:\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mavg\u001b[39m\u001b[39m'\u001b[39m],logit1_metrics\u001b[39m.\u001b[39mloc[\u001b[39m'\u001b[39m\u001b[39mAccuracy:\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mavg\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/python-script/propensity%20score%20estimation%20with%20DAG%20feature%20selection%20%28best%20params%29.ipynb#ch0000116?line=2'>3</a>\u001b[0m nsw_psid_lalonde_acc \u001b[39m=\u001b[39m [ANN2_metrics\u001b[39m.\u001b[39mloc[\u001b[39m'\u001b[39m\u001b[39mAccuracy:\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mavg\u001b[39m\u001b[39m'\u001b[39m],boost2_metrics\u001b[39m.\u001b[39mloc[\u001b[39m'\u001b[39m\u001b[39mAccuracy:\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mavg\u001b[39m\u001b[39m'\u001b[39m],forest2_metrics\u001b[39m.\u001b[39mloc[\u001b[39m'\u001b[39m\u001b[39mAccuracy:\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mavg\u001b[39m\u001b[39m'\u001b[39m],cart2_metrics\u001b[39m.\u001b[39mloc[\u001b[39m'\u001b[39m\u001b[39mAccuracy:\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mavg\u001b[39m\u001b[39m'\u001b[39m],logit2_metrics\u001b[39m.\u001b[39mloc[\u001b[39m'\u001b[39m\u001b[39mAccuracy:\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mavg\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/python-script/propensity%20score%20estimation%20with%20DAG%20feature%20selection%20%28best%20params%29.ipynb#ch0000116?line=3'>4</a>\u001b[0m nsw_cps_dehWab_acc \u001b[39m=\u001b[39m [ANN3_metrics\u001b[39m.\u001b[39mloc[\u001b[39m'\u001b[39m\u001b[39mAccuracy:\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mavg\u001b[39m\u001b[39m'\u001b[39m],boost3_metrics\u001b[39m.\u001b[39mloc[\u001b[39m'\u001b[39m\u001b[39mAccuracy:\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mavg\u001b[39m\u001b[39m'\u001b[39m],forest3_metrics\u001b[39m.\u001b[39mloc[\u001b[39m'\u001b[39m\u001b[39mAccuracy:\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mavg\u001b[39m\u001b[39m'\u001b[39m],cart3_metrics\u001b[39m.\u001b[39mloc[\u001b[39m'\u001b[39m\u001b[39mAccuracy:\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mavg\u001b[39m\u001b[39m'\u001b[39m],logit3_metrics\u001b[39m.\u001b[39mloc[\u001b[39m'\u001b[39m\u001b[39mAccuracy:\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mavg\u001b[39m\u001b[39m'\u001b[39m]]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'boost1_metrics' is not defined"
     ]
    }
   ],
   "source": [
    "# Average accuracy\n",
    "nsw_cps_lalonde_acc = [ANN1_metrics.loc['Accuracy:','avg'],boost1_metrics.loc['Accuracy:','avg'],forest1_metrics.loc['Accuracy:','avg'],cart1_metrics.loc['Accuracy:','avg'],logit1_metrics.loc['Accuracy:','avg']]\n",
    "nsw_psid_lalonde_acc = [ANN2_metrics.loc['Accuracy:','avg'],boost2_metrics.loc['Accuracy:','avg'],forest2_metrics.loc['Accuracy:','avg'],cart2_metrics.loc['Accuracy:','avg'],logit2_metrics.loc['Accuracy:','avg']]\n",
    "nsw_cps_dehWab_acc = [ANN3_metrics.loc['Accuracy:','avg'],boost3_metrics.loc['Accuracy:','avg'],forest3_metrics.loc['Accuracy:','avg'],cart3_metrics.loc['Accuracy:','avg'],logit3_metrics.loc['Accuracy:','avg']]\n",
    "nsw_psid_dehWab_acc = [ANN4_metrics.loc['Accuracy:','avg'],boost4_metrics.loc['Accuracy:','avg'],forest4_metrics.loc['Accuracy:','avg'],cart4_metrics.loc['Accuracy:','avg'],logit4_metrics.loc['Accuracy:','avg']]\n",
    "\n",
    "# Average precison\n",
    "nsw_cps_lalonde_prec = [ANN1_metrics.loc['Precision:','avg'],boost1_metrics.loc['Precision:','avg'],forest1_metrics.loc['Precision:','avg'],cart1_metrics.loc['Precision:','avg'],logit1_metrics.loc['Precision:','avg']]\n",
    "nsw_psid_lalonde_prec = [ANN2_metrics.loc['Precision:','avg'],boost2_metrics.loc['Precision:','avg'],forest2_metrics.loc['Precision:','avg'],cart2_metrics.loc['Precision:','avg'],logit2_metrics.loc['Precision:','avg']]\n",
    "nsw_cps_dehWab_prec = [ANN3_metrics.loc['Precision:','avg'],boost3_metrics.loc['Precision:','avg'],forest3_metrics.loc['Precision:','avg'],cart3_metrics.loc['Precision:','avg'],logit3_metrics.loc['Precision:','avg']]\n",
    "nsw_psid_dehWab_prec = [ANN4_metrics.loc['Precision:','avg'],boost4_metrics.loc['Precision:','avg'],forest4_metrics.loc['Precision:','avg'],cart4_metrics.loc['Precision:','avg'],logit4_metrics.loc['Precision:','avg']]\n",
    "\n",
    "# Average recall \n",
    "nsw_cps_lalonde_rec = [ANN1_metrics.loc['Recall:','avg'],boost1_metrics.loc['Recall:','avg'],forest1_metrics.loc['Recall:','avg'],cart1_metrics.loc['Recall:','avg'],logit1_metrics.loc['Recall:','avg']]\n",
    "nsw_psid_lalonde_rec = [ANN2_metrics.loc['Recall:','avg'],boost2_metrics.loc['Recall:','avg'],forest2_metrics.loc['Recall:','avg'],cart2_metrics.loc['Recall:','avg'],logit2_metrics.loc['Recall:','avg']]\n",
    "nsw_cps_dehWab_rec = [ANN3_metrics.loc['Recall:','avg'],boost3_metrics.loc['Recall:','avg'],forest3_metrics.loc['Recall:','avg'],cart3_metrics.loc['Recall:','avg'],logit3_metrics.loc['Recall:','avg']]\n",
    "nsw_psid_dehWab_rec = [ANN4_metrics.loc['Recall:','avg'],boost4_metrics.loc['Recall:','avg'],forest4_metrics.loc['Recall:','avg'],cart4_metrics.loc['Recall:','avg'],logit4_metrics.loc['Recall:','avg']]\n",
    "\n",
    "# Average F1 \n",
    "nsw_cps_lalonde_f1 = [ANN1_metrics.loc['F1:','avg'],boost1_metrics.loc['F1:','avg'],forest1_metrics.loc['F1:','avg'],cart1_metrics.loc['F1:','avg'],logit1_metrics.loc['F1:','avg']]\n",
    "nsw_psid_lalonde_f1 = [ANN2_metrics.loc['F1:','avg'],boost2_metrics.loc['F1:','avg'],forest2_metrics.loc['F1:','avg'],cart2_metrics.loc['F1:','avg'],logit2_metrics.loc['F1:','avg']]\n",
    "nsw_cps_dehWab_f1 = [ANN3_metrics.loc['F1:','avg'],boost3_metrics.loc['F1:','avg'],forest3_metrics.loc['F1:','avg'],cart3_metrics.loc['F1:','avg'],logit3_metrics.loc['F1:','avg']]\n",
    "nsw_psid_dehWab_f1 = [ANN4_metrics.loc['F1:','avg'],boost4_metrics.loc['F1:','avg'],forest4_metrics.loc['F1:','avg'],cart4_metrics.loc['F1:','avg'],logit4_metrics.loc['F1:','avg']]\n",
    "\n",
    "# Average log loss\n",
    "nsw_cps_lalonde_logloss = [ANN1_metrics.loc['logloss:','avg'],boost1_metrics.loc['log-loss:','avg'],forest1_metrics.loc['log-loss:','avg'],cart1_metrics.loc['log-loss:','avg'],logit1_metrics.loc['log-loss:','avg']]\n",
    "nsw_psid_lalonde_logloss = [ANN2_metrics.loc['logloss:','avg'],boost2_metrics.loc['log-loss:','avg'],forest2_metrics.loc['log-loss:','avg'],cart2_metrics.loc['log-loss:','avg'],logit2_metrics.loc['log-loss:','avg']]\n",
    "nsw_cps_dehWab_logloss = [ANN3_metrics.loc['logloss:','avg'],boost3_metrics.loc['log-loss:','avg'],forest3_metrics.loc['log-loss:','avg'],cart3_metrics.loc['log-loss:','avg'],logit3_metrics.loc['log-loss:','avg']]\n",
    "nsw_psid_dehWab_logloss = [ANN4_metrics.loc['logloss:','avg'],boost4_metrics.loc['log-loss:','avg'],forest4_metrics.loc['log-loss:','avg'],cart4_metrics.loc['log-loss:','avg'],logit4_metrics.loc['log-loss:','avg']]\n",
    "\n",
    "# Average Roc Auc \n",
    "nsw_cps_lalonde_rocauc = [ANN1_metrics.loc['roc_auc:','avg'],boost1_metrics.loc['roc_auc:','avg'],forest1_metrics.loc['roc_auc:','avg'],cart1_metrics.loc['roc_auc:','avg'],logit1_metrics.loc['roc_auc:','avg']]\n",
    "nsw_psid_lalonde_rocauc = [ANN2_metrics.loc['roc_auc:','avg'],boost2_metrics.loc['roc_auc:','avg'],forest2_metrics.loc['roc_auc:','avg'],cart2_metrics.loc['roc_auc:','avg'],logit2_metrics.loc['roc_auc:','avg']]\n",
    "nsw_cps_dehWab_rocauc = [ANN3_metrics.loc['roc_auc:','avg'],boost3_metrics.loc['roc_auc:','avg'],forest3_metrics.loc['roc_auc:','avg'],cart3_metrics.loc['roc_auc:','avg'],logit3_metrics.loc['roc_auc:','avg']]\n",
    "nsw_psid_dehWab_rocauc = [ANN4_metrics.loc['roc_auc:','avg'],boost4_metrics.loc['roc_auc:','avg'],forest4_metrics.loc['roc_auc:','avg'],cart4_metrics.loc['roc_auc:','avg'],logit4_metrics.loc['roc_auc:','avg']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bundles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/python-script/propensity score estimation with DAG feature selection (best params).ipynb Cell 121\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/python-script/propensity%20score%20estimation%20with%20DAG%20feature%20selection%20%28best%20params%29.ipynb#ch0000117?line=4'>5</a>\u001b[0m \u001b[39m# Neat formatting for plots \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/python-script/propensity%20score%20estimation%20with%20DAG%20feature%20selection%20%28best%20params%29.ipynb#ch0000117?line=5'>6</a>\u001b[0m plt\u001b[39m.\u001b[39mrcParams\u001b[39m.\u001b[39mupdate({\u001b[39m\"\u001b[39m\u001b[39mfigure.dpi\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m300\u001b[39m}) \n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/python-script/propensity%20score%20estimation%20with%20DAG%20feature%20selection%20%28best%20params%29.ipynb#ch0000117?line=6'>7</a>\u001b[0m plt\u001b[39m.\u001b[39mrcParams\u001b[39m.\u001b[39mupdate(bundles\u001b[39m.\u001b[39mneurips2022())\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/python-script/propensity%20score%20estimation%20with%20DAG%20feature%20selection%20%28best%20params%29.ipynb#ch0000117?line=7'>8</a>\u001b[0m plt\u001b[39m.\u001b[39mrcParams\u001b[39m.\u001b[39mupdate(fonts\u001b[39m.\u001b[39mneurips2022())\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/python-script/propensity%20score%20estimation%20with%20DAG%20feature%20selection%20%28best%20params%29.ipynb#ch0000117?line=8'>9</a>\u001b[0m plt\u001b[39m.\u001b[39mrcParams\u001b[39m.\u001b[39mupdate(axes\u001b[39m.\u001b[39mtick_direction( y\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39min\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bundles' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x864 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create plot \n",
    "# Update plot paramaters \n",
    "fig, ax = plt.subplots(nrows=2, ncols=3,figsize = (20,12))\n",
    "\n",
    "# Neat formatting for plots \n",
    "plt.rcParams.update({\"figure.dpi\": 300}) \n",
    "plt.rcParams.update(bundles.neurips2022())\n",
    "plt.rcParams.update(fonts.neurips2022())\n",
    "plt.rcParams.update(axes.tick_direction( y=\"in\"))\n",
    "plt.rcParams.update(axes.color(base=\"black\"))\n",
    "plt.rcParams.update(figsizes.neurips2022(nrows=2, ncols=3))\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "y = np.arange(5)\n",
    "x = np.arange(1)\n",
    "width = 0.2\n",
    "# Balanced accuracy\n",
    "\n",
    "ax[0,0].set_title(\"Balanced accuracy\",fontsize=20)\n",
    "ax[0,0].set_xlabel('score',fontsize=16)\n",
    "ax[0,0].set_yticks(y+2*width, ['neural network', 'boosted tree', 'random forest', 'cart', 'logistic regression'],fontsize=16)\n",
    "ax[0,0].tick_params(axis='x', which='major', labelsize=16)\n",
    "\n",
    "colour1 = \"#E31B23\" # CPS lalonde\n",
    "colour2 = \"#e31b8c\" # PSID lalonde\n",
    "colour3 = \"#003366\" # CPS dehwab\n",
    "colour4 = \"#006614\" # PSID dehwab\n",
    "\n",
    "ax[0,0].barh(y,nsw_cps_lalonde_acc,width,color = colour1)\n",
    "ax[0,0].barh(y+width,nsw_psid_lalonde_acc,width,color = colour2)\n",
    "ax[0,0].barh(y+2*width,nsw_cps_dehWab_acc,width,color = colour3)\n",
    "ax[0,0].barh(y+(3*width),nsw_psid_dehWab_acc,width,color = colour4)\n",
    "\n",
    "ax[0,0].set_facecolor('white')\n",
    "ax[0,0].grid(color='white', axis='y')\n",
    "ax[0,0].grid(color='white', axis='x')\n",
    "ax[0,0].spines['left']\n",
    "ax[0,0].spines['right']\n",
    "ax[0,0].spines['bottom']\n",
    "ax[0,0].tick_params(right='on')\n",
    "ax[0,0].tick_params(left='on')\n",
    "ax[0,0].tick_params(bottom='on')\n",
    "\n",
    "# precision\n",
    "\n",
    "ax[0,1].set_title(\"precision\",fontsize=20)\n",
    "ax[0,1].set_xlabel('score',fontsize=16)\n",
    "ax[0,1].set_yticks(y+2*width, [' ', ' ', ' ', ' ', ' '],fontsize=16)\n",
    "ax[0,1].tick_params(axis='x', which='major', labelsize=16)\n",
    "\n",
    "ax[0,1].barh(y,nsw_cps_lalonde_prec,width,color = colour1)\n",
    "ax[0,1].barh(y+width,nsw_psid_lalonde_prec,width,color = colour2)\n",
    "ax[0,1].barh(y+2*width,nsw_cps_dehWab_prec,width,color = colour3)\n",
    "ax[0,1].barh(y+(3*width),nsw_psid_dehWab_prec,width,color = colour4)\n",
    "\n",
    "ax[0,1].set_facecolor('white')\n",
    "ax[0,1].grid(color='white', axis='y')\n",
    "ax[0,1].grid(color='white', axis='x')\n",
    "ax[0,1].spines['left']\n",
    "ax[0,1].spines['right']\n",
    "ax[0,1].spines['bottom']\n",
    "ax[0,1].tick_params(right='on')\n",
    "ax[0,1].tick_params(left='on')\n",
    "ax[0,1].tick_params(bottom='on')\n",
    "\n",
    "# recall \n",
    "\n",
    "ax[0,2].set_title(\"Recall\",fontsize=20)\n",
    "ax[0,2].set_xlabel('score',fontsize=16)\n",
    "ax[0,2].set_yticks(y+2*width, [' ', ' ', ' ', ' ', ' '],fontsize=16)\n",
    "ax[0,2].tick_params(axis='x', which='major', labelsize=16)\n",
    "\n",
    "ax[0,2].barh(y,nsw_cps_lalonde_rec,width,color = colour1)\n",
    "ax[0,2].barh(y+width,nsw_psid_lalonde_rec,width,color = colour2)\n",
    "ax[0,2].barh(y+2*width,nsw_cps_dehWab_rec,width,color = colour3)\n",
    "ax[0,2].barh(y+(3*width),nsw_psid_dehWab_rec,width,color = colour4)\n",
    "\n",
    "ax[0,2].set_facecolor('white')\n",
    "ax[0,2].grid(color='white', axis='y')\n",
    "ax[0,2].grid(color='white', axis='x')\n",
    "ax[0,2].spines['left']\n",
    "ax[0,2].spines['right']\n",
    "ax[0,2].spines['bottom']\n",
    "ax[0,2].tick_params(right='on')\n",
    "ax[0,2].tick_params(left='on')\n",
    "ax[0,2].tick_params(bottom='on')\n",
    "\n",
    "# F1\n",
    "ax[1,0].set_title(\"F1 score\",fontsize=20)\n",
    "ax[1,0].set_xlabel('score',fontsize=16)\n",
    "ax[1,0].set_yticks(y+2*width, ['neural network', 'boosted tree', 'random forest', 'cart', 'logistic regression'],fontsize=16)\n",
    "ax[1,0].tick_params(axis='x', which='major', labelsize=16)\n",
    "\n",
    "ax[1,0].barh(y,nsw_cps_lalonde_f1,width,color = colour1)\n",
    "ax[1,0].barh(y+width,nsw_psid_lalonde_f1,width,color = colour2)\n",
    "ax[1,0].barh(y+2*width,nsw_cps_dehWab_f1,width,color = colour3)\n",
    "ax[1,0].barh(y+(3*width),nsw_psid_dehWab_f1,width,color = colour4)\n",
    "\n",
    "ax[1,0].set_facecolor('white')\n",
    "ax[1,0].grid(color='white', axis='y')\n",
    "ax[1,0].grid(color='white', axis='x')\n",
    "ax[1,0].spines['left']\n",
    "ax[1,0].spines['right']\n",
    "ax[1,0].spines['bottom']\n",
    "ax[1,0].tick_params(right='on')\n",
    "ax[1,0].tick_params(left='on')\n",
    "ax[1,0].tick_params(bottom='on')\n",
    "\n",
    "\n",
    "# Log loss\n",
    "ax[1,1].set_title(\"Log loss\",fontsize=20)\n",
    "ax[1,1].set_xlabel('score',fontsize=16)\n",
    "ax[1,1].set_yticks(y+2*width, [' ', ' ', ' ', ' ', ' '],fontsize=16)\n",
    "ax[1,1].tick_params(axis='x', which='major', labelsize=16)\n",
    "\n",
    "ax[1,1].barh(y,nsw_cps_lalonde_logloss,width,color = colour1)\n",
    "ax[1,1].barh(y+width,nsw_psid_lalonde_logloss,width,color = colour2)\n",
    "ax[1,1].barh(y+2*width,nsw_cps_dehWab_logloss,width,color = colour3)\n",
    "ax[1,1].barh(y+(3*width),nsw_psid_dehWab_logloss,width,color = colour4)\n",
    "\n",
    "ax[1,1].set_facecolor('white')\n",
    "ax[1,1].grid(color='white', axis='y')\n",
    "ax[1,1].grid(color='white', axis='x')\n",
    "ax[1,1].spines['left']\n",
    "ax[1,1].spines['right']\n",
    "ax[1,1].spines['bottom']\n",
    "ax[1,1].tick_params(right='on')\n",
    "ax[1,1].tick_params(left='on')\n",
    "ax[1,1].tick_params(bottom='on')\n",
    "\n",
    "# Roc-AUC\n",
    "ax[1,2].set_title(\"Roc-Auc\",fontsize=20)\n",
    "ax[1,2].set_xlabel('score',fontsize=16)\n",
    "ax[1,2].set_yticks(y+2*width, [' ', ' ', ' ', ' ', ' '],fontsize=16)\n",
    "ax[1,2].tick_params(axis='x', which='major', labelsize=16)\n",
    "\n",
    "ax[1,2].barh(y,nsw_cps_lalonde_rocauc,width,color = colour1)\n",
    "ax[1,2].barh(y+width,nsw_psid_lalonde_rocauc,width,color = colour2)\n",
    "ax[1,2].barh(y+2*width,nsw_cps_dehWab_rocauc,width,color = colour3)\n",
    "ax[1,2].barh(y+(3*width),nsw_psid_dehWab_rocauc,width,color = colour4)\n",
    "\n",
    "ax[1,2].set_facecolor('white')\n",
    "ax[1,2].grid(color='white', axis='y')\n",
    "ax[1,2].grid(color='white', axis='x')\n",
    "ax[1,2].spines['left']\n",
    "ax[1,2].spines['right']\n",
    "ax[1,2].spines['bottom']\n",
    "ax[1,2].tick_params(right='on')\n",
    "ax[1,2].tick_params(left='on')\n",
    "ax[1,2].tick_params(bottom='on')\n",
    "\n",
    "labels = ['CPS - Lalonde','PSID - Lalonde','CPS - DW','PSID - DW']\n",
    "fig.legend(labels, loc='lower center', bbox_to_anchor=(0.5,-0.1), ncol=len(labels),markerscale=3, bbox_transform=fig.transFigure,prop={'size': 18})\n",
    "\n",
    "plt.savefig('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/Jupyter notebooks/lalonde_notebook_plots/fig8_training_evaluation_averages.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis on model outptus - Probability calibration curves "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nswCps_lalonde =  pd.read_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/unmatched data/Quasi_NswCps_lalonde.csv')\n",
    "nswPsid_lalonde = pd.read_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/unmatched data/Quasi_NswPsid_lalonde.csv')\n",
    "nswCps_dehWab =   pd.read_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/unmatched data/Quasi_NswCps_dehWab.csv')\n",
    "nswPsid_dehWab =  pd.read_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/unmatched data/Quasi_NswPsid_dehWab.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree      \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Take a random sample from the data with an even number of treated and control for testing\n",
    "\n",
    "nswCps_lalonde_valid = nswCps_lalonde.groupby('treat',group_keys=False).apply(lambda x: x.sample(50))\n",
    "nswCps_lalonde = nswCps_lalonde.drop(nswCps_lalonde_valid.index) # Remove all samples from training which are in the sample \n",
    "\n",
    "nswPsid_lalonde_valid = nswPsid_lalonde.groupby('treat',group_keys=False).apply(lambda x: x.sample(50))\n",
    "nswPsid_lalonde = nswPsid_lalonde.drop(nswPsid_lalonde_valid.index) # Remove all samples from training which are in the sample \n",
    "\n",
    "nswCps_dehWab_valid = nswCps_dehWab.groupby('treat',group_keys=False).apply(lambda x: x.sample(50))\n",
    "nswCps_dehWab = nswCps_dehWab.drop(nswCps_dehWab_valid.index)# Remove all samples from training which are in the sample \n",
    "\n",
    "nswPsid_Dehwab_valid = nswPsid_dehWab.groupby('treat',group_keys=False).apply(lambda x: x.sample(50))\n",
    "nswPsid_dehWab = nswPsid_dehWab.drop(nswPsid_Dehwab_valid.index) # Remove all samples from training which are in the sample \n",
    "\n",
    "# covariates used in each mdoel \n",
    "logit_vars = ['treat','age','agesq','education*','educsq','black','hispanic','married','nodegree','re75','u75','educ_re75']\n",
    "logit_vars2 = ['treat','age','agesq','education*','educsq','black','hispanic','married','nodegree','re74','re75','u74','u75','educ_re75']\n",
    "tree_vars = ['treat','age','education*','black','hispanic','married','nodegree','re75']\n",
    "tree_vars2 = ['treat','age','education*','black','hispanic','married','nodegree','re74','re75']\n",
    "ann_vars = ['treat','ageboxcox','black','hispanic','married','nodegree','lnre75','education_8', 'education_9','education_10', 'education_11', 'education_12', 'education_13','education_14', 'education_15', 'education_16', 'education_17']\n",
    "ann_vars2 = ['treat','ageboxcox','black','hispanic','married','nodegree','lnre74','lnre75','education_8', 'education_9','education_10', 'education_11', 'education_12',  'education_13', 'education_14', 'education_15', 'education_16', 'education_17']\n",
    "continuos_vars = ['ageboxcox','lnre75'] \n",
    "continuos_vars2= ['ageboxcox','lnre74','lnre75'] \n",
    "# ======== Lalonde - sample ======== #\n",
    "\n",
    "# ~~~~~~~~~~\n",
    "# 1 cps\n",
    "# ~~~~~~~~~~\n",
    "\n",
    "#logit model \n",
    "cps_lalonde_subset_logit = nswCps_lalonde[logit_vars]\n",
    "cps_lalonde_subset_valid_logit = nswCps_lalonde_valid[logit_vars]\n",
    "#decision tree models \n",
    "cps_lalonde_subset_trees = nswCps_lalonde[tree_vars]\n",
    "cps_lalonde_subset_valid_trees = nswCps_lalonde_valid[tree_vars]\n",
    "#ann model \n",
    "cps_lalonde_subset_ann = nswCps_lalonde[ann_vars]\n",
    "cps_lalonde_subset_valid_ann = nswCps_lalonde_valid[ann_vars]\n",
    "cps_lalonde_subset_ann[continuos_vars] = sc.fit_transform(cps_lalonde_subset_ann[continuos_vars])\n",
    "cps_lalonde_subset_valid_ann[continuos_vars] = sc.fit_transform(cps_lalonde_subset_valid_ann[continuos_vars])\n",
    "\n",
    "# ~~~~~~~~~~\n",
    "# 2 PSID \n",
    "# ~~~~~~~~~~\n",
    "\n",
    "#logit model \n",
    "psid_lalonde_subset_logit = nswPsid_lalonde[logit_vars]\n",
    "psid_lalonde_subset_valid_logit = nswPsid_lalonde_valid[logit_vars]\n",
    "#decision tree models \n",
    "psid_lalonde_subset_trees = nswPsid_lalonde[tree_vars]\n",
    "psid_lalonde_subset_valid_trees = nswPsid_lalonde_valid[tree_vars]\n",
    "#ann model \n",
    "psid_lalonde_subset_ann = nswPsid_lalonde[ann_vars]\n",
    "psid_lalonde_subset_valid_ann = nswPsid_lalonde_valid[ann_vars]\n",
    "psid_lalonde_subset_ann[continuos_vars] = sc.fit_transform(psid_lalonde_subset_ann[continuos_vars])\n",
    "psid_lalonde_subset_valid_ann[continuos_vars] = sc.fit_transform(psid_lalonde_subset_valid_ann[continuos_vars])\n",
    "\n",
    "\n",
    "# ======== Dehejia - Wahba sample ======== #\n",
    "\n",
    "# ~~~~~~~~~~\n",
    "# 3 CPS\n",
    "# ~~~~~~~~~~\n",
    "# logit \n",
    "cps_dehWab_subset_logit = nswCps_dehWab[logit_vars2]\n",
    "cps_dehWab_subset_valid_logit = nswCps_dehWab_valid[logit_vars2]\n",
    "# decision tree's\n",
    "cps_dehWab_subset_trees = nswCps_dehWab[tree_vars2]\n",
    "cps_dehWab_subset_valid_trees = nswCps_dehWab_valid[tree_vars2]\n",
    "#ann\n",
    "cps_dehWab_subset_ann = nswCps_dehWab[ann_vars2]\n",
    "cps_dehWab_subset_valid_ann = nswCps_dehWab_valid[ann_vars2]\n",
    "cps_dehWab_subset_ann[continuos_vars] = sc.fit_transform(cps_dehWab_subset_ann[continuos_vars])\n",
    "cps_dehWab_subset_valid_ann[continuos_vars] = sc.fit_transform(cps_dehWab_subset_valid_ann[continuos_vars])\n",
    "\n",
    "# ~~~~~~~~~~\n",
    "# 4 PSID\n",
    "# ~~~~~~~~~~\n",
    "# logit \n",
    "psid_dehWab_subset_logit = nswCps_dehWab[logit_vars2]\n",
    "psid_dehWab_subset_valid_logit = nswCps_dehWab_valid[logit_vars2]\n",
    "# decision tree's\n",
    "psid_dehWab_subset_trees = nswCps_dehWab[tree_vars2]\n",
    "psid_dehWab_subset_valid_trees = nswCps_dehWab_valid[tree_vars2]\n",
    "#ann\n",
    "psid_dehWab_subset_ann = nswCps_dehWab[ann_vars2]\n",
    "psid_dehWab_subset_valid_ann = nswCps_dehWab_valid[ann_vars2]\n",
    "psid_dehWab_subset_ann[continuos_vars] = sc.fit_transform(cps_dehWab_subset_ann[continuos_vars])\n",
    "psid_dehWab_subset_valid_ann[continuos_vars] = sc.fit_transform(cps_dehWab_subset_valid_ann[continuos_vars])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to run all models and get a prediction - using the best features from our grid search \n",
    "# Inputs are the dataset's we want to look at \n",
    "\n",
    "\n",
    "def fit_and_predict_all_models(logit_data,logit_valid,tree_data,tree_valid,ann_data,ann_valid,neuralnet):\n",
    "    \n",
    "    '''\n",
    "    could make this function do all the smote and rejoing the validation set again sincce i am making my own, so there are more training examples\n",
    "    \n",
    "    '''\n",
    "   \n",
    "  \n",
    "    \n",
    "    #Train test split \n",
    "    def return_trainTest_split(Dataset):\n",
    "        # shuffle data\n",
    "        Dataset = Dataset.sample(frac = 1,random_state=0)\n",
    "        Features = Dataset.drop('treat', axis=1)\n",
    "        Target = Dataset['treat']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(Features, Target, test_size=0.3, shuffle=True)\n",
    "        return X_train, X_test, y_train, y_test;\n",
    "\n",
    "    \n",
    "    resample=SMOTEENN(random_state=0) # resampling\n",
    "    \n",
    "    '''\n",
    "    '''\n",
    "    \n",
    "    # Logit\n",
    "    \n",
    "    X_train1, X_test1, y_train1, y_test1 = return_trainTest_split(logit_data)\n",
    "    X_train1, y_train1 = resample.fit_resample(X_train1, y_train1)\t\n",
    "    \n",
    "    shuffled = pd.concat([pd.DataFrame(X_train1),pd.DataFrame(y_train1)],axis=1).sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "    X_train1, y_train1 = shuffled.iloc[:,:-1], shuffled.iloc[:,-1]\n",
    "    \n",
    "    logit_demo = LogisticRegression(penalty='none',solver='lbfgs', class_weight=\"balanced\",max_iter=500,random_state=0)\n",
    "    \n",
    "    logit_demo.fit(X_train1,y_train1)\n",
    "    \n",
    "    logit_valid = logit_valid.sample(frac = 1,random_state=0)\n",
    "    X_test1 = logit_valid.drop('treat',axis=1, inplace = False)\n",
    "    y_test1 = logit_valid['treat']\n",
    "    \n",
    "    predicted_proba_logit = logit_demo.predict_proba(X_test1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Decision tree models \n",
    "    \n",
    "    X_train2, X_test2, y_train2, y_test2 = return_trainTest_split(tree_data)\n",
    "\n",
    "    X_train2, y_train2 = resample.fit_resample(X_train2, y_train2)\t\n",
    "    shuffled = pd.concat([pd.DataFrame(X_train2),pd.DataFrame(y_train2)],axis=1).sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "    X_train2, y_train2 = shuffled.iloc[:,:-1], shuffled.iloc[:,-1]\n",
    "\n",
    "    tree_valid = tree_valid.sample(frac = 1,random_state=0)\n",
    "    X_test2 = tree_valid.drop('treat',axis=1, inplace = False)\n",
    "    y_test2 = tree_valid['treat']\n",
    "\n",
    "    # Cart\n",
    "    \n",
    "    CART_demo = tree.DecisionTreeClassifier(random_state=0,criterion = 'entropy',max_depth = 5, max_leaf_nodes=12,min_samples_leaf=2)\n",
    "    CART_demo.fit(X_train2,y_train2)\n",
    "    predicted_proba_cart = CART_demo.predict_proba(X_test2)\n",
    "    #RF\n",
    "    forest_demo = RandomForestClassifier(random_state=0,criterion='entropy',max_depth=8,max_features='auto',min_samples_split=3,n_estimators=500)\n",
    "    forest_demo.fit(X_train2,y_train2)\n",
    "    predicted_proba_forest = forest_demo.predict_proba(X_test2)\n",
    "    #boost\n",
    "    boost_demo = XGBClassifier(objective= 'binary:logistic',booser='dart',colsample_bytree=0.3,gamma=0.5,learning_rate=0.05,mind_child_weight=1,subsample=0.5,seed=0,nthread=4)  \n",
    "    boost_demo.fit(X_train2,y_train2)\n",
    "    predicted_proba_boost = boost_demo.predict_proba(X_test2)\n",
    "\n",
    "    #ANN\n",
    "    X_train3, X_test3, y_train3, y_test3 = return_trainTest_split(ann_data)\n",
    "    X_train3, y_train3 = resample.fit_resample(X_train3, y_train3)\t\n",
    "    shuffled = pd.concat([pd.DataFrame(X_train3),pd.DataFrame(y_train3)],axis=1).sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "    X_train3, y_train3 = shuffled.iloc[:,:-1], shuffled.iloc[:,-1]\n",
    "\n",
    "    ann_valid = ann_valid.sample(frac = 1,random_state=0)\n",
    "    X_test3 = ann_valid.drop('treat',axis=1, inplace = False)\n",
    "    y_test3 = ann_valid['treat']\n",
    "\n",
    "    Epochs = 100\n",
    "    learning_rate = 0.0001\n",
    "    weight_decay = 0.05\n",
    "    batch_size = 256\n",
    "\n",
    "    X_train3 = X_train3.to_numpy()\n",
    "    y_train3 = y_train3.to_numpy()\n",
    "    X_train3 = X_train3.astype(np.float32)\n",
    "    y_train3 = y_train3.astype(np.float32)\n",
    "    y_train3 = np.reshape(y_train3,(y_train3.shape[0],1))\n",
    "\n",
    "    X_test3 = X_test3.to_numpy()\n",
    "    y_test3 = y_test3.to_numpy()\n",
    "    X_test3 = X_test3.astype(np.float32)\n",
    "    y_test3 = y_test3.astype(np.float32)\n",
    "    y_test3 = np.reshape(y_test3,(y_test3.shape[0],1))\n",
    "\n",
    "    ann_demo = NeuralNetClassifier(neuralnet,max_epochs=Epochs,lr=learning_rate,optimizer__weight_decay=weight_decay,batch_size =batch_size,optimizer = optim.Adam,criterion = nn.BCELoss,iterator_valid__shuffle=False,verbose=0 )\n",
    "\n",
    "    ann_demo.fit(X_train3,y_train3)\n",
    "    predicted_proba_ANN = ann_demo.predict_proba(X_test3)\n",
    "    \n",
    "    return predicted_proba_logit,predicted_proba_cart,predicted_proba_forest,predicted_proba_boost,predicted_proba_ANN,y_test1,y_test2,y_test3;\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logit_proba1,cart_proba1,forest_proba1,boost_proba1,ann_proba1,yl1,yt1,ya1 = fit_and_predict_all_models(cps_lalonde_subset_logit,\n",
    "                                                                                            cps_lalonde_subset_valid_logit,\n",
    "                                                                                            cps_lalonde_subset_trees,\n",
    "                                                                                            cps_lalonde_subset_valid_trees,\n",
    "                                                                                            cps_lalonde_subset_ann,\n",
    "                                                                                            cps_lalonde_subset_valid_ann,\n",
    "                                                                                            twoLayerNN1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logit_proba2,cart_proba2,forest_proba2,boost_proba2,ann_proba2,yl2,yt2,ya2 = fit_and_predict_all_models(psid_lalonde_subset_logit,\n",
    "                                                                                            psid_lalonde_subset_valid_logit,\n",
    "                                                                                            psid_lalonde_subset_trees,\n",
    "                                                                                            psid_lalonde_subset_valid_trees,\n",
    "                                                                                            psid_lalonde_subset_ann,\n",
    "                                                                                            psid_lalonde_subset_valid_ann,\n",
    "                                                                                            twoLayerNN1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logit_proba3,cart_proba3,forest_proba3,boost_proba3,ann_proba3,yl3,yt3,ya3 = fit_and_predict_all_models(cps_dehWab_subset_logit,\n",
    "                                                                                            cps_dehWab_subset_valid_logit,\n",
    "                                                                                            cps_dehWab_subset_trees,\n",
    "                                                                                            cps_dehWab_subset_valid_trees,\n",
    "                                                                                            cps_dehWab_subset_ann,\n",
    "                                                                                            cps_dehWab_subset_valid_ann,\n",
    "                                                                                            twoLayerNN2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logit_proba4,cart_proba4,forest_proba4,boost_proba4,ann_proba4,yl4,yt4,ya4 = fit_and_predict_all_models(psid_dehWab_subset_logit,\n",
    "                                                                                            psid_dehWab_subset_valid_logit,\n",
    "                                                                                            psid_dehWab_subset_trees,\n",
    "                                                                                            psid_dehWab_subset_valid_trees,\n",
    "                                                                                            psid_dehWab_subset_ann,\n",
    "                                                                                            psid_dehWab_subset_valid_ann,\n",
    "                                                                                            twoLayerNN2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration curve for each models prediction \n",
    "from sklearn.calibration import calibration_curve\n",
    "logit_fop1  , logit_mpv1  = calibration_curve(yl1, logit_proba1[:,1], n_bins=10, normalize=True)\n",
    "cart_fop1   , cart_mpv1   = calibration_curve(yt1, cart_proba1[:,1], n_bins=10, normalize=True)\n",
    "forest_fop1  , forest_mpv1 = calibration_curve(yt1, forest_proba1[:,1], n_bins=10, normalize=True)\n",
    "boost_fop1  , boost_mpv1  = calibration_curve(yt1, boost_proba1[:,1], n_bins=10, normalize=True)\n",
    "ann_fop1    , ann_mpv1    = calibration_curve(ya1, ann_proba1, n_bins=10, normalize=True)\n",
    "\n",
    "logit_fop2  , logit_mpv2  = calibration_curve(yl2, logit_proba2[:,1], n_bins=10, normalize=True)\n",
    "cart_fop2   , cart_mpv2   = calibration_curve(yt2, cart_proba2[:,1], n_bins=10, normalize=True)\n",
    "forest_fop2  , forest_mpv2 = calibration_curve(yt2, forest_proba2[:,1], n_bins=10, normalize=True)\n",
    "boost_fop2  , boost_mpv2  = calibration_curve(yt2, boost_proba2[:,1], n_bins=10, normalize=True)\n",
    "ann_fop2    , ann_mpv2    = calibration_curve(ya2, ann_proba2, n_bins=10, normalize=True)\n",
    "\n",
    "logit_fop3  , logit_mpv3  = calibration_curve(yl3, logit_proba3[:,1], n_bins=10, normalize=True)\n",
    "cart_fop3   , cart_mpv3   = calibration_curve(yt3, cart_proba3[:,1], n_bins=10, normalize=True)\n",
    "forest_fop3  , forest_mpv3 = calibration_curve(yt3, forest_proba3[:,1], n_bins=10, normalize=True)\n",
    "boost_fop3  , boost_mpv3  = calibration_curve(yt3, boost_proba3[:,1], n_bins=10, normalize=True)\n",
    "ann_fop3    , ann_mpv3    = calibration_curve(ya3, ann_proba3, n_bins=10, normalize=True)\n",
    "\n",
    "logit_fop4  , logit_mpv4  = calibration_curve(yl4, logit_proba4[:,1], n_bins=10, normalize=True)\n",
    "cart_fop4   , cart_mpv4   = calibration_curve(yt4, cart_proba4[:,1], n_bins=10, normalize=True)\n",
    "forest_fop4  , forest_mpv4 = calibration_curve(yt4, forest_proba4[:,1], n_bins=10, normalize=True)\n",
    "boost_fop4  , boost_mpv4  = calibration_curve(yt4, boost_proba4[:,1], n_bins=10, normalize=True)\n",
    "ann_fop4    , ann_mpv4    = calibration_curve(ya4, ann_proba4, n_bins=10, normalize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import scipy.stats as stat\n",
    "import pylab \n",
    "from tueplots import axes, bundles , figsizes, fonts,fontsizes\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2,figsize = (8,6))\n",
    "\n",
    "plt.rcParams.update({\"figure.dpi\": 150}) \n",
    "plt.rcParams.update(bundles.neurips2022())\n",
    "plt.rcParams.update(fonts.neurips2022())\n",
    "plt.rcParams.update(axes.tick_direction( y=\"in\"))\n",
    "plt.rcParams.update(axes.color(base=\"black\"))\n",
    "plt.rcParams.update(axes.lines())\n",
    "\n",
    "\n",
    "ax[0,0].grid(color='white', axis='x')\n",
    "ax[0,0].grid(color='white', axis='y')\n",
    "ax[0,1].grid(color='white', axis='x')\n",
    "ax[0,1].grid(color='white', axis='y')\n",
    "ax[1,0].grid(color='white', axis='x')\n",
    "ax[1,0].grid(color='white', axis='y')\n",
    "ax[1,1].grid(color='white', axis='x')\n",
    "ax[1,1].grid(color='white', axis='y')\n",
    "\n",
    "\n",
    "marker = 's'\n",
    "linewidth = 1\n",
    "marker_size = 3\n",
    "\n",
    "ax[0,0].plot([0, 1], [0, 1], linestyle='dotted', label='perfectly calibrated',linewidth=1,color='black')\n",
    "ax[0,0].plot(logit_mpv1  , logit_fop1 , marker = marker , label='Logit ',linewidth=linewidth,markersize=marker_size)\n",
    "ax[0,0].plot(cart_mpv1   , cart_fop1, marker  = marker  ,label='cart ',linewidth=linewidth,markersize=marker_size)\n",
    "ax[0,0].plot(forest_mpv1 , forest_fop1,marker = marker  , label='forest ',linewidth=linewidth,markersize=marker_size)\n",
    "ax[0,0].plot(boost_mpv1  , boost_fop1,marker = marker  , label='boost ',linewidth=linewidth,markersize=marker_size)\n",
    "ax[0,0].plot(ann_mpv1    , ann_fop1 ,marker = marker , label='ANN ',linewidth=linewidth,markersize=marker_size)\n",
    "ax[0,0].set_xlabel('Mean predicted value')\n",
    "ax[0,0].set_ylabel('Fraction of positives')\n",
    "\n",
    "ax[0,1].plot([0, 1], [0, 1], linestyle='dotted', label='perfectly calibrated',linewidth=1,color='black')\n",
    "ax[0,1].plot(logit_mpv2  , logit_fop2 , marker = marker , label='Logit ',linewidth=linewidth,markersize=marker_size)\n",
    "ax[0,1].plot(cart_mpv2   , cart_fop2  , marker  = marker  ,label='cart ',linewidth=linewidth,markersize=marker_size)\n",
    "ax[0,1].plot(forest_mpv2 , forest_fop2 ,marker = marker  , label='forest ',linewidth=linewidth,markersize=marker_size)\n",
    "ax[0,1].plot(boost_mpv2  , boost_fop2  ,marker = marker  , label='boost ',linewidth=linewidth,markersize=marker_size)\n",
    "ax[0,1].plot(ann_mpv2    , ann_fop2    , marker = marker , label='ANN ',linewidth=linewidth,markersize=marker_size)\n",
    "ax[0,1].set_xlabel('Mean predicted value')\n",
    "ax[0,1].set_ylabel('Fraction of positives')\n",
    "\n",
    "ax[1,0].plot([0, 1], [0, 1], linestyle='dotted', label='perfectly calibrated',linewidth=1,color='black')\n",
    "ax[1,0].plot(logit_mpv3  , logit_fop3  ,marker = marker , label='Logit ',linewidth=linewidth,markersize=marker_size)\n",
    "ax[1,0].plot(cart_mpv3   , cart_fop3   ,marker  = marker  ,label='cart ',linewidth=linewidth,markersize=marker_size)\n",
    "ax[1,0].plot(forest_mpv3 , forest_fop3 ,marker = marker  , label='forest ',linewidth=linewidth,markersize=marker_size)\n",
    "ax[1,0].plot(boost_mpv3  , boost_fop3  ,marker = marker  , label='boost ',linewidth=linewidth,markersize=marker_size)\n",
    "ax[1,0].plot(ann_mpv3    , ann_fop3    , marker = marker , label='ANN ',linewidth=linewidth,markersize=marker_size)\n",
    "ax[1,0].set_xlabel('Mean predicted value')\n",
    "ax[1,0].set_ylabel('Fraction of positives')\n",
    "\n",
    "ax[1,1].plot([0, 1], [0, 1], linestyle='dotted', label='perfectly calibrated',linewidth=linewidth,color='black')\n",
    "ax[1,1].plot(logit_mpv4  , logit_fop4  ,marker = marker , label='Logit ',linewidth=linewidth,markersize=marker_size)\n",
    "ax[1,1].plot(cart_mpv4   , cart_fop4   ,marker  = marker  ,label='cart ',linewidth=linewidth,markersize=marker_size)\n",
    "ax[1,1].plot(forest_mpv4 , forest_fop4 ,marker = marker  , label='forest ',linewidth=linewidth,markersize=marker_size)\n",
    "ax[1,1].plot(boost_mpv4  , boost_fop4  ,marker = marker  , label='boost ',linewidth=linewidth,markersize=marker_size)\n",
    "ax[1,1].plot(ann_mpv4    , ann_fop4    , marker = marker , label='ANN ',linewidth=linewidth,markersize=marker_size)\n",
    "ax[1,1].set_xlabel('Mean predicted value')\n",
    "ax[1,1].set_ylabel('Fraction of positives')\n",
    "\n",
    "\n",
    "plt.legend(loc=(1.04, 0.1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Box plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Box plots of estimated propensity scores across each model - CPS group \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "# Neat formatting for plots \n",
    "plt.rcParams.update({\"figure.dpi\": 150}) \n",
    "plt.rcParams.update(bundles.neurips2022())\n",
    "plt.rcParams.update(fonts.neurips2022())\n",
    "plt.rcParams.update(axes.tick_direction( y=\"in\"))\n",
    "plt.rcParams.update(axes.color(base=\"black\"))\n",
    "plt.rcParams.update(figsizes.neurips2022())\n",
    "plt.rcParams.update(fontsizes.neurips2022())\n",
    "\n",
    "ax.set_facecolor('white')\n",
    "# Set boarder colour \n",
    "for spine in ax.spines:\n",
    "    ax.spines[spine].set_color('black')\n",
    "    ax.spines[spine].set_linewidth(1)\n",
    "\n",
    "ax.grid(color='darkgrey', axis='y', linestyle='--', linewidth=1, alpha=1)\n",
    "ax.grid(color='white', axis='x')\n",
    "\n",
    "# Cusotmise median line \n",
    "medianprops = dict(linestyle='-', linewidth = 1, color='k')\n",
    "boxprops = dict(linewidth = 1)\n",
    "whiskerprops = dict(linewidth = 1)\n",
    "capprops = dict(linewidth = 1)\n",
    "\n",
    "flierprops = {'marker': 'o', 'markersize': 3,'linewidth':1}\n",
    "\n",
    "\n",
    "treated_propensity_PS = logitNswCps_lalonde['propensity_score'][logitNswCps_lalonde['treat']==1]\n",
    "\n",
    "Logit_Cpscomprison_PS = logitNswCps_lalonde['propensity_score'][logitNswCps_lalonde['treat']==0]\n",
    "Logit_Psidcomprison_PS = logitNswPsid_lalonde['propensity_score'][logitNswPsid_lalonde['treat']==0]\n",
    "\n",
    "cart_Cpscomprison_PS = cartNswCps_lalonde['propensity_score'][cartNswCps_lalonde['treat']==0]\n",
    "cart_Psidcomprison_PS = cartNswPsid_lalonde['propensity_score'][cartNswPsid_lalonde['treat']==0]\n",
    "\n",
    "ax.boxplot([treated_propensity_PS,Logit_Cpscomprison_PS,Logit_Psidcomprison_PS,cart_Cpscomprison_PS,cart_Psidcomprison_PS],\n",
    "            medianprops=medianprops,\n",
    "            boxprops = boxprops,\n",
    "            whiskerprops=whiskerprops,\n",
    "            capprops = capprops ,\n",
    "            flierprops = flierprops,\n",
    "            widths=0.3) \n",
    "\n",
    "#ax.set_xlim(0.5,5.5)\n",
    "#ax.set_ylim(-2000,62000)\n",
    "\n",
    "plt.xticks([1,2,3,4,5],['Treated', 'Logit cps ','Logit psid','Cart  cps','Cart psid'])\n",
    "\n",
    "\n",
    "ax.set_xlabel('Model')\n",
    "\n",
    "ax.set_ylabel('Propensity score')\n",
    "\n",
    "plt.savefig('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/Jupyter notebooks/lalonde_notebook_plots/_boxplots.png', bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "44a7c5cf9512a4ad670122a007d348488b56f79d3796a47615dc74da9f36a764"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
