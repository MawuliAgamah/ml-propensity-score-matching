{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries \n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import random \n",
    "import numpy.random as rand\n",
    "from random import randrange\n",
    "from scipy.stats import bernoulli, binom\n",
    "import seaborn as sns\n",
    "from scipy import stats as stat\n",
    "import pylab \n",
    "from tueplots import axes, bundles , figsizes, fonts,fontsizes\n",
    "from matplotlib.lines import Line2D\n",
    "from imblearn.combine import SMOTEENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for LR\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree          \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
    "from sklearn import metrics\n",
    "# Import libraries \n",
    "from sklearn import tree          \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Import libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import graphviz  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import  DataLoader,SubsetRandomSampler \n",
    "import torch.optim as optim # Optimization package\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that returns dataset with propensity scores and propensity logits from logistic regression\n",
    "def propensity_score_funct(dataset,model):\n",
    "    # Generate propensity score prediction \n",
    "    probabilities = model.predict_proba(dataset.drop('treat', axis=1))\n",
    "    probabilities = pd.DataFrame(probabilities)\n",
    "    ps = probabilities[1] # propensity score \n",
    "    # merge prediction and existing dataset \n",
    "    dataset_proba = pd.merge(dataset, ps, left_index=True, right_index=True)\n",
    "    dataset_proba.rename(index=int, columns={1:'propensity_score'}, inplace = True) # rename column\n",
    "    \n",
    "    #dataset_proba['propensity_logit'] = np.log(dataset_proba['propensity_score'] / (1-dataset_proba['propensity_score']))\n",
    "    dataset_proba['propensity_logit'] = pd.DataFrame(np.nan_to_num(np.log(dataset_proba['propensity_score'] / (1-dataset_proba['propensity_score'])),posinf=0,neginf=0))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample=SMOTEENN(random_state=0)\n",
    "# KFold cross validation \n",
    "def kfold_evaluation_SMOTEENN(input_model,features,target,store):\n",
    "\t# prepare the cross-validation procedure\n",
    "\tskf = StratifiedKFold(n_splits=5, shuffle=True,random_state=1)\n",
    "\t\n",
    "\t#Function to get model scores\t\n",
    "\tdef get_score(model,X_train , X_test, y_train , y_test,fold,store):\n",
    "\t\tmodel.fit(X_train,y_train)\n",
    "\t\tprediction = model.predict(X_test)\n",
    "\t\tpredicted_proba = model.predict_proba(X_test)\n",
    "\t\tprediction_proba_df = pd.DataFrame(predicted_proba)\n",
    "\t\tpos_predicted_proba = prediction_proba_df[1] # Take positive probability predictions for ROC_AUC score  \n",
    "\t\tstore.loc['Accuracy:', fold] = metrics.balanced_accuracy_score(y_test, prediction)\n",
    "\t\tstore.loc['Precision:', fold] = metrics.precision_score(y_test, prediction)\n",
    "\t\tstore.loc['Recall:', fold] = metrics.recall_score(y_test, prediction)\n",
    "\t\tstore.loc['MSE:', fold] =  metrics.mean_squared_error(y_test, prediction)\n",
    "\t\tstore.loc['MAE:', fold] =  metrics.mean_absolute_error(y_test, prediction)\n",
    "\t\tstore.loc['R^2:', fold] =  metrics.r2_score(y_test, prediction)\n",
    "\t\tstore.loc['auc:', fold] =  metrics.roc_auc_score(y_test, prediction)\n",
    "\t\tstore.loc['roc_auc:', fold] =  metrics.roc_auc_score(y_test, pos_predicted_proba)\n",
    "\t\tstore.loc['F1:', fold] =  metrics.f1_score(y_test, prediction)\n",
    "\t\tstore.loc['log-loss:', fold] = metrics.log_loss(y_test, predicted_proba)\n",
    "\t\n",
    "\t#Kfold training loop\n",
    "\tfor fold, (train_index , test_index) in enumerate(skf.split(features,target)):\n",
    "\t\t#print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "\t\tX_train , X_test, y_train , y_test = features.iloc[train_index,:],features.iloc[test_index,:],\\\n",
    "\t\t\t                                 target[train_index] , target[test_index]\n",
    "\t\t\t\t\t\t\t\t\t\t\t \n",
    "\t\tX_train, y_train = resample.fit_resample(X_train, y_train)\t\n",
    "\t\tshuffled = pd.concat([pd.DataFrame(X_train),pd.DataFrame(y_train)],axis=1).sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "\t\t#resplit into test and train \n",
    "\t\tX_train, y_train = shuffled.iloc[:,:-1], shuffled.iloc[:,-1]\n",
    "\t\tget_score(input_model,X_train , X_test, y_train , y_test,fold,store)\n",
    "\t\t# return statement here with python dataframe of metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Load data sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load all datasets.\n",
    "nsw = Male sub sample of treated and control units as used by Lalonde (1986).\n",
    "nswre74  = Further subsample extracted by Dehejia and Wahba (1999) which includes information on earnings in 1974 (re74)\n",
    "\n",
    "cps1 = full Current Population Survey dataset\n",
    "cps2 , cps3 and cps4 are further subsamples exctracted by lalonde to with distributions which better match the nsw treated group.\n",
    "\n",
    "psid1 = full Panel Study of Income Dynamics dataset\n",
    "psid2,psid3,psid4 are again subsamples extracted by Lalonde \n",
    "\n",
    "'''\n",
    "# experimental data \n",
    "\n",
    "nsw = pd.read_stata(\"/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/nsw.dta\")\n",
    "nsw.drop('data_id', axis=1, inplace=True)\n",
    "nswre74_control = pd.read_excel('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/nswre74_control.xls')\n",
    "nswre74_treated = pd.read_excel('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/nswre74_treated.xlsx')\n",
    "nswre74 = pd.concat([nswre74_control,nswre74_treated],axis=0,ignore_index=True) #Combine treatment into a single control as pandas dataframe\n",
    "\n",
    "# non experimental data (cps)\n",
    "\n",
    "cps = pd.read_stata(\"/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/cps_controls.dta\")\n",
    "cps.drop('data_id', axis=1, inplace=True)\n",
    "\n",
    "#cps['treat'] == 0 # created treated column\n",
    "#load non experimental data (psid)\n",
    "\n",
    "psid = pd.read_stata(\"/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/psid_controls.dta\")\n",
    "psid.drop('data_id', axis=1, inplace=True)\n",
    "\n",
    "# reset indexes\n",
    "\n",
    "nsw = nsw.reset_index(drop=True)\n",
    "nswre74 = nswre74.reset_index(drop=True)\n",
    "cps = cps.reset_index(drop=True)\n",
    "psid = psid.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PS estimation helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propensity_score_funct(dataset,model,dataset2,dehwab):\n",
    "    \n",
    "    dataset2.columns = dataset2.columns.str.strip() \n",
    "    \n",
    "    if dehwab == True: \n",
    "        dataset2 = dataset2[['treat','age','education*','black','hispanic','married','nodegree','re74','re75','re78']]\n",
    "    else:\n",
    "        dataset2 = dataset2[['treat','age','education*','black','hispanic','married','nodegree','re75','re78']]\n",
    "\n",
    "    treat =  dataset.iloc[:,0]\n",
    "    dataset =  dataset.iloc[:,1:len(dataset)]\n",
    "\n",
    "    probabilities = model.predict_proba(dataset)\n",
    "    probabilities = pd.DataFrame(probabilities)\n",
    "    ps = probabilities[1]  # propensity score \n",
    "\n",
    "    dataset_proba = pd.merge(dataset2, ps, left_index=True, right_index=True)\n",
    "    dataset_proba.rename(index=int, columns={1:'propensity_score'}, inplace = True) # rename column\n",
    "    dataset_proba['propensity_logit'] = np.log(dataset_proba['propensity_score'] / (1-dataset_proba['propensity_score']))\n",
    "    return dataset_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample=SMOTEENN(random_state=0)\n",
    "# KFold cross validation \n",
    "def kfold_evaluation_SMOTEENN(input_model,features,target,store):\n",
    "\t# prepare the cross-validation procedure\n",
    "\tskf = StratifiedKFold(n_splits=5, shuffle=True,random_state=1)\n",
    "\t\n",
    "\t#Function to get model scores\t\n",
    "\tdef get_score(model,X_train , X_test, y_train , y_test,fold,store):\n",
    "\t\tmodel.fit(X_train,y_train)\n",
    "\t\tprediction = model.predict(X_test)\n",
    "\t\tpredicted_proba = model.predict_proba(X_test)\n",
    "\t\tprediction_proba_df = pd.DataFrame(predicted_proba)\n",
    "\t\tpos_predicted_proba = prediction_proba_df[1] # Take positive probability predictions for ROC_AUC score  \n",
    "\t\tstore.loc['Accuracy:', fold] = metrics.balanced_accuracy_score(y_test, prediction)\n",
    "\t\tstore.loc['Precision:', fold] = metrics.precision_score(y_test, prediction)\n",
    "\t\tstore.loc['Recall:', fold] = metrics.recall_score(y_test, prediction)\n",
    "\t\tstore.loc['MSE:', fold] =  metrics.mean_squared_error(y_test, prediction)\n",
    "\t\tstore.loc['MAE:', fold] =  metrics.mean_absolute_error(y_test, prediction)\n",
    "\t\tstore.loc['R^2:', fold] =  metrics.r2_score(y_test, prediction)\n",
    "\t\tstore.loc['auc:', fold] =  metrics.roc_auc_score(y_test, prediction)\n",
    "\t\tstore.loc['roc_auc:', fold] =  metrics.roc_auc_score(y_test, pos_predicted_proba)\n",
    "\t\tstore.loc['F1:', fold] =  metrics.f1_score(y_test, prediction)\n",
    "\t\tstore.loc['log-loss:', fold] = metrics.log_loss(y_test, predicted_proba)\n",
    "\t\n",
    "\n",
    "\t#Kfold training loop\n",
    "\tfor fold, (train_index , test_index) in enumerate(skf.split(features,target)):\n",
    "\t\t#print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "\t\tX_train , X_test, y_train , y_test = features.iloc[train_index,:],features.iloc[test_index,:],\\\n",
    "\t\t\t                                 target[train_index] , target[test_index]\n",
    "\t\t\t\t\t\t\t\t\t\t\t \n",
    "\t\tX_train, y_train = resample.fit_resample(X_train, y_train)\t\n",
    "\t\tshuffled = pd.concat([pd.DataFrame(X_train),pd.DataFrame(y_train)],axis=1).sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "\t\t#resplit into test and train \n",
    "\t\tX_train, y_train = shuffled.iloc[:,:-1], shuffled.iloc[:,-1]\n",
    "\t\tget_score(input_model,X_train , X_test, y_train , y_test,fold,store)\n",
    "\t\t# return statement here with python dataframe of metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Propensity score estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load quasi-experimental dataset\n",
    "nswCps_lalonde =  pd.read_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/unmatched data/Quasi_NswCps_lalonde.csv')\n",
    "nswPsid_lalonde = pd.read_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/unmatched data/Quasi_NswPsid_lalonde.csv')\n",
    "nswCps_dehWab =   pd.read_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/unmatched data/Quasi_NswCps_dehWab.csv')\n",
    "nswPsid_dehWab =  pd.read_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/unmatched data/Quasi_NswPsid_dehWab.csv')\n",
    "\n",
    "nswCps_lalonde.columns = nswCps_lalonde.columns.str.strip() \n",
    "nswPsid_lalonde.columns = nswPsid_lalonde.columns.str.strip() \n",
    "nswCps_dehWab.columns = nswCps_dehWab.columns.str.strip() \n",
    "nswPsid_dehWab.columns = nswPsid_dehWab.columns.str.strip() \n",
    "\n",
    "# Store outcome variable prior to modelling\n",
    "nswCps_lalonde_re78 = nswCps_lalonde.re78\n",
    "nswPsid_lalonde_re78 = nswPsid_lalonde.re78\n",
    "nswCps_dehWab_re78 = nswCps_dehWab.re78\n",
    "nswPsid_dehWab_re78 = nswPsid_dehWab.re78"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PS specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_specification_logit_1 = ['treat','education*','black','hispanic','married','married_u75','nodegree','re75']\n",
    "ps_specification_logit_2 = ['treat','age','education*','black','married','hispanic','black_education','hisp_re75','nodegree','re75']\n",
    "ps_specification_logit_3 = ['treat','age','education*','black','hispanic','nodegree','black_age','married','re74','re75']\n",
    "ps_specification_logit_4 = ['treat','age','education*','black','married','hispanic','married_u75','nodegree','re74','re75']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_blanket_specification_1 = ['treat','age','education*','black','married','nodegree','re75']# CPS Lalonde sample \n",
    "markov_blanket_specification_2 = ['treat','age','education*','married','nodegree','re75']# PSID Lalonde sample  \n",
    "markov_blanket_specification_3 = ['treat','age','education*','black','married','re74','re75'] # CPS Dehwab sample\n",
    "markov_blanket_specification_4 = ['treat','age','education*','married','nodegree','re74','re75'] # PSID Dehwab and wahba sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "annSpecification_1 = ['treat','ageboxcox','education*','black','married','nodegree','re75'] # CPS Lalonde sample \n",
    "annSpecification_2 = ['treat','ageboxcox','education*','married','nodegree','re75']# PSID Lalonde sample  \n",
    "annSpecification_3 = ['treat','ageboxcox','education*','black','married','re74','re75'] # CPS Dehwab sample\n",
    "annSpecification_4 = ['treat','ageboxcox','education*','married','nodegree','re74','re75'] # PSID Dehwab and wahba sample \n",
    "nsw_continuos_vars1 = ['ageboxcox','re75'] \n",
    "nsw_continuos_vars2 = ['ageboxcox','re75'] \n",
    "nswre74_continuos_vars1 = ['ageboxcox','re75',] \n",
    "nswre74_continuos_vars2 = ['ageboxcox','re74','re75',] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model (1) Logistic Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select covariates for use in Logit model \n",
    "\n",
    "# ======== Lalonde (1986) data ======== #\n",
    "\n",
    "nswCps_lalonde_subset = nswCps_lalonde[ps_specification_logit_1]\n",
    "nswPsid_lalonde_subset = nswPsid_lalonde[ps_specification_logit_2]\n",
    "\n",
    "nswCps_lalonde_target = nswCps_lalonde_subset['treat']\n",
    "nswCps_lalonde_features = nswCps_lalonde_subset.drop('treat', axis=1)\n",
    "nswPsid_lalonde_target = nswPsid_lalonde_subset['treat']\n",
    "nswPsid_lalonde_features = nswPsid_lalonde_subset.drop('treat', axis=1)\n",
    "\n",
    "# ======== Dehejia & Wahba (1986) sub sample ======== #\n",
    "\n",
    "nswCps_dehWab_subset = nswCps_dehWab[ps_specification_logit_3]\n",
    "nswPsid_dehWab_subset = nswPsid_dehWab[ps_specification_logit_4]\n",
    "\n",
    "nswCps_dehWab_target = nswCps_dehWab_subset['treat']\n",
    "nswCps_dehWab_features = nswCps_dehWab_subset.drop('treat', axis=1)\n",
    "nswPsid_dehWab_target = nswPsid_dehWab_subset['treat']\n",
    "nswPsid_dehWab_features = nswPsid_dehWab_subset.drop('treat', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
    "from sklearn import metrics\n",
    "\n",
    "# KFold cross validation \n",
    "def kfold_evaluation(input_model,features,target,store):\n",
    "\t# prepare the cross-validation procedure\n",
    "\tskf = StratifiedKFold(n_splits=5, shuffle=True,random_state=1)\n",
    "\t\n",
    "\t#Function to get model scores\t\n",
    "\tdef get_score(model,X_train , X_test, y_train , y_test,fold,store):\n",
    "\t\tmodel.fit(X_train,y_train)\n",
    "\t\tprediction = model.predict(X_test)\n",
    "\t\tpredicted_proba = model.predict_proba(X_test)\n",
    "\t\tprediction_proba_df = pd.DataFrame(predicted_proba)\n",
    "\t\tpos_predicted_proba = prediction_proba_df[1] # Take positive probability predictions for ROC_AUC score  \n",
    "\t\tstore.loc['Accuracy:', fold] = metrics.balanced_accuracy_score(y_test, prediction)\n",
    "\t\tstore.loc['Precision:', fold] = metrics.precision_score(y_test, prediction)\n",
    "\t\tstore.loc['Recall:', fold] = metrics.recall_score(y_test, prediction)\n",
    "\t\tstore.loc['MSE:', fold] =  metrics.mean_squared_error(y_test, prediction)\n",
    "\t\tstore.loc['MAE:', fold] =  metrics.mean_absolute_error(y_test, prediction)\n",
    "\t\tstore.loc['R^2:', fold] =  metrics.r2_score(y_test, prediction)\n",
    "\t\tstore.loc['auc:', fold] =  metrics.roc_auc_score(y_test, prediction)\n",
    "\t\tstore.loc['roc_auc:', fold] =  metrics.roc_auc_score(y_test, pos_predicted_proba)\n",
    "\t\tstore.loc['F1:', fold] =  metrics.f1_score(y_test, prediction)\n",
    "\t\tstore.loc['log-loss:', fold] = metrics.log_loss(y_test, predicted_proba)\n",
    "\t\t\n",
    "\n",
    "\t#Kfold training loop\n",
    "\t\n",
    "\tfor fold, (train_index , test_index) in enumerate(skf.split(features,target)):\n",
    "\t\t#print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "\t\tX_train , X_test, y_train , y_test = features.iloc[train_index,:],features.iloc[test_index,:],\\\n",
    "\t\t\t                                 target[train_index] , target[test_index]\n",
    "\t\t\t\t\t\t\t\t\t\t\t \n",
    "\t\tget_score(input_model,X_train , X_test, y_train , y_test,fold,store)\n",
    "\t\t# return statement here with python dataframe of metrics \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# dataframes to store resutls \n",
    "logit1_metrics = pd.DataFrame()\n",
    "logit2_metrics = pd.DataFrame()\n",
    "logit3_metrics = pd.DataFrame()\n",
    "logit4_metrics = pd.DataFrame()\n",
    "#models \n",
    "logit1 = LogisticRegression(penalty='none',solver='lbfgs', class_weight=\"balanced\",max_iter=500,random_state=0)\n",
    "logit2 = LogisticRegression(penalty='none',solver='lbfgs', class_weight=\"balanced\",max_iter=500,random_state=0)\n",
    "logit3 = LogisticRegression(penalty='none',solver='lbfgs', class_weight=\"balanced\",max_iter=500,random_state=0)\n",
    "logit4 = LogisticRegression(penalty='none',solver='lbfgs', class_weight=\"balanced\",max_iter=500,random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model validation - K fold \n",
    "# ======== Lalonde (1986) sample ======== #\n",
    "kfold_evaluation(logit1,nswCps_lalonde_features,nswCps_lalonde_target,logit1_metrics) # cps\n",
    "kfold_evaluation(logit2,nswPsid_lalonde_features,nswPsid_lalonde_target,logit2_metrics) # psid\n",
    "# ======== Dehejia & Wahba (1999) sub sample ======== #\n",
    "kfold_evaluation(logit3,nswCps_dehWab_features,nswCps_dehWab_target,logit3_metrics) # cps\n",
    "kfold_evaluation(logit4,nswPsid_dehWab_features,nswPsid_dehWab_target,logit4_metrics) # psid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit1_metrics['avg'] = np.mean(logit1_metrics,axis=1)\n",
    "logit2_metrics['avg'] = np.mean(logit2_metrics,axis=1)\n",
    "logit3_metrics['avg'] = np.mean(logit3_metrics,axis=1)\n",
    "logit4_metrics['avg'] = np.mean(logit4_metrics,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy:</th>\n",
       "      <td>0.888868</td>\n",
       "      <td>0.864759</td>\n",
       "      <td>0.861991</td>\n",
       "      <td>0.900891</td>\n",
       "      <td>0.892650</td>\n",
       "      <td>0.881832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision:</th>\n",
       "      <td>0.121348</td>\n",
       "      <td>0.130890</td>\n",
       "      <td>0.112360</td>\n",
       "      <td>0.129496</td>\n",
       "      <td>0.096552</td>\n",
       "      <td>0.118129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall:</th>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.847458</td>\n",
       "      <td>0.915254</td>\n",
       "      <td>0.949153</td>\n",
       "      <td>0.889040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE:</th>\n",
       "      <td>0.121854</td>\n",
       "      <td>0.104972</td>\n",
       "      <td>0.124002</td>\n",
       "      <td>0.112953</td>\n",
       "      <td>0.161805</td>\n",
       "      <td>0.125117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE:</th>\n",
       "      <td>0.121854</td>\n",
       "      <td>0.104972</td>\n",
       "      <td>0.124002</td>\n",
       "      <td>0.112953</td>\n",
       "      <td>0.161805</td>\n",
       "      <td>0.125117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R^2:</th>\n",
       "      <td>-5.740807</td>\n",
       "      <td>-4.806942</td>\n",
       "      <td>-5.973747</td>\n",
       "      <td>-5.352324</td>\n",
       "      <td>-8.096994</td>\n",
       "      <td>-5.994163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc:</th>\n",
       "      <td>0.888868</td>\n",
       "      <td>0.864759</td>\n",
       "      <td>0.861991</td>\n",
       "      <td>0.900891</td>\n",
       "      <td>0.892650</td>\n",
       "      <td>0.881832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc:</th>\n",
       "      <td>0.962044</td>\n",
       "      <td>0.947871</td>\n",
       "      <td>0.945158</td>\n",
       "      <td>0.959619</td>\n",
       "      <td>0.964756</td>\n",
       "      <td>0.955889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1:</th>\n",
       "      <td>0.213861</td>\n",
       "      <td>0.226244</td>\n",
       "      <td>0.198413</td>\n",
       "      <td>0.226891</td>\n",
       "      <td>0.175274</td>\n",
       "      <td>0.208137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log-loss:</th>\n",
       "      <td>0.263571</td>\n",
       "      <td>0.241323</td>\n",
       "      <td>0.262708</td>\n",
       "      <td>0.270125</td>\n",
       "      <td>0.337827</td>\n",
       "      <td>0.275111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4       avg\n",
       "Accuracy:   0.888868  0.864759  0.861991  0.900891  0.892650  0.881832\n",
       "Precision:  0.121348  0.130890  0.112360  0.129496  0.096552  0.118129\n",
       "Recall:     0.900000  0.833333  0.847458  0.915254  0.949153  0.889040\n",
       "MSE:        0.121854  0.104972  0.124002  0.112953  0.161805  0.125117\n",
       "MAE:        0.121854  0.104972  0.124002  0.112953  0.161805  0.125117\n",
       "R^2:       -5.740807 -4.806942 -5.973747 -5.352324 -8.096994 -5.994163\n",
       "auc:        0.888868  0.864759  0.861991  0.900891  0.892650  0.881832\n",
       "roc_auc:    0.962044  0.947871  0.945158  0.959619  0.964756  0.955889\n",
       "F1:         0.213861  0.226244  0.198413  0.226891  0.175274  0.208137\n",
       "log-loss:   0.263571  0.241323  0.262708  0.270125  0.337827  0.275111"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit1_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'class_weight': 'balanced',\n",
       " 'dual': False,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1,\n",
       " 'l1_ratio': None,\n",
       " 'max_iter': 500,\n",
       " 'multi_class': 'auto',\n",
       " 'n_jobs': None,\n",
       " 'penalty': 'none',\n",
       " 'random_state': 0,\n",
       " 'solver': 'lbfgs',\n",
       " 'tol': 0.0001,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit1.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy:</th>\n",
       "      <td>0.869779</td>\n",
       "      <td>0.913855</td>\n",
       "      <td>0.862603</td>\n",
       "      <td>0.904976</td>\n",
       "      <td>0.917790</td>\n",
       "      <td>0.893801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision:</th>\n",
       "      <td>0.481132</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.546392</td>\n",
       "      <td>0.467213</td>\n",
       "      <td>0.523295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall:</th>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.813559</td>\n",
       "      <td>0.898305</td>\n",
       "      <td>0.966102</td>\n",
       "      <td>0.885593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE:</th>\n",
       "      <td>0.114695</td>\n",
       "      <td>0.075269</td>\n",
       "      <td>0.098743</td>\n",
       "      <td>0.089767</td>\n",
       "      <td>0.120287</td>\n",
       "      <td>0.099752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE:</th>\n",
       "      <td>0.114695</td>\n",
       "      <td>0.075269</td>\n",
       "      <td>0.098743</td>\n",
       "      <td>0.089767</td>\n",
       "      <td>0.120287</td>\n",
       "      <td>0.099752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R^2:</th>\n",
       "      <td>-0.195181</td>\n",
       "      <td>0.215663</td>\n",
       "      <td>-0.042645</td>\n",
       "      <td>0.052141</td>\n",
       "      <td>-0.270131</td>\n",
       "      <td>-0.048031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc:</th>\n",
       "      <td>0.869779</td>\n",
       "      <td>0.913855</td>\n",
       "      <td>0.862603</td>\n",
       "      <td>0.904976</td>\n",
       "      <td>0.917790</td>\n",
       "      <td>0.893801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc:</th>\n",
       "      <td>0.960793</td>\n",
       "      <td>0.972122</td>\n",
       "      <td>0.951603</td>\n",
       "      <td>0.972262</td>\n",
       "      <td>0.974032</td>\n",
       "      <td>0.966162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1:</th>\n",
       "      <td>0.614458</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.635762</td>\n",
       "      <td>0.679487</td>\n",
       "      <td>0.629834</td>\n",
       "      <td>0.655908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log-loss:</th>\n",
       "      <td>0.250577</td>\n",
       "      <td>0.200025</td>\n",
       "      <td>0.241414</td>\n",
       "      <td>0.204177</td>\n",
       "      <td>0.283941</td>\n",
       "      <td>0.236027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4       avg\n",
       "Accuracy:   0.869779  0.913855  0.862603  0.904976  0.917790  0.893801\n",
       "Precision:  0.481132  0.600000  0.521739  0.546392  0.467213  0.523295\n",
       "Recall:     0.850000  0.900000  0.813559  0.898305  0.966102  0.885593\n",
       "MSE:        0.114695  0.075269  0.098743  0.089767  0.120287  0.099752\n",
       "MAE:        0.114695  0.075269  0.098743  0.089767  0.120287  0.099752\n",
       "R^2:       -0.195181  0.215663 -0.042645  0.052141 -0.270131 -0.048031\n",
       "auc:        0.869779  0.913855  0.862603  0.904976  0.917790  0.893801\n",
       "roc_auc:    0.960793  0.972122  0.951603  0.972262  0.974032  0.966162\n",
       "F1:         0.614458  0.720000  0.635762  0.679487  0.629834  0.655908\n",
       "log-loss:   0.250577  0.200025  0.241414  0.204177  0.283941  0.236027"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit2_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'class_weight': 'balanced',\n",
       " 'dual': False,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1,\n",
       " 'l1_ratio': None,\n",
       " 'max_iter': 500,\n",
       " 'multi_class': 'auto',\n",
       " 'n_jobs': None,\n",
       " 'penalty': 'none',\n",
       " 'random_state': 0,\n",
       " 'solver': 'lbfgs',\n",
       " 'tol': 0.0001,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit2.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy:</th>\n",
       "      <td>0.879603</td>\n",
       "      <td>0.878040</td>\n",
       "      <td>0.875767</td>\n",
       "      <td>0.893413</td>\n",
       "      <td>0.936143</td>\n",
       "      <td>0.892593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision:</th>\n",
       "      <td>0.086486</td>\n",
       "      <td>0.084211</td>\n",
       "      <td>0.100977</td>\n",
       "      <td>0.089431</td>\n",
       "      <td>0.100559</td>\n",
       "      <td>0.092333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall:</th>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.891892</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.886486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE:</th>\n",
       "      <td>0.105995</td>\n",
       "      <td>0.109085</td>\n",
       "      <td>0.087172</td>\n",
       "      <td>0.105100</td>\n",
       "      <td>0.099845</td>\n",
       "      <td>0.101440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE:</th>\n",
       "      <td>0.105995</td>\n",
       "      <td>0.109085</td>\n",
       "      <td>0.087172</td>\n",
       "      <td>0.105100</td>\n",
       "      <td>0.099845</td>\n",
       "      <td>0.101440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R^2:</th>\n",
       "      <td>-8.377491</td>\n",
       "      <td>-8.650888</td>\n",
       "      <td>-6.709802</td>\n",
       "      <td>-8.295506</td>\n",
       "      <td>-7.830730</td>\n",
       "      <td>-7.972883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc:</th>\n",
       "      <td>0.879603</td>\n",
       "      <td>0.878040</td>\n",
       "      <td>0.875767</td>\n",
       "      <td>0.893413</td>\n",
       "      <td>0.936143</td>\n",
       "      <td>0.892593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc:</th>\n",
       "      <td>0.961863</td>\n",
       "      <td>0.948616</td>\n",
       "      <td>0.969677</td>\n",
       "      <td>0.953949</td>\n",
       "      <td>0.983055</td>\n",
       "      <td>0.963432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1:</th>\n",
       "      <td>0.157248</td>\n",
       "      <td>0.153477</td>\n",
       "      <td>0.180233</td>\n",
       "      <td>0.162562</td>\n",
       "      <td>0.182278</td>\n",
       "      <td>0.167160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log-loss:</th>\n",
       "      <td>0.233103</td>\n",
       "      <td>0.260611</td>\n",
       "      <td>0.213819</td>\n",
       "      <td>0.247897</td>\n",
       "      <td>0.239693</td>\n",
       "      <td>0.239025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4       avg\n",
       "Accuracy:   0.879603  0.878040  0.875767  0.893413  0.936143  0.892593\n",
       "Precision:  0.086486  0.084211  0.100977  0.089431  0.100559  0.092333\n",
       "Recall:     0.864865  0.864865  0.837838  0.891892  0.972973  0.886486\n",
       "MSE:        0.105995  0.109085  0.087172  0.105100  0.099845  0.101440\n",
       "MAE:        0.105995  0.109085  0.087172  0.105100  0.099845  0.101440\n",
       "R^2:       -8.377491 -8.650888 -6.709802 -8.295506 -7.830730 -7.972883\n",
       "auc:        0.879603  0.878040  0.875767  0.893413  0.936143  0.892593\n",
       "roc_auc:    0.961863  0.948616  0.969677  0.953949  0.983055  0.963432\n",
       "F1:         0.157248  0.153477  0.180233  0.162562  0.182278  0.167160\n",
       "log-loss:   0.233103  0.260611  0.213819  0.247897  0.239693  0.239025"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit3_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.get_params of LogisticRegression(class_weight='balanced', max_iter=500, penalty='none',\n",
       "                   random_state=0)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit3.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy:</th>\n",
       "      <td>0.942310</td>\n",
       "      <td>0.897292</td>\n",
       "      <td>0.957831</td>\n",
       "      <td>0.885244</td>\n",
       "      <td>0.957831</td>\n",
       "      <td>0.928102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision:</th>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.477612</td>\n",
       "      <td>0.468354</td>\n",
       "      <td>0.405063</td>\n",
       "      <td>0.468354</td>\n",
       "      <td>0.453877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall:</th>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.940541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE:</th>\n",
       "      <td>0.084112</td>\n",
       "      <td>0.074766</td>\n",
       "      <td>0.078505</td>\n",
       "      <td>0.097196</td>\n",
       "      <td>0.078505</td>\n",
       "      <td>0.082617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE:</th>\n",
       "      <td>0.084112</td>\n",
       "      <td>0.074766</td>\n",
       "      <td>0.078505</td>\n",
       "      <td>0.097196</td>\n",
       "      <td>0.078505</td>\n",
       "      <td>0.082617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R^2:</th>\n",
       "      <td>-0.306578</td>\n",
       "      <td>-0.161402</td>\n",
       "      <td>-0.219472</td>\n",
       "      <td>-0.509823</td>\n",
       "      <td>-0.219472</td>\n",
       "      <td>-0.283350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc:</th>\n",
       "      <td>0.942310</td>\n",
       "      <td>0.897292</td>\n",
       "      <td>0.957831</td>\n",
       "      <td>0.885244</td>\n",
       "      <td>0.957831</td>\n",
       "      <td>0.928102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc:</th>\n",
       "      <td>0.983122</td>\n",
       "      <td>0.962254</td>\n",
       "      <td>0.982036</td>\n",
       "      <td>0.949256</td>\n",
       "      <td>0.986215</td>\n",
       "      <td>0.972577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1:</th>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.637931</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.637931</td>\n",
       "      <td>0.611671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log-loss:</th>\n",
       "      <td>0.192810</td>\n",
       "      <td>0.177873</td>\n",
       "      <td>0.200565</td>\n",
       "      <td>0.260109</td>\n",
       "      <td>0.206918</td>\n",
       "      <td>0.207655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4       avg\n",
       "Accuracy:   0.942310  0.897292  0.957831  0.885244  0.957831  0.928102\n",
       "Precision:  0.450000  0.477612  0.468354  0.405063  0.468354  0.453877\n",
       "Recall:     0.972973  0.864865  1.000000  0.864865  1.000000  0.940541\n",
       "MSE:        0.084112  0.074766  0.078505  0.097196  0.078505  0.082617\n",
       "MAE:        0.084112  0.074766  0.078505  0.097196  0.078505  0.082617\n",
       "R^2:       -0.306578 -0.161402 -0.219472 -0.509823 -0.219472 -0.283350\n",
       "auc:        0.942310  0.897292  0.957831  0.885244  0.957831  0.928102\n",
       "roc_auc:    0.983122  0.962254  0.982036  0.949256  0.986215  0.972577\n",
       "F1:         0.615385  0.615385  0.637931  0.551724  0.637931  0.611671\n",
       "log-loss:   0.192810  0.177873  0.200565  0.260109  0.206918  0.207655"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit4_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'class_weight': 'balanced',\n",
       " 'dual': False,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1,\n",
       " 'l1_ratio': None,\n",
       " 'max_iter': 500,\n",
       " 'multi_class': 'auto',\n",
       " 'n_jobs': None,\n",
       " 'penalty': 'none',\n",
       " 'random_state': 0,\n",
       " 'solver': 'lbfgs',\n",
       " 'tol': 0.0001,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit4.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;, max_iter=500, penalty=&#x27;none&#x27;,\n",
       "                   random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;, max_iter=500, penalty=&#x27;none&#x27;,\n",
       "                   random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(class_weight='balanced', max_iter=500, penalty='none',\n",
       "                   random_state=0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit Logistic Regression using sklearn\n",
    "# ======== Lalonde (1986) sample ======== #\n",
    "logit1.fit(nswCps_lalonde_features,nswCps_lalonde_target) # cps\n",
    "logit2.fit(nswPsid_lalonde_features,nswPsid_lalonde_target) # psid\n",
    "# ======== Dehejia & Wahba (1999) sub sample ======== #\n",
    "logit3.fit(nswCps_dehWab_features,nswCps_dehWab_target) # cps\n",
    "logit4.fit(nswPsid_dehWab_features,nswPsid_dehWab_target) # psid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change logisistic regression to STAT's model implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict get scores on NSW , CPS and PSID \n",
    "# ============ Lalonde Subsample ============ # \n",
    "nswCps_lalonde_ps_LOGIT_withRe78 = propensity_score_funct(nswCps_lalonde_subset,logit1,nswCps_lalonde,False)\n",
    "nswPsid_lalonde_ps_LOGIT_withRe78= propensity_score_funct(nswPsid_lalonde_subset,logit2,nswPsid_lalonde,False)\n",
    "# ============ Dehejia & Wahba sub sample ============ # \n",
    "# predict propensity scores\n",
    "nswCps_dehWab_ps_LOGIT_withRe78 = propensity_score_funct(nswCps_dehWab_subset,logit3,nswCps_dehWab,True)\n",
    "nswPsid_dehWab_ps_LOGIT_withRe78 = propensity_score_funct(nswPsid_dehWab_subset,logit4,nswPsid_dehWab,True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save unmatched logit datasets \n",
    "nswCps_lalonde_ps_LOGIT_withRe78.to_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/logit/unmatched/nswCps_lalonde_ps_unmatched_LOGIT_FS1.csv')\n",
    "nswPsid_lalonde_ps_LOGIT_withRe78.to_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/logit/unmatched/nswPsid_lalonde_ps_unmatched_LOGIT_FS1.csv')\n",
    "nswCps_dehWab_ps_LOGIT_withRe78.to_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/logit/unmatched/nswCps_dehWab_ps_unmatched_LOGIT_FS1.csv')\n",
    "nswPsid_dehWab_ps_LOGIT_withRe78.to_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/logit/unmatched/nswPsid_dehWab_ps_unmatched_LOGIT_FS1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model (2) ; CART "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load quasi-experimental dataset\n",
    "nswCps_lalonde =  pd.read_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/unmatched data/Quasi_NswCps_lalonde.csv')\n",
    "nswPsid_lalonde = pd.read_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/unmatched data/Quasi_NswPsid_lalonde.csv')\n",
    "nswCps_dehWab =   pd.read_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/unmatched data/Quasi_NswCps_dehWab.csv')\n",
    "nswPsid_dehWab =  pd.read_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/unmatched data/Quasi_NswPsid_dehWab.csv')\n",
    "\n",
    "nswCps_lalonde.columns = nswCps_lalonde.columns.str.strip() \n",
    "nswPsid_lalonde.columns = nswPsid_lalonde.columns.str.strip() \n",
    "nswCps_dehWab.columns = nswCps_dehWab.columns.str.strip() \n",
    "nswPsid_dehWab.columns = nswPsid_dehWab.columns.str.strip() \n",
    "\n",
    "# Store outcome variable prior to modelling\n",
    "\n",
    "nswCps_lalonde_re78 = nswCps_lalonde.re78\n",
    "nswPsid_lalonde_re78 = nswPsid_lalonde.re78\n",
    "nswCps_dehWab_re78 = nswCps_dehWab.re78\n",
    "nswPsid_dehWab_re78 = nswPsid_dehWab.re78"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CART - Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Select covariates for use in CART model - from GNN features \"\"\"\n",
    "# ======== Lalonde (1986) data ======== #\n",
    "nswCps_lalonde_subset = nswCps_lalonde[markov_blanket_specification_1]\n",
    "nswPsid_lalonde_subset = nswPsid_lalonde[markov_blanket_specification_2]\n",
    "\n",
    "nswCps_lalonde_target = nswCps_lalonde_subset['treat']\n",
    "nswCps_lalonde_features = nswCps_lalonde_subset.drop('treat', axis=1)\n",
    "nswPsid_lalonde_target = nswPsid_lalonde_subset['treat']\n",
    "nswPsid_lalonde_features = nswPsid_lalonde_subset.drop('treat', axis=1)\n",
    "\n",
    "# ======== Dehejia & Wahba (1986) sub sample ======== #\n",
    "\n",
    "# Apply selection\n",
    "\n",
    "nswCps_dehWab_subset = nswCps_dehWab[markov_blanket_specification_3]\n",
    "nswPsid_dehWab_subset = nswPsid_dehWab[markov_blanket_specification_4]\n",
    "\n",
    "nswCps_dehWab_target = nswCps_dehWab_subset['treat']\n",
    "nswCps_dehWab_features = nswCps_dehWab_subset.drop('treat', axis=1)\n",
    "nswPsid_dehWab_target = nswPsid_dehWab_subset['treat']\n",
    "nswPsid_dehWab_features = nswPsid_dehWab_subset.drop('treat', axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search \n",
    "tree_param = [{'criterion': ['entropy', 'gini'], \n",
    "               'max_depth': [1,2,3,4,5,6,7,8,9,10,None],\n",
    "               'max_leaf_nodes':list(range(0, 100)),\n",
    "              'min_samples_leaf': [2, 3, 4]}]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cart1_metrics = pd.DataFrame()\n",
    "cart2_metrics = pd.DataFrame()\n",
    "cart3_metrics = pd.DataFrame()\n",
    "cart4_metrics = pd.DataFrame()\n",
    "\n",
    "grid_search_cart1 = tree.DecisionTreeClassifier(random_state=0,criterion = 'entropy',max_depth = 5,max_leaf_nodes = 12,min_samples_leaf = 2)\n",
    "grid_search_cart2 = tree.DecisionTreeClassifier(random_state=0,criterion = 'entropy',max_depth = 7,max_leaf_nodes = 10,min_samples_leaf = 2)\n",
    "grid_search_cart3 = tree.DecisionTreeClassifier(random_state=0,criterion = 'entropy',max_depth = 5,max_leaf_nodes = 10,min_samples_leaf = 2)\n",
    "grid_search_cart4 = tree.DecisionTreeClassifier(random_state=0,criterion = 'entropy',max_depth = 5,max_leaf_nodes = 10,min_samples_leaf = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_evaluation_SMOTEENN(grid_search_cart1,nswCps_lalonde_features,nswCps_lalonde_target,cart1_metrics) # cps\n",
    "kfold_evaluation_SMOTEENN(grid_search_cart2,nswPsid_lalonde_features,nswPsid_lalonde_target,cart2_metrics) # psid\n",
    "kfold_evaluation_SMOTEENN(grid_search_cart3,nswCps_dehWab_features,nswCps_dehWab_target,cart3_metrics) # cps\n",
    "kfold_evaluation_SMOTEENN(grid_search_cart4,nswPsid_dehWab_features,nswPsid_dehWab_target,cart4_metrics) # psid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> model evaluation </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cart1_metrics['avg'] = np.mean(cart1_metrics,axis=1)\n",
    "cart2_metrics['avg'] = np.mean(cart2_metrics,axis=1)\n",
    "cart3_metrics['avg'] = np.mean(cart3_metrics,axis=1)\n",
    "cart4_metrics['avg'] = np.mean(cart4_metrics,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy:</th>\n",
       "      <td>0.828377</td>\n",
       "      <td>0.844575</td>\n",
       "      <td>0.811093</td>\n",
       "      <td>0.855063</td>\n",
       "      <td>0.861876</td>\n",
       "      <td>0.840197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision:</th>\n",
       "      <td>0.325397</td>\n",
       "      <td>0.328244</td>\n",
       "      <td>0.351852</td>\n",
       "      <td>0.278481</td>\n",
       "      <td>0.167857</td>\n",
       "      <td>0.290366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall:</th>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.644068</td>\n",
       "      <td>0.745763</td>\n",
       "      <td>0.796610</td>\n",
       "      <td>0.717288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE:</th>\n",
       "      <td>0.031921</td>\n",
       "      <td>0.032228</td>\n",
       "      <td>0.027931</td>\n",
       "      <td>0.039595</td>\n",
       "      <td>0.075223</td>\n",
       "      <td>0.041380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE:</th>\n",
       "      <td>0.031921</td>\n",
       "      <td>0.032228</td>\n",
       "      <td>0.027931</td>\n",
       "      <td>0.039595</td>\n",
       "      <td>0.075223</td>\n",
       "      <td>0.041380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R^2:</th>\n",
       "      <td>-0.765854</td>\n",
       "      <td>-0.782833</td>\n",
       "      <td>-0.570819</td>\n",
       "      <td>-1.226766</td>\n",
       "      <td>-3.229153</td>\n",
       "      <td>-1.315085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc:</th>\n",
       "      <td>0.828377</td>\n",
       "      <td>0.844575</td>\n",
       "      <td>0.811093</td>\n",
       "      <td>0.855063</td>\n",
       "      <td>0.861876</td>\n",
       "      <td>0.840197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc:</th>\n",
       "      <td>0.912344</td>\n",
       "      <td>0.904073</td>\n",
       "      <td>0.934929</td>\n",
       "      <td>0.952414</td>\n",
       "      <td>0.950856</td>\n",
       "      <td>0.930923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1:</th>\n",
       "      <td>0.440860</td>\n",
       "      <td>0.450262</td>\n",
       "      <td>0.455090</td>\n",
       "      <td>0.405530</td>\n",
       "      <td>0.277286</td>\n",
       "      <td>0.405806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log-loss:</th>\n",
       "      <td>0.203878</td>\n",
       "      <td>0.153560</td>\n",
       "      <td>0.163919</td>\n",
       "      <td>0.171362</td>\n",
       "      <td>0.270913</td>\n",
       "      <td>0.192726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4       avg\n",
       "Accuracy:   0.828377  0.844575  0.811093  0.855063  0.861876  0.840197\n",
       "Precision:  0.325397  0.328244  0.351852  0.278481  0.167857  0.290366\n",
       "Recall:     0.683333  0.716667  0.644068  0.745763  0.796610  0.717288\n",
       "MSE:        0.031921  0.032228  0.027931  0.039595  0.075223  0.041380\n",
       "MAE:        0.031921  0.032228  0.027931  0.039595  0.075223  0.041380\n",
       "R^2:       -0.765854 -0.782833 -0.570819 -1.226766 -3.229153 -1.315085\n",
       "auc:        0.828377  0.844575  0.811093  0.855063  0.861876  0.840197\n",
       "roc_auc:    0.912344  0.904073  0.934929  0.952414  0.950856  0.930923\n",
       "F1:         0.440860  0.450262  0.455090  0.405530  0.277286  0.405806\n",
       "log-loss:   0.203878  0.153560  0.163919  0.171362  0.270913  0.192726"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cart1_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy:</th>\n",
       "      <td>0.826205</td>\n",
       "      <td>0.878916</td>\n",
       "      <td>0.848104</td>\n",
       "      <td>0.916020</td>\n",
       "      <td>0.904414</td>\n",
       "      <td>0.874732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision:</th>\n",
       "      <td>0.573333</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.484536</td>\n",
       "      <td>0.616279</td>\n",
       "      <td>0.504673</td>\n",
       "      <td>0.574895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall:</th>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.796610</td>\n",
       "      <td>0.898305</td>\n",
       "      <td>0.915254</td>\n",
       "      <td>0.825367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE:</th>\n",
       "      <td>0.087814</td>\n",
       "      <td>0.059140</td>\n",
       "      <td>0.111311</td>\n",
       "      <td>0.070018</td>\n",
       "      <td>0.104129</td>\n",
       "      <td>0.086482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE:</th>\n",
       "      <td>0.087814</td>\n",
       "      <td>0.059140</td>\n",
       "      <td>0.111311</td>\n",
       "      <td>0.070018</td>\n",
       "      <td>0.104129</td>\n",
       "      <td>0.086482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R^2:</th>\n",
       "      <td>0.084940</td>\n",
       "      <td>0.383735</td>\n",
       "      <td>-0.175345</td>\n",
       "      <td>0.260670</td>\n",
       "      <td>-0.099517</td>\n",
       "      <td>0.090896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc:</th>\n",
       "      <td>0.826205</td>\n",
       "      <td>0.878916</td>\n",
       "      <td>0.848104</td>\n",
       "      <td>0.916020</td>\n",
       "      <td>0.904414</td>\n",
       "      <td>0.874732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc:</th>\n",
       "      <td>0.886044</td>\n",
       "      <td>0.932112</td>\n",
       "      <td>0.918930</td>\n",
       "      <td>0.952709</td>\n",
       "      <td>0.960724</td>\n",
       "      <td>0.930104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1:</th>\n",
       "      <td>0.637037</td>\n",
       "      <td>0.744186</td>\n",
       "      <td>0.602564</td>\n",
       "      <td>0.731034</td>\n",
       "      <td>0.650602</td>\n",
       "      <td>0.673085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log-loss:</th>\n",
       "      <td>1.055793</td>\n",
       "      <td>0.325228</td>\n",
       "      <td>0.364697</td>\n",
       "      <td>0.233530</td>\n",
       "      <td>0.288515</td>\n",
       "      <td>0.453553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4       avg\n",
       "Accuracy:   0.826205  0.878916  0.848104  0.916020  0.904414  0.874732\n",
       "Precision:  0.573333  0.695652  0.484536  0.616279  0.504673  0.574895\n",
       "Recall:     0.716667  0.800000  0.796610  0.898305  0.915254  0.825367\n",
       "MSE:        0.087814  0.059140  0.111311  0.070018  0.104129  0.086482\n",
       "MAE:        0.087814  0.059140  0.111311  0.070018  0.104129  0.086482\n",
       "R^2:        0.084940  0.383735 -0.175345  0.260670 -0.099517  0.090896\n",
       "auc:        0.826205  0.878916  0.848104  0.916020  0.904414  0.874732\n",
       "roc_auc:    0.886044  0.932112  0.918930  0.952709  0.960724  0.930104\n",
       "F1:         0.637037  0.744186  0.602564  0.731034  0.650602  0.673085\n",
       "log-loss:   1.055793  0.325228  0.364697  0.233530  0.288515  0.453553"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cart2_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy:</th>\n",
       "      <td>0.907040</td>\n",
       "      <td>0.883055</td>\n",
       "      <td>0.891803</td>\n",
       "      <td>0.903753</td>\n",
       "      <td>0.922270</td>\n",
       "      <td>0.901584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision:</th>\n",
       "      <td>0.289720</td>\n",
       "      <td>0.173410</td>\n",
       "      <td>0.256410</td>\n",
       "      <td>0.242188</td>\n",
       "      <td>0.329897</td>\n",
       "      <td>0.258325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall:</th>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.832432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE:</th>\n",
       "      <td>0.025340</td>\n",
       "      <td>0.046354</td>\n",
       "      <td>0.029057</td>\n",
       "      <td>0.031839</td>\n",
       "      <td>0.021638</td>\n",
       "      <td>0.030846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE:</th>\n",
       "      <td>0.025340</td>\n",
       "      <td>0.046354</td>\n",
       "      <td>0.029057</td>\n",
       "      <td>0.031839</td>\n",
       "      <td>0.021638</td>\n",
       "      <td>0.030846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R^2:</th>\n",
       "      <td>-1.241849</td>\n",
       "      <td>-3.100944</td>\n",
       "      <td>-1.569934</td>\n",
       "      <td>-1.815991</td>\n",
       "      <td>-0.913781</td>\n",
       "      <td>-1.728500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc:</th>\n",
       "      <td>0.907040</td>\n",
       "      <td>0.883055</td>\n",
       "      <td>0.891803</td>\n",
       "      <td>0.903753</td>\n",
       "      <td>0.922270</td>\n",
       "      <td>0.901584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc:</th>\n",
       "      <td>0.946613</td>\n",
       "      <td>0.921542</td>\n",
       "      <td>0.931469</td>\n",
       "      <td>0.957871</td>\n",
       "      <td>0.974059</td>\n",
       "      <td>0.946311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1:</th>\n",
       "      <td>0.430556</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.389610</td>\n",
       "      <td>0.375758</td>\n",
       "      <td>0.477612</td>\n",
       "      <td>0.391850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log-loss:</th>\n",
       "      <td>0.107374</td>\n",
       "      <td>0.154577</td>\n",
       "      <td>0.108449</td>\n",
       "      <td>0.136092</td>\n",
       "      <td>0.108931</td>\n",
       "      <td>0.123084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4       avg\n",
       "Accuracy:   0.907040  0.883055  0.891803  0.903753  0.922270  0.901584\n",
       "Precision:  0.289720  0.173410  0.256410  0.242188  0.329897  0.258325\n",
       "Recall:     0.837838  0.810811  0.810811  0.837838  0.864865  0.832432\n",
       "MSE:        0.025340  0.046354  0.029057  0.031839  0.021638  0.030846\n",
       "MAE:        0.025340  0.046354  0.029057  0.031839  0.021638  0.030846\n",
       "R^2:       -1.241849 -3.100944 -1.569934 -1.815991 -0.913781 -1.728500\n",
       "auc:        0.907040  0.883055  0.891803  0.903753  0.922270  0.901584\n",
       "roc_auc:    0.946613  0.921542  0.931469  0.957871  0.974059  0.946311\n",
       "F1:         0.430556  0.285714  0.389610  0.375758  0.477612  0.391850\n",
       "log-loss:   0.107374  0.154577  0.108449  0.136092  0.108931  0.123084"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cart3_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy:</th>\n",
       "      <td>0.933355</td>\n",
       "      <td>0.874824</td>\n",
       "      <td>0.941848</td>\n",
       "      <td>0.860767</td>\n",
       "      <td>0.918376</td>\n",
       "      <td>0.905834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision:</th>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.530303</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.581278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall:</th>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.859459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE:</th>\n",
       "      <td>0.054206</td>\n",
       "      <td>0.046729</td>\n",
       "      <td>0.061682</td>\n",
       "      <td>0.072897</td>\n",
       "      <td>0.035514</td>\n",
       "      <td>0.054206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE:</th>\n",
       "      <td>0.054206</td>\n",
       "      <td>0.046729</td>\n",
       "      <td>0.061682</td>\n",
       "      <td>0.072897</td>\n",
       "      <td>0.035514</td>\n",
       "      <td>0.054206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R^2:</th>\n",
       "      <td>0.157983</td>\n",
       "      <td>0.274124</td>\n",
       "      <td>0.041843</td>\n",
       "      <td>-0.132367</td>\n",
       "      <td>0.448334</td>\n",
       "      <td>0.157983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc:</th>\n",
       "      <td>0.933355</td>\n",
       "      <td>0.874824</td>\n",
       "      <td>0.941848</td>\n",
       "      <td>0.860767</td>\n",
       "      <td>0.918376</td>\n",
       "      <td>0.905834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc:</th>\n",
       "      <td>0.976419</td>\n",
       "      <td>0.943721</td>\n",
       "      <td>0.979893</td>\n",
       "      <td>0.886356</td>\n",
       "      <td>0.925594</td>\n",
       "      <td>0.942397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1:</th>\n",
       "      <td>0.701031</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.679612</td>\n",
       "      <td>0.597938</td>\n",
       "      <td>0.771084</td>\n",
       "      <td>0.689692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log-loss:</th>\n",
       "      <td>0.200567</td>\n",
       "      <td>0.563904</td>\n",
       "      <td>0.612808</td>\n",
       "      <td>0.612825</td>\n",
       "      <td>0.417297</td>\n",
       "      <td>0.481480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4       avg\n",
       "Accuracy:   0.933355  0.874824  0.941848  0.860767  0.918376  0.905834\n",
       "Precision:  0.566667  0.630435  0.530303  0.483333  0.695652  0.581278\n",
       "Recall:     0.918919  0.783784  0.945946  0.783784  0.864865  0.859459\n",
       "MSE:        0.054206  0.046729  0.061682  0.072897  0.035514  0.054206\n",
       "MAE:        0.054206  0.046729  0.061682  0.072897  0.035514  0.054206\n",
       "R^2:        0.157983  0.274124  0.041843 -0.132367  0.448334  0.157983\n",
       "auc:        0.933355  0.874824  0.941848  0.860767  0.918376  0.905834\n",
       "roc_auc:    0.976419  0.943721  0.979893  0.886356  0.925594  0.942397\n",
       "F1:         0.701031  0.698795  0.679612  0.597938  0.771084  0.689692\n",
       "log-loss:   0.200567  0.563904  0.612808  0.612825  0.417297  0.481480"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cart4_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/pandas/core/arraylike.py:397: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/pandas/core/arraylike.py:397: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/pandas/core/arraylike.py:397: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "nswCps_lalonde_ps_CART_withRe78 = propensity_score_funct(nswCps_lalonde_subset,grid_search_cart1,nswCps_lalonde,False)\n",
    "nswPsid_lalonde_ps_CART_withRe78= propensity_score_funct(nswPsid_lalonde_subset,grid_search_cart2,nswPsid_lalonde,False)\n",
    "# ============ Dehejia & Wahba sub sample ============ # \n",
    "# predict propensity scores\n",
    "nswCps_dehWab_ps_CART_withRe78 = propensity_score_funct(nswCps_dehWab_subset,grid_search_cart3,nswCps_dehWab,True)\n",
    "nswPsid_dehWab_ps_CART_withRe78 = propensity_score_funct(nswPsid_dehWab_subset,grid_search_cart4,nswPsid_dehWab,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>treat</th>\n",
       "      <th>age</th>\n",
       "      <th>education*</th>\n",
       "      <th>black</th>\n",
       "      <th>hispanic</th>\n",
       "      <th>married</th>\n",
       "      <th>nodegree</th>\n",
       "      <th>re74</th>\n",
       "      <th>re75</th>\n",
       "      <th>re78</th>\n",
       "      <th>propensity_score</th>\n",
       "      <th>propensity_logit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9930</td>\n",
       "      <td>0.913636</td>\n",
       "      <td>2.358866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3595</td>\n",
       "      <td>0.090841</td>\n",
       "      <td>-2.303411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24909</td>\n",
       "      <td>0.997095</td>\n",
       "      <td>5.838260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>27</td>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7506</td>\n",
       "      <td>0.997095</td>\n",
       "      <td>5.838260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>33</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>289</td>\n",
       "      <td>0.997095</td>\n",
       "      <td>5.838260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16172</th>\n",
       "      <td>0.0</td>\n",
       "      <td>22</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3975</td>\n",
       "      <td>6801</td>\n",
       "      <td>2757</td>\n",
       "      <td>0.973103</td>\n",
       "      <td>3.588482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16173</th>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1445</td>\n",
       "      <td>11832</td>\n",
       "      <td>6895</td>\n",
       "      <td>0.204762</td>\n",
       "      <td>-1.356794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16174</th>\n",
       "      <td>0.0</td>\n",
       "      <td>37</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1733</td>\n",
       "      <td>1559</td>\n",
       "      <td>4221</td>\n",
       "      <td>0.090841</td>\n",
       "      <td>-2.303411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16175</th>\n",
       "      <td>0.0</td>\n",
       "      <td>47</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16914</td>\n",
       "      <td>11384</td>\n",
       "      <td>13671</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16176</th>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13628</td>\n",
       "      <td>13144</td>\n",
       "      <td>7979</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16177 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       treat  age  education*  black  hispanic  married  nodegree   re74  \\\n",
       "0        1.0   37          11    1.0       0.0      1.0       1.0      0   \n",
       "1        1.0   22           9    0.0       1.0      0.0       1.0      0   \n",
       "2        1.0   30          12    1.0       0.0      0.0       0.0      0   \n",
       "3        1.0   27          11    1.0       0.0      0.0       1.0      0   \n",
       "4        1.0   33           8    1.0       0.0      0.0       1.0      0   \n",
       "...      ...  ...         ...    ...       ...      ...       ...    ...   \n",
       "16172    0.0   22          12    1.0       0.0      0.0       0.0   3975   \n",
       "16173    0.0   20          12    1.0       0.0      1.0       0.0   1445   \n",
       "16174    0.0   37          12    0.0       0.0      0.0       0.0   1733   \n",
       "16175    0.0   47           9    0.0       0.0      1.0       1.0  16914   \n",
       "16176    0.0   40          10    0.0       0.0      0.0       1.0  13628   \n",
       "\n",
       "        re75   re78  propensity_score  propensity_logit  \n",
       "0          0   9930          0.913636          2.358866  \n",
       "1          0   3595          0.090841         -2.303411  \n",
       "2          0  24909          0.997095          5.838260  \n",
       "3          0   7506          0.997095          5.838260  \n",
       "4          0    289          0.997095          5.838260  \n",
       "...      ...    ...               ...               ...  \n",
       "16172   6801   2757          0.973103          3.588482  \n",
       "16173  11832   6895          0.204762         -1.356794  \n",
       "16174   1559   4221          0.090841         -2.303411  \n",
       "16175  11384  13671          0.000000              -inf  \n",
       "16176  13144   7979          0.000000              -inf  \n",
       "\n",
       "[16177 rows x 12 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nswCps_dehWab_ps_CART_withRe78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save unmatched CART datasets \n",
    "nswCps_lalonde_ps_CART_withRe78.to_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/cart/unmatched/nswCps_lalonde_ps_unmatched_CART_FS1.csv')\n",
    "nswPsid_lalonde_ps_CART_withRe78.to_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/cart/unmatched/nswPsid_lalonde_ps_unmatched_CART_FS1.csv')\n",
    "nswCps_dehWab_ps_CART_withRe78.to_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/cart/unmatched/nswCps_dehWab_ps_unmatched_CART_FS1.csv')\n",
    "nswPsid_dehWab_ps_CART_withRe78.to_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/cart/unmatched/nswPsid_dehWab_ps_unmatched_CART_FS1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model (3) ; Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load quasi-experimental dataset\n",
    "nswCps_lalonde =  pd.read_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/unmatched data/Quasi_NswCps_lalonde.csv')\n",
    "nswPsid_lalonde = pd.read_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/unmatched data/Quasi_NswPsid_lalonde.csv')\n",
    "nswCps_dehWab =   pd.read_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/unmatched data/Quasi_NswCps_dehWab.csv')\n",
    "nswPsid_dehWab =  pd.read_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/unmatched data/Quasi_NswPsid_dehWab.csv')\n",
    "\n",
    "nswCps_lalonde.columns = nswCps_lalonde.columns.str.strip() \n",
    "nswPsid_lalonde.columns = nswPsid_lalonde.columns.str.strip() \n",
    "nswCps_dehWab.columns = nswCps_dehWab.columns.str.strip() \n",
    "nswPsid_dehWab.columns = nswPsid_dehWab.columns.str.strip() \n",
    "\n",
    "# Store outcome variable prior to modelling\n",
    "nswCps_lalonde_re78 = nswCps_lalonde.re78\n",
    "nswPsid_lalonde_re78 = nswPsid_lalonde.re78\n",
    "nswCps_dehWab_re78 = nswCps_dehWab.re78\n",
    "nswPsid_dehWab_re78 = nswPsid_dehWab.re78\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Select covariates for use in CART model \"\"\"\n",
    "#sc = StandardScaler()\n",
    "# ======== Lalonde (1986) data ======== #\n",
    "\n",
    "nswCps_lalonde_subset = nswCps_lalonde[markov_blanket_specification_1]\n",
    "nswPsid_lalonde_subset = nswPsid_lalonde[markov_blanket_specification_2]\n",
    "\n",
    "nswCps_lalonde_target = nswCps_lalonde_subset['treat']\n",
    "nswCps_lalonde_features = nswCps_lalonde_subset.drop('treat', axis=1)\n",
    "nswPsid_lalonde_target = nswPsid_lalonde_subset['treat']\n",
    "nswPsid_lalonde_features = nswPsid_lalonde_subset.drop('treat', axis=1)\n",
    "\n",
    "# ======== Dehejia & Wahba (1986) sub sample ======== #\n",
    "\n",
    "nswCps_dehWab_subset = nswCps_dehWab[markov_blanket_specification_3]\n",
    "nswPsid_dehWab_subset = nswPsid_dehWab[markov_blanket_specification_4]\n",
    "\n",
    "nswCps_dehWab_target = nswCps_dehWab_subset['treat']\n",
    "nswCps_dehWab_features = nswCps_dehWab_subset.drop('treat', axis=1)\n",
    "nswPsid_dehWab_target = nswPsid_dehWab_subset['treat']\n",
    "nswPsid_dehWab_features = nswPsid_dehWab_subset.drop('treat', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest1_metrics = pd.DataFrame()\n",
    "forest2_metrics = pd.DataFrame()\n",
    "forest3_metrics = pd.DataFrame()\n",
    "forest4_metrics = pd.DataFrame()\n",
    "\n",
    "grid_search_forest1 = RandomForestClassifier(random_state=0,criterion = 'entropy',max_depth = 8,max_features = 'sqrt',min_samples_split = 2,n_estimators= 100)\n",
    "grid_search_forest2 = RandomForestClassifier(random_state=0,criterion = 'gini',max_depth = 8,max_features = 'sqrt',min_samples_split = 2,n_estimators= 100)\n",
    "grid_search_forest3 = RandomForestClassifier(random_state=0,criterion = 'entropy',max_depth = 8,max_features = 'log2',min_samples_split = 3,n_estimators= 500)\n",
    "grid_search_forest4 = RandomForestClassifier(random_state=0,criterion = 'gini',max_depth = 8,max_features = 'log2',min_samples_leaf = 2,n_estimators= 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_evaluation_SMOTEENN(grid_search_forest1,nswCps_lalonde_features,nswCps_lalonde_target,forest1_metrics) # cps\n",
    "kfold_evaluation_SMOTEENN(grid_search_forest2,nswPsid_lalonde_features,nswPsid_lalonde_target,forest2_metrics) # psid\n",
    "kfold_evaluation_SMOTEENN(grid_search_forest3,nswCps_dehWab_features,nswCps_dehWab_target,forest3_metrics) # cps\n",
    "kfold_evaluation_SMOTEENN(grid_search_forest4,nswPsid_dehWab_features,nswPsid_dehWab_target,forest4_metrics) # psid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest1_metrics['avg'] = np.mean(forest1_metrics,axis=1)\n",
    "forest2_metrics['avg'] = np.mean(forest2_metrics,axis=1)\n",
    "forest3_metrics['avg'] = np.mean(forest3_metrics,axis=1)\n",
    "forest4_metrics['avg'] = np.mean(forest4_metrics,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy:</th>\n",
       "      <td>0.835460</td>\n",
       "      <td>0.831348</td>\n",
       "      <td>0.812690</td>\n",
       "      <td>0.857877</td>\n",
       "      <td>0.920353</td>\n",
       "      <td>0.851545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision:</th>\n",
       "      <td>0.311111</td>\n",
       "      <td>0.383178</td>\n",
       "      <td>0.254902</td>\n",
       "      <td>0.314286</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.309838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall:</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.661017</td>\n",
       "      <td>0.745763</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>0.734294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE:</th>\n",
       "      <td>0.034070</td>\n",
       "      <td>0.026090</td>\n",
       "      <td>0.041130</td>\n",
       "      <td>0.034070</td>\n",
       "      <td>0.042063</td>\n",
       "      <td>0.035484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE:</th>\n",
       "      <td>0.034070</td>\n",
       "      <td>0.026090</td>\n",
       "      <td>0.041130</td>\n",
       "      <td>0.034070</td>\n",
       "      <td>0.042063</td>\n",
       "      <td>0.035484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R^2:</th>\n",
       "      <td>-0.884709</td>\n",
       "      <td>-0.443246</td>\n",
       "      <td>-1.313075</td>\n",
       "      <td>-0.916054</td>\n",
       "      <td>-1.364873</td>\n",
       "      <td>-0.984391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc:</th>\n",
       "      <td>0.835460</td>\n",
       "      <td>0.831348</td>\n",
       "      <td>0.812690</td>\n",
       "      <td>0.857877</td>\n",
       "      <td>0.920353</td>\n",
       "      <td>0.851545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc:</th>\n",
       "      <td>0.937093</td>\n",
       "      <td>0.956087</td>\n",
       "      <td>0.934887</td>\n",
       "      <td>0.969506</td>\n",
       "      <td>0.979028</td>\n",
       "      <td>0.955320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1:</th>\n",
       "      <td>0.430769</td>\n",
       "      <td>0.491018</td>\n",
       "      <td>0.367925</td>\n",
       "      <td>0.442211</td>\n",
       "      <td>0.431535</td>\n",
       "      <td>0.432692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log-loss:</th>\n",
       "      <td>0.124007</td>\n",
       "      <td>0.099098</td>\n",
       "      <td>0.123954</td>\n",
       "      <td>0.113170</td>\n",
       "      <td>0.122900</td>\n",
       "      <td>0.116626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4       avg\n",
       "Accuracy:   0.835460  0.831348  0.812690  0.857877  0.920353  0.851545\n",
       "Precision:  0.311111  0.383178  0.254902  0.314286  0.285714  0.309838\n",
       "Recall:     0.700000  0.683333  0.661017  0.745763  0.881356  0.734294\n",
       "MSE:        0.034070  0.026090  0.041130  0.034070  0.042063  0.035484\n",
       "MAE:        0.034070  0.026090  0.041130  0.034070  0.042063  0.035484\n",
       "R^2:       -0.884709 -0.443246 -1.313075 -0.916054 -1.364873 -0.984391\n",
       "auc:        0.835460  0.831348  0.812690  0.857877  0.920353  0.851545\n",
       "roc_auc:    0.937093  0.956087  0.934887  0.969506  0.979028  0.955320\n",
       "F1:         0.430769  0.491018  0.367925  0.442211  0.431535  0.432692\n",
       "log-loss:   0.124007  0.099098  0.123954  0.113170  0.122900  0.116626"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest1_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy:</th>\n",
       "      <td>0.833233</td>\n",
       "      <td>0.881928</td>\n",
       "      <td>0.867181</td>\n",
       "      <td>0.890154</td>\n",
       "      <td>0.924495</td>\n",
       "      <td>0.879398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision:</th>\n",
       "      <td>0.632353</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.602564</td>\n",
       "      <td>0.662162</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.649008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall:</th>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.796610</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.915254</td>\n",
       "      <td>0.811808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE:</th>\n",
       "      <td>0.075269</td>\n",
       "      <td>0.053763</td>\n",
       "      <td>0.077199</td>\n",
       "      <td>0.062837</td>\n",
       "      <td>0.068223</td>\n",
       "      <td>0.067458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE:</th>\n",
       "      <td>0.075269</td>\n",
       "      <td>0.053763</td>\n",
       "      <td>0.077199</td>\n",
       "      <td>0.062837</td>\n",
       "      <td>0.068223</td>\n",
       "      <td>0.067458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R^2:</th>\n",
       "      <td>0.215663</td>\n",
       "      <td>0.439759</td>\n",
       "      <td>0.184841</td>\n",
       "      <td>0.336499</td>\n",
       "      <td>0.279627</td>\n",
       "      <td>0.291278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc:</th>\n",
       "      <td>0.833233</td>\n",
       "      <td>0.881928</td>\n",
       "      <td>0.867181</td>\n",
       "      <td>0.890154</td>\n",
       "      <td>0.924495</td>\n",
       "      <td>0.879398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc:</th>\n",
       "      <td>0.924916</td>\n",
       "      <td>0.960609</td>\n",
       "      <td>0.930502</td>\n",
       "      <td>0.967820</td>\n",
       "      <td>0.972075</td>\n",
       "      <td>0.951184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1:</th>\n",
       "      <td>0.671875</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.686131</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.739726</td>\n",
       "      <td>0.719296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log-loss:</th>\n",
       "      <td>0.394292</td>\n",
       "      <td>0.222957</td>\n",
       "      <td>0.390829</td>\n",
       "      <td>0.185341</td>\n",
       "      <td>0.217657</td>\n",
       "      <td>0.282215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4       avg\n",
       "Accuracy:   0.833233  0.881928  0.867181  0.890154  0.924495  0.879398\n",
       "Precision:  0.632353  0.727273  0.602564  0.662162  0.620690  0.649008\n",
       "Recall:     0.716667  0.800000  0.796610  0.830508  0.915254  0.811808\n",
       "MSE:        0.075269  0.053763  0.077199  0.062837  0.068223  0.067458\n",
       "MAE:        0.075269  0.053763  0.077199  0.062837  0.068223  0.067458\n",
       "R^2:        0.215663  0.439759  0.184841  0.336499  0.279627  0.291278\n",
       "auc:        0.833233  0.881928  0.867181  0.890154  0.924495  0.879398\n",
       "roc_auc:    0.924916  0.960609  0.930502  0.967820  0.972075  0.951184\n",
       "F1:         0.671875  0.761905  0.686131  0.736842  0.739726  0.719296\n",
       "log-loss:   0.394292  0.222957  0.390829  0.185341  0.217657  0.282215"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest2_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy:</th>\n",
       "      <td>0.885484</td>\n",
       "      <td>0.895402</td>\n",
       "      <td>0.858924</td>\n",
       "      <td>0.899464</td>\n",
       "      <td>0.925240</td>\n",
       "      <td>0.892903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision:</th>\n",
       "      <td>0.414286</td>\n",
       "      <td>0.319149</td>\n",
       "      <td>0.415385</td>\n",
       "      <td>0.441176</td>\n",
       "      <td>0.410256</td>\n",
       "      <td>0.400050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall:</th>\n",
       "      <td>0.783784</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE:</th>\n",
       "      <td>0.015142</td>\n",
       "      <td>0.021941</td>\n",
       "      <td>0.014838</td>\n",
       "      <td>0.013910</td>\n",
       "      <td>0.015765</td>\n",
       "      <td>0.016319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE:</th>\n",
       "      <td>0.015142</td>\n",
       "      <td>0.021941</td>\n",
       "      <td>0.014838</td>\n",
       "      <td>0.013910</td>\n",
       "      <td>0.015765</td>\n",
       "      <td>0.016319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R^2:</th>\n",
       "      <td>-0.339642</td>\n",
       "      <td>-0.941113</td>\n",
       "      <td>-0.312307</td>\n",
       "      <td>-0.230288</td>\n",
       "      <td>-0.394326</td>\n",
       "      <td>-0.443535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc:</th>\n",
       "      <td>0.885484</td>\n",
       "      <td>0.895402</td>\n",
       "      <td>0.858924</td>\n",
       "      <td>0.899464</td>\n",
       "      <td>0.925240</td>\n",
       "      <td>0.892903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc:</th>\n",
       "      <td>0.981958</td>\n",
       "      <td>0.959628</td>\n",
       "      <td>0.986246</td>\n",
       "      <td>0.962084</td>\n",
       "      <td>0.988143</td>\n",
       "      <td>0.975612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1:</th>\n",
       "      <td>0.542056</td>\n",
       "      <td>0.458015</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.556522</td>\n",
       "      <td>0.531487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log-loss:</th>\n",
       "      <td>0.056206</td>\n",
       "      <td>0.082109</td>\n",
       "      <td>0.047755</td>\n",
       "      <td>0.066157</td>\n",
       "      <td>0.061855</td>\n",
       "      <td>0.062817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4       avg\n",
       "Accuracy:   0.885484  0.895402  0.858924  0.899464  0.925240  0.892903\n",
       "Precision:  0.414286  0.319149  0.415385  0.441176  0.410256  0.400050\n",
       "Recall:     0.783784  0.810811  0.729730  0.810811  0.864865  0.800000\n",
       "MSE:        0.015142  0.021941  0.014838  0.013910  0.015765  0.016319\n",
       "MAE:        0.015142  0.021941  0.014838  0.013910  0.015765  0.016319\n",
       "R^2:       -0.339642 -0.941113 -0.312307 -0.230288 -0.394326 -0.443535\n",
       "auc:        0.885484  0.895402  0.858924  0.899464  0.925240  0.892903\n",
       "roc_auc:    0.981958  0.959628  0.986246  0.962084  0.988143  0.975612\n",
       "F1:         0.542056  0.458015  0.529412  0.571429  0.556522  0.531487\n",
       "log-loss:   0.056206  0.082109  0.047755  0.066157  0.061855  0.062817"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest3_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy:</th>\n",
       "      <td>0.927874</td>\n",
       "      <td>0.877836</td>\n",
       "      <td>0.946869</td>\n",
       "      <td>0.879301</td>\n",
       "      <td>0.929882</td>\n",
       "      <td>0.912352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision:</th>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.674419</td>\n",
       "      <td>0.573770</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.673469</td>\n",
       "      <td>0.620886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall:</th>\n",
       "      <td>0.891892</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.891892</td>\n",
       "      <td>0.864865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE:</th>\n",
       "      <td>0.041121</td>\n",
       "      <td>0.041121</td>\n",
       "      <td>0.052336</td>\n",
       "      <td>0.061682</td>\n",
       "      <td>0.037383</td>\n",
       "      <td>0.046729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE:</th>\n",
       "      <td>0.041121</td>\n",
       "      <td>0.041121</td>\n",
       "      <td>0.052336</td>\n",
       "      <td>0.061682</td>\n",
       "      <td>0.037383</td>\n",
       "      <td>0.046729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R^2:</th>\n",
       "      <td>0.361229</td>\n",
       "      <td>0.361229</td>\n",
       "      <td>0.187018</td>\n",
       "      <td>0.041843</td>\n",
       "      <td>0.419299</td>\n",
       "      <td>0.274124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc:</th>\n",
       "      <td>0.927874</td>\n",
       "      <td>0.877836</td>\n",
       "      <td>0.946869</td>\n",
       "      <td>0.879301</td>\n",
       "      <td>0.929882</td>\n",
       "      <td>0.912352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc:</th>\n",
       "      <td>0.984099</td>\n",
       "      <td>0.960708</td>\n",
       "      <td>0.988549</td>\n",
       "      <td>0.917128</td>\n",
       "      <td>0.988549</td>\n",
       "      <td>0.967806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1:</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.645161</td>\n",
       "      <td>0.767442</td>\n",
       "      <td>0.720378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log-loss:</th>\n",
       "      <td>0.103740</td>\n",
       "      <td>0.151643</td>\n",
       "      <td>0.131280</td>\n",
       "      <td>0.355930</td>\n",
       "      <td>0.108682</td>\n",
       "      <td>0.170255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4       avg\n",
       "Accuracy:   0.927874  0.877836  0.946869  0.879301  0.929882  0.912352\n",
       "Precision:  0.647059  0.674419  0.573770  0.535714  0.673469  0.620886\n",
       "Recall:     0.891892  0.783784  0.945946  0.810811  0.891892  0.864865\n",
       "MSE:        0.041121  0.041121  0.052336  0.061682  0.037383  0.046729\n",
       "MAE:        0.041121  0.041121  0.052336  0.061682  0.037383  0.046729\n",
       "R^2:        0.361229  0.361229  0.187018  0.041843  0.419299  0.274124\n",
       "auc:        0.927874  0.877836  0.946869  0.879301  0.929882  0.912352\n",
       "roc_auc:    0.984099  0.960708  0.988549  0.917128  0.988549  0.967806\n",
       "F1:         0.750000  0.725000  0.714286  0.645161  0.767442  0.720378\n",
       "log-loss:   0.103740  0.151643  0.131280  0.355930  0.108682  0.170255"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest4_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/pandas/core/arraylike.py:397: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/pandas/core/arraylike.py:397: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/pandas/core/arraylike.py:397: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/pandas/core/arraylike.py:397: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#  Lalonde Subsample \n",
    "nswCps_lalonde_ps_FOREST_withRe78 = propensity_score_funct(nswCps_lalonde_subset,grid_search_forest1,nswCps_lalonde,False)\n",
    "nswPsid_lalonde_ps_FOREST_withRe78= propensity_score_funct(nswPsid_lalonde_subset,grid_search_forest2,nswPsid_lalonde,False)\n",
    "#  Dehejia & Wahba sub sample \n",
    "nswCps_dehWab_ps_FOREST_withRe78 = propensity_score_funct(nswCps_dehWab_subset,grid_search_forest3,nswCps_dehWab,True)\n",
    "nswPsid_dehWab_ps_FOREST_withRe78 = propensity_score_funct(nswPsid_dehWab_subset,grid_search_forest4,nswPsid_dehWab,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>treat</th>\n",
       "      <th>age</th>\n",
       "      <th>education*</th>\n",
       "      <th>black</th>\n",
       "      <th>hispanic</th>\n",
       "      <th>married</th>\n",
       "      <th>nodegree</th>\n",
       "      <th>re74</th>\n",
       "      <th>re75</th>\n",
       "      <th>re78</th>\n",
       "      <th>propensity_score</th>\n",
       "      <th>propensity_logit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9930</td>\n",
       "      <td>0.908419</td>\n",
       "      <td>2.294488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3595</td>\n",
       "      <td>0.595480</td>\n",
       "      <td>0.386667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24909</td>\n",
       "      <td>0.993234</td>\n",
       "      <td>4.989093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>27</td>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7506</td>\n",
       "      <td>0.997318</td>\n",
       "      <td>5.918679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>33</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>289</td>\n",
       "      <td>0.997181</td>\n",
       "      <td>5.868710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16172</th>\n",
       "      <td>0.0</td>\n",
       "      <td>22</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3975</td>\n",
       "      <td>6801</td>\n",
       "      <td>2757</td>\n",
       "      <td>0.795504</td>\n",
       "      <td>1.358431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16173</th>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1445</td>\n",
       "      <td>11832</td>\n",
       "      <td>6895</td>\n",
       "      <td>0.037091</td>\n",
       "      <td>-3.256592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16174</th>\n",
       "      <td>0.0</td>\n",
       "      <td>37</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1733</td>\n",
       "      <td>1559</td>\n",
       "      <td>4221</td>\n",
       "      <td>0.016992</td>\n",
       "      <td>-4.057850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16175</th>\n",
       "      <td>0.0</td>\n",
       "      <td>47</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16914</td>\n",
       "      <td>11384</td>\n",
       "      <td>13671</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>-8.565793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16176</th>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13628</td>\n",
       "      <td>13144</td>\n",
       "      <td>7979</td>\n",
       "      <td>0.000420</td>\n",
       "      <td>-7.773816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16177 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       treat  age  education*  black  hispanic  married  nodegree   re74  \\\n",
       "0        1.0   37          11    1.0       0.0      1.0       1.0      0   \n",
       "1        1.0   22           9    0.0       1.0      0.0       1.0      0   \n",
       "2        1.0   30          12    1.0       0.0      0.0       0.0      0   \n",
       "3        1.0   27          11    1.0       0.0      0.0       1.0      0   \n",
       "4        1.0   33           8    1.0       0.0      0.0       1.0      0   \n",
       "...      ...  ...         ...    ...       ...      ...       ...    ...   \n",
       "16172    0.0   22          12    1.0       0.0      0.0       0.0   3975   \n",
       "16173    0.0   20          12    1.0       0.0      1.0       0.0   1445   \n",
       "16174    0.0   37          12    0.0       0.0      0.0       0.0   1733   \n",
       "16175    0.0   47           9    0.0       0.0      1.0       1.0  16914   \n",
       "16176    0.0   40          10    0.0       0.0      0.0       1.0  13628   \n",
       "\n",
       "        re75   re78  propensity_score  propensity_logit  \n",
       "0          0   9930          0.908419          2.294488  \n",
       "1          0   3595          0.595480          0.386667  \n",
       "2          0  24909          0.993234          4.989093  \n",
       "3          0   7506          0.997318          5.918679  \n",
       "4          0    289          0.997181          5.868710  \n",
       "...      ...    ...               ...               ...  \n",
       "16172   6801   2757          0.795504          1.358431  \n",
       "16173  11832   6895          0.037091         -3.256592  \n",
       "16174   1559   4221          0.016992         -4.057850  \n",
       "16175  11384  13671          0.000190         -8.565793  \n",
       "16176  13144   7979          0.000420         -7.773816  \n",
       "\n",
       "[16177 rows x 12 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nswCps_dehWab_ps_FOREST_withRe78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save unmatched logit datasets \n",
    "nswCps_lalonde_ps_FOREST_withRe78.to_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/forest/unmatched/nswCps_lalonde_ps_unmatched_FOREST_FS1.csv')\n",
    "nswPsid_lalonde_ps_FOREST_withRe78.to_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/forest/unmatched/nswPsid_lalonde_ps_unmatched_FOREST_FS1.csv')\n",
    "nswCps_dehWab_ps_FOREST_withRe78.to_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/forest/unmatched/nswCps_dehWab_ps_unmatched_FOREST_FS1.csv')\n",
    "nswPsid_dehWab_ps_FOREST_withRe78.to_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/forest/unmatched/nswPsid_dehWab_ps_unmatched_FOREST_FS1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model (4) ; Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Select covariates for use in BOOST model \"\"\"\n",
    "#sc = StandardScaler()\n",
    "# ======== Lalonde (1986) data ======== #\n",
    "\n",
    "# Apply selection\n",
    "nswCps_lalonde_subset = nswCps_lalonde[markov_blanket_specification_1]\n",
    "nswPsid_lalonde_subset = nswPsid_lalonde[markov_blanket_specification_2]\n",
    "\n",
    "nswCps_lalonde_target = nswCps_lalonde_subset['treat']\n",
    "nswCps_lalonde_features = nswCps_lalonde_subset.drop('treat', axis=1)\n",
    "nswPsid_lalonde_target = nswPsid_lalonde_subset['treat']\n",
    "nswPsid_lalonde_features = nswPsid_lalonde_subset.drop('treat', axis=1)\n",
    "\n",
    "# ======== Dehejia & Wahba (1986) sub sample ======== #\n",
    "\n",
    "# Apply selection\n",
    "nswCps_dehWab_subset = nswCps_dehWab[markov_blanket_specification_3]\n",
    "nswPsid_dehWab_subset = nswPsid_dehWab[markov_blanket_specification_4]\n",
    "\n",
    "nswCps_dehWab_target = nswCps_dehWab_subset['treat']\n",
    "nswCps_dehWab_features = nswCps_dehWab_subset.drop('treat', axis=1)\n",
    "nswPsid_dehWab_target = nswPsid_dehWab_subset['treat']\n",
    "nswPsid_dehWab_features = nswPsid_dehWab_subset.drop('treat', axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframes to store results \n",
    "boost1_metrics = pd.DataFrame()\n",
    "boost2_metrics = pd.DataFrame()\n",
    "boost3_metrics = pd.DataFrame()\n",
    "boost4_metrics = pd.DataFrame()\n",
    "    \n",
    "\n",
    "# models \n",
    "grid_search_boost1 = XGBClassifier(objective= 'binary:logistic',\n",
    "                                    seed=0,\n",
    "                                    booster = 'dart',\n",
    "                                    colsample_bytree = 0.3,\n",
    "                                    gamma = 0.5,\n",
    "                                    learning_rate = 0.05,\n",
    "                                    max_depth = None,\n",
    "                                    min_child_weight = 2,\n",
    "                                    subsample = 0.5,\n",
    "                                    nthread=4) \n",
    "grid_search_boost2 = XGBClassifier(objective= 'binary:logistic',\n",
    "                                    seed=0,\n",
    "                                    booster = 'dart',\n",
    "                                    colsample_bytree = 0.3,\n",
    "                                    gamma = 0.5,\n",
    "                                    learning_rate = 0.05,\n",
    "                                    max_depth = None,\n",
    "                                    min_child_weight = 2,\n",
    "                                    subsample = 0.5,\n",
    "                                    nthread=4) \n",
    "grid_search_boost3 = XGBClassifier(objective= 'binary:logistic',\n",
    "                                    seed=0,\n",
    "                                    booster = 'gbtree',\n",
    "                                    colsample_bytree = 0.3,\n",
    "                                    gamma = 1,\n",
    "                                    learning_rate = 0.05,\n",
    "                                    min_child_weight = 2,\n",
    "                                    subsample = 0.5,\n",
    "                                    nthread=4) \n",
    "grid_search_boost4 = XGBClassifier(objective= 'binary:logistic',\n",
    "                                    seed=0,\n",
    "                                    booster = 'gbtree',\n",
    "                                    colsample_bytree = 0.3,\n",
    "                                    gamma = 0.5,\n",
    "                                    learning_rate = 0.05,\n",
    "                                    min_child_weight = 2,\n",
    "                                    subsample = 0.5,\n",
    "                                    nthread=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_evaluation_SMOTEENN(grid_search_boost1,nswCps_lalonde_features,nswCps_lalonde_target,boost1_metrics) # cps\n",
    "kfold_evaluation_SMOTEENN(grid_search_boost2,nswPsid_lalonde_features,nswPsid_lalonde_target,boost2_metrics) # psid\n",
    "kfold_evaluation_SMOTEENN(grid_search_boost3,nswCps_dehWab_features,nswCps_dehWab_target,boost3_metrics) # cps\n",
    "kfold_evaluation_SMOTEENN(grid_search_boost4,nswPsid_dehWab_features,nswPsid_dehWab_target,boost4_metrics) # psid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost1_metrics['avg'] = np.mean(boost1_metrics,axis=1)\n",
    "boost2_metrics['avg'] = np.mean(boost2_metrics,axis=1)\n",
    "boost3_metrics['avg'] = np.mean(boost3_metrics,axis=1)\n",
    "boost4_metrics['avg'] = np.mean(boost4_metrics,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy:</th>\n",
       "      <td>0.851220</td>\n",
       "      <td>0.894199</td>\n",
       "      <td>0.843322</td>\n",
       "      <td>0.887971</td>\n",
       "      <td>0.924889</td>\n",
       "      <td>0.880320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision:</th>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.205645</td>\n",
       "      <td>0.105727</td>\n",
       "      <td>0.152695</td>\n",
       "      <td>0.132867</td>\n",
       "      <td>0.146053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall:</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.813559</td>\n",
       "      <td>0.864407</td>\n",
       "      <td>0.966102</td>\n",
       "      <td>0.858814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE:</th>\n",
       "      <td>0.099448</td>\n",
       "      <td>0.063229</td>\n",
       "      <td>0.127993</td>\n",
       "      <td>0.089319</td>\n",
       "      <td>0.114830</td>\n",
       "      <td>0.098963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE:</th>\n",
       "      <td>0.099448</td>\n",
       "      <td>0.063229</td>\n",
       "      <td>0.127993</td>\n",
       "      <td>0.089319</td>\n",
       "      <td>0.114830</td>\n",
       "      <td>0.098963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R^2:</th>\n",
       "      <td>-4.501313</td>\n",
       "      <td>-2.497749</td>\n",
       "      <td>-6.198150</td>\n",
       "      <td>-4.023169</td>\n",
       "      <td>-5.455931</td>\n",
       "      <td>-4.535262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc:</th>\n",
       "      <td>0.851220</td>\n",
       "      <td>0.894199</td>\n",
       "      <td>0.843322</td>\n",
       "      <td>0.887971</td>\n",
       "      <td>0.924889</td>\n",
       "      <td>0.880320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc:</th>\n",
       "      <td>0.943264</td>\n",
       "      <td>0.955910</td>\n",
       "      <td>0.926452</td>\n",
       "      <td>0.957691</td>\n",
       "      <td>0.965794</td>\n",
       "      <td>0.949822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1:</th>\n",
       "      <td>0.228571</td>\n",
       "      <td>0.331169</td>\n",
       "      <td>0.187135</td>\n",
       "      <td>0.259542</td>\n",
       "      <td>0.233607</td>\n",
       "      <td>0.248005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log-loss:</th>\n",
       "      <td>0.226585</td>\n",
       "      <td>0.170982</td>\n",
       "      <td>0.252035</td>\n",
       "      <td>0.229116</td>\n",
       "      <td>0.259508</td>\n",
       "      <td>0.227645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4       avg\n",
       "Accuracy:   0.851220  0.894199  0.843322  0.887971  0.924889  0.880320\n",
       "Precision:  0.133333  0.205645  0.105727  0.152695  0.132867  0.146053\n",
       "Recall:     0.800000  0.850000  0.813559  0.864407  0.966102  0.858814\n",
       "MSE:        0.099448  0.063229  0.127993  0.089319  0.114830  0.098963\n",
       "MAE:        0.099448  0.063229  0.127993  0.089319  0.114830  0.098963\n",
       "R^2:       -4.501313 -2.497749 -6.198150 -4.023169 -5.455931 -4.535262\n",
       "auc:        0.851220  0.894199  0.843322  0.887971  0.924889  0.880320\n",
       "roc_auc:    0.943264  0.955910  0.926452  0.957691  0.965794  0.949822\n",
       "F1:         0.228571  0.331169  0.187135  0.259542  0.233607  0.248005\n",
       "log-loss:   0.226585  0.170982  0.252035  0.229116  0.259508  0.227645"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boost1_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy:</th>\n",
       "      <td>0.880823</td>\n",
       "      <td>0.915562</td>\n",
       "      <td>0.876098</td>\n",
       "      <td>0.897624</td>\n",
       "      <td>0.931404</td>\n",
       "      <td>0.900302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision:</th>\n",
       "      <td>0.536842</td>\n",
       "      <td>0.670886</td>\n",
       "      <td>0.556818</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.565657</td>\n",
       "      <td>0.597620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall:</th>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.847458</td>\n",
       "      <td>0.949153</td>\n",
       "      <td>0.872090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE:</th>\n",
       "      <td>0.094982</td>\n",
       "      <td>0.059140</td>\n",
       "      <td>0.087971</td>\n",
       "      <td>0.062837</td>\n",
       "      <td>0.082585</td>\n",
       "      <td>0.077503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE:</th>\n",
       "      <td>0.094982</td>\n",
       "      <td>0.059140</td>\n",
       "      <td>0.087971</td>\n",
       "      <td>0.062837</td>\n",
       "      <td>0.082585</td>\n",
       "      <td>0.077503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R^2:</th>\n",
       "      <td>0.010241</td>\n",
       "      <td>0.383735</td>\n",
       "      <td>0.071098</td>\n",
       "      <td>0.336499</td>\n",
       "      <td>0.127970</td>\n",
       "      <td>0.185908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc:</th>\n",
       "      <td>0.880823</td>\n",
       "      <td>0.915562</td>\n",
       "      <td>0.876098</td>\n",
       "      <td>0.897624</td>\n",
       "      <td>0.931404</td>\n",
       "      <td>0.900302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc:</th>\n",
       "      <td>0.944478</td>\n",
       "      <td>0.969093</td>\n",
       "      <td>0.936628</td>\n",
       "      <td>0.967701</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.957556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1:</th>\n",
       "      <td>0.658065</td>\n",
       "      <td>0.762590</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.708861</td>\n",
       "      <td>0.707385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log-loss:</th>\n",
       "      <td>0.229203</td>\n",
       "      <td>0.184353</td>\n",
       "      <td>0.241779</td>\n",
       "      <td>0.198972</td>\n",
       "      <td>0.239462</td>\n",
       "      <td>0.218754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4       avg\n",
       "Accuracy:   0.880823  0.915562  0.876098  0.897624  0.931404  0.900302\n",
       "Precision:  0.536842  0.670886  0.556818  0.657895  0.565657  0.597620\n",
       "Recall:     0.850000  0.883333  0.830508  0.847458  0.949153  0.872090\n",
       "MSE:        0.094982  0.059140  0.087971  0.062837  0.082585  0.077503\n",
       "MAE:        0.094982  0.059140  0.087971  0.062837  0.082585  0.077503\n",
       "R^2:        0.010241  0.383735  0.071098  0.336499  0.127970  0.185908\n",
       "auc:        0.880823  0.915562  0.876098  0.897624  0.931404  0.900302\n",
       "roc_auc:    0.944478  0.969093  0.936628  0.967701  0.969880  0.957556\n",
       "F1:         0.658065  0.762590  0.666667  0.740741  0.708861  0.707385\n",
       "log-loss:   0.229203  0.184353  0.241779  0.198972  0.239462  0.218754"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boost2_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy:</th>\n",
       "      <td>0.909300</td>\n",
       "      <td>0.894452</td>\n",
       "      <td>0.898594</td>\n",
       "      <td>0.905384</td>\n",
       "      <td>0.938733</td>\n",
       "      <td>0.909293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision:</th>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.116364</td>\n",
       "      <td>0.192547</td>\n",
       "      <td>0.156098</td>\n",
       "      <td>0.137795</td>\n",
       "      <td>0.156116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall:</th>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.875676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE:</th>\n",
       "      <td>0.047281</td>\n",
       "      <td>0.076638</td>\n",
       "      <td>0.042040</td>\n",
       "      <td>0.055023</td>\n",
       "      <td>0.068315</td>\n",
       "      <td>0.057859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE:</th>\n",
       "      <td>0.047281</td>\n",
       "      <td>0.076638</td>\n",
       "      <td>0.042040</td>\n",
       "      <td>0.055023</td>\n",
       "      <td>0.068315</td>\n",
       "      <td>0.057859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R^2:</th>\n",
       "      <td>-3.182963</td>\n",
       "      <td>-5.780227</td>\n",
       "      <td>-2.718202</td>\n",
       "      <td>-3.866471</td>\n",
       "      <td>-5.042079</td>\n",
       "      <td>-4.117988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc:</th>\n",
       "      <td>0.909300</td>\n",
       "      <td>0.894452</td>\n",
       "      <td>0.898594</td>\n",
       "      <td>0.905384</td>\n",
       "      <td>0.938733</td>\n",
       "      <td>0.909293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc:</th>\n",
       "      <td>0.976610</td>\n",
       "      <td>0.949676</td>\n",
       "      <td>0.981327</td>\n",
       "      <td>0.961517</td>\n",
       "      <td>0.986689</td>\n",
       "      <td>0.971164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1:</th>\n",
       "      <td>0.294931</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>0.313131</td>\n",
       "      <td>0.264463</td>\n",
       "      <td>0.240550</td>\n",
       "      <td>0.263641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log-loss:</th>\n",
       "      <td>0.150345</td>\n",
       "      <td>0.187916</td>\n",
       "      <td>0.143167</td>\n",
       "      <td>0.157134</td>\n",
       "      <td>0.174151</td>\n",
       "      <td>0.162542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4       avg\n",
       "Accuracy:   0.909300  0.894452  0.898594  0.905384  0.938733  0.909293\n",
       "Precision:  0.177778  0.116364  0.192547  0.156098  0.137795  0.156116\n",
       "Recall:     0.864865  0.864865  0.837838  0.864865  0.945946  0.875676\n",
       "MSE:        0.047281  0.076638  0.042040  0.055023  0.068315  0.057859\n",
       "MAE:        0.047281  0.076638  0.042040  0.055023  0.068315  0.057859\n",
       "R^2:       -3.182963 -5.780227 -2.718202 -3.866471 -5.042079 -4.117988\n",
       "auc:        0.909300  0.894452  0.898594  0.905384  0.938733  0.909293\n",
       "roc_auc:    0.976610  0.949676  0.981327  0.961517  0.986689  0.971164\n",
       "F1:         0.294931  0.205128  0.313131  0.264463  0.240550  0.263641\n",
       "log-loss:   0.150345  0.187916  0.143167  0.157134  0.174151  0.162542"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boost3_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy:</th>\n",
       "      <td>0.934820</td>\n",
       "      <td>0.887794</td>\n",
       "      <td>0.935824</td>\n",
       "      <td>0.873195</td>\n",
       "      <td>0.939840</td>\n",
       "      <td>0.914295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision:</th>\n",
       "      <td>0.479452</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.486111</td>\n",
       "      <td>0.351648</td>\n",
       "      <td>0.514706</td>\n",
       "      <td>0.466383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall:</th>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.908108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE:</th>\n",
       "      <td>0.074766</td>\n",
       "      <td>0.069159</td>\n",
       "      <td>0.072897</td>\n",
       "      <td>0.119626</td>\n",
       "      <td>0.065421</td>\n",
       "      <td>0.080374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE:</th>\n",
       "      <td>0.074766</td>\n",
       "      <td>0.069159</td>\n",
       "      <td>0.072897</td>\n",
       "      <td>0.119626</td>\n",
       "      <td>0.065421</td>\n",
       "      <td>0.080374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R^2:</th>\n",
       "      <td>-0.161402</td>\n",
       "      <td>-0.074297</td>\n",
       "      <td>-0.132367</td>\n",
       "      <td>-0.858244</td>\n",
       "      <td>-0.016227</td>\n",
       "      <td>-0.248508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc:</th>\n",
       "      <td>0.934820</td>\n",
       "      <td>0.887794</td>\n",
       "      <td>0.935824</td>\n",
       "      <td>0.873195</td>\n",
       "      <td>0.939840</td>\n",
       "      <td>0.914295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc:</th>\n",
       "      <td>0.984641</td>\n",
       "      <td>0.959541</td>\n",
       "      <td>0.983773</td>\n",
       "      <td>0.922148</td>\n",
       "      <td>0.983149</td>\n",
       "      <td>0.966650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1:</th>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.626263</td>\n",
       "      <td>0.642202</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.614299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log-loss:</th>\n",
       "      <td>0.176870</td>\n",
       "      <td>0.174284</td>\n",
       "      <td>0.187314</td>\n",
       "      <td>0.259563</td>\n",
       "      <td>0.173311</td>\n",
       "      <td>0.194268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4       avg\n",
       "Accuracy:   0.934820  0.887794  0.935824  0.873195  0.939840  0.914295\n",
       "Precision:  0.479452  0.500000  0.486111  0.351648  0.514706  0.466383\n",
       "Recall:     0.945946  0.837838  0.945946  0.864865  0.945946  0.908108\n",
       "MSE:        0.074766  0.069159  0.072897  0.119626  0.065421  0.080374\n",
       "MAE:        0.074766  0.069159  0.072897  0.119626  0.065421  0.080374\n",
       "R^2:       -0.161402 -0.074297 -0.132367 -0.858244 -0.016227 -0.248508\n",
       "auc:        0.934820  0.887794  0.935824  0.873195  0.939840  0.914295\n",
       "roc_auc:    0.984641  0.959541  0.983773  0.922148  0.983149  0.966650\n",
       "F1:         0.636364  0.626263  0.642202  0.500000  0.666667  0.614299\n",
       "log-loss:   0.176870  0.174284  0.187314  0.259563  0.173311  0.194268"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boost4_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/pandas/core/arraylike.py:397: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/pandas/core/arraylike.py:397: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#  Lalonde Subsample \n",
    "nswCps_lalonde_ps_BOOST_withRe78 = propensity_score_funct(nswCps_lalonde_subset,grid_search_boost1,nswCps_lalonde,False)\n",
    "nswPsid_lalonde_ps_BOOST_withRe78= propensity_score_funct(nswPsid_lalonde_subset,grid_search_boost2,nswPsid_lalonde,False)\n",
    "#  Dehejia & Wahba sub sample \n",
    "nswCps_dehWab_ps_BOOST_withRe78 = propensity_score_funct(nswCps_dehWab_subset,grid_search_forest3,nswCps_dehWab,True)\n",
    "nswPsid_dehWab_ps_BOOST_withRe78 = propensity_score_funct(nswPsid_dehWab_subset,grid_search_forest4,nswPsid_dehWab,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save unmatched boost datasets \n",
    "nswCps_lalonde_ps_BOOST_withRe78.to_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/boost/unmatched/nswCps_lalonde_ps_unmatched_BOOST_FS1.csv')\n",
    "nswPsid_lalonde_ps_BOOST_withRe78.to_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/boost/unmatched/nswPsid_lalonde_ps_unmatched_BOOST_FS1.csv')\n",
    "nswCps_dehWab_ps_BOOST_withRe78.to_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/boost/unmatched/nswCps_dehWab_ps_unmatched_BOOST_FS1.csv')\n",
    "nswPsid_dehWab_ps_BOOST_withRe78.to_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/boost/unmatched/nswPsid_dehWab_ps_unmatched_BOOST_FS1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>treat</th>\n",
       "      <th>age</th>\n",
       "      <th>education*</th>\n",
       "      <th>black</th>\n",
       "      <th>hispanic</th>\n",
       "      <th>married</th>\n",
       "      <th>nodegree</th>\n",
       "      <th>re75</th>\n",
       "      <th>re78</th>\n",
       "      <th>propensity_score</th>\n",
       "      <th>propensity_logit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9930</td>\n",
       "      <td>0.524682</td>\n",
       "      <td>0.098808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3595</td>\n",
       "      <td>0.925366</td>\n",
       "      <td>2.517594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24909</td>\n",
       "      <td>0.649052</td>\n",
       "      <td>0.614874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>27</td>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7506</td>\n",
       "      <td>0.917422</td>\n",
       "      <td>2.407825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>33</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>289</td>\n",
       "      <td>0.751119</td>\n",
       "      <td>1.104587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2782</th>\n",
       "      <td>0.0</td>\n",
       "      <td>47</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33837</td>\n",
       "      <td>38568</td>\n",
       "      <td>0.063609</td>\n",
       "      <td>-2.689278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2783</th>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67137</td>\n",
       "      <td>59109</td>\n",
       "      <td>0.136257</td>\n",
       "      <td>-1.846732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2784</th>\n",
       "      <td>0.0</td>\n",
       "      <td>47</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>47968</td>\n",
       "      <td>55710</td>\n",
       "      <td>0.090488</td>\n",
       "      <td>-2.307686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2785</th>\n",
       "      <td>0.0</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44220</td>\n",
       "      <td>20540</td>\n",
       "      <td>0.034184</td>\n",
       "      <td>-3.341218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2786</th>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55500</td>\n",
       "      <td>53198</td>\n",
       "      <td>0.106333</td>\n",
       "      <td>-2.128757</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2787 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      treat  age  education*  black  hispanic  married  nodegree   re75  \\\n",
       "0       1.0   37          11    1.0       0.0      1.0       1.0      0   \n",
       "1       1.0   22           9    0.0       1.0      0.0       1.0      0   \n",
       "2       1.0   30          12    1.0       0.0      0.0       0.0      0   \n",
       "3       1.0   27          11    1.0       0.0      0.0       1.0      0   \n",
       "4       1.0   33           8    1.0       0.0      0.0       1.0      0   \n",
       "...     ...  ...         ...    ...       ...      ...       ...    ...   \n",
       "2782    0.0   47           8    0.0       0.0      1.0       1.0  33837   \n",
       "2783    0.0   32           8    0.0       0.0      1.0       1.0  67137   \n",
       "2784    0.0   47          10    0.0       0.0      1.0       1.0  47968   \n",
       "2785    0.0   54           0    0.0       1.0      1.0       1.0  44220   \n",
       "2786    0.0   40           8    0.0       0.0      1.0       1.0  55500   \n",
       "\n",
       "       re78  propensity_score  propensity_logit  \n",
       "0      9930          0.524682          0.098808  \n",
       "1      3595          0.925366          2.517594  \n",
       "2     24909          0.649052          0.614874  \n",
       "3      7506          0.917422          2.407825  \n",
       "4       289          0.751119          1.104587  \n",
       "...     ...               ...               ...  \n",
       "2782  38568          0.063609         -2.689278  \n",
       "2783  59109          0.136257         -1.846732  \n",
       "2784  55710          0.090488         -2.307686  \n",
       "2785  20540          0.034184         -3.341218  \n",
       "2786  53198          0.106333         -2.128757  \n",
       "\n",
       "[2787 rows x 11 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nswPsid_lalonde_ps_BOOST_withRe78"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model (5) - artifical neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import  DataLoader,SubsetRandomSampler \n",
    "from sklearn.preprocessing import StandardScaler , MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Import standard libraries \n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import random \n",
    "import numpy.random as rand\n",
    "from random import randrange\n",
    "import seaborn as sns\n",
    "from scipy import stats \n",
    "from psmpy.functions import cohenD\n",
    "from psmpy.plotting import *\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from imblearn.combine import SMOTEENN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "annSpecification_1 = ['treat','ageboxcox','education*','black','married','nodegree','re75'] # CPS Lalonde sample \n",
    "annSpecification_2 = ['treat','ageboxcox','education*','married','nodegree','re75']# PSID Lalonde sample  \n",
    "annSpecification_3 = ['treat','ageboxcox','education*','black','married','re74','re75'] # CPS Dehwab sample\n",
    "annSpecification_4 = ['treat','ageboxcox','education*','married','nodegree','re74','re75'] # PSID Dehwab and wahba sample \n",
    "nsw_continuos_vars1 = ['ageboxcox','re75'] \n",
    "nsw_continuos_vars2 = ['ageboxcox','re75'] \n",
    "nswre74_continuos_vars1 = ['ageboxcox','re75',] \n",
    "nswre74_continuos_vars2 = ['ageboxcox','re74','re75',] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample=SMOTEENN(random_state=0)\n",
    "sm = SMOTE(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load quasi-experimental dataset\n",
    "\n",
    "nswCps_lalonde =  pd.read_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/unmatched data/Quasi_NswCps_lalonde.csv')\n",
    "nswPsid_lalonde = pd.read_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/unmatched data/Quasi_NswPsid_lalonde.csv')\n",
    "nswCps_dehWab =   pd.read_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/unmatched data/Quasi_NswCps_dehWab.csv')\n",
    "nswPsid_dehWab =  pd.read_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/unmatched data/Quasi_NswPsid_dehWab.csv')\n",
    "\n",
    "nswCps_lalonde.columns = nswCps_lalonde.columns.str.strip() \n",
    "nswPsid_lalonde.columns = nswPsid_lalonde.columns.str.strip() \n",
    "nswCps_dehWab.columns = nswCps_dehWab.columns.str.strip() \n",
    "nswPsid_dehWab.columns = nswPsid_dehWab.columns.str.strip() \n",
    "\n",
    "# Store outcome variable prior to modelling\n",
    "nswCps_lalonde_re78 = nswCps_lalonde.re78\n",
    "nswPsid_lalonde_re78 = nswPsid_lalonde.re78\n",
    "nswCps_dehWab_re78 = nswCps_dehWab.re78\n",
    "nswPsid_dehWab_re78 = nswPsid_dehWab.re78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sp/c9zy568j6s94zqs20ynd8pz00000gn/T/ipykernel_53989/3854445176.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  nswCps_lalonde_subset[nsw_continuos_vars1] = sc.fit_transform(nswCps_lalonde_subset[nsw_continuos_vars1])\n",
      "/var/folders/sp/c9zy568j6s94zqs20ynd8pz00000gn/T/ipykernel_53989/3854445176.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  nswCps_dehWab_subset[nswre74_continuos_vars1] = sc.fit_transform(nswCps_dehWab_subset[nswre74_continuos_vars1])\n",
      "/var/folders/sp/c9zy568j6s94zqs20ynd8pz00000gn/T/ipykernel_53989/3854445176.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  nswPsid_dehWab_subset[nswre74_continuos_vars2] = sc.fit_transform(nswPsid_dehWab_subset[nswre74_continuos_vars2])\n"
     ]
    }
   ],
   "source": [
    "# ANN data pre-processing\n",
    "# Select covariates \n",
    "# ======== Lalonde (1986) data ======== #\n",
    "\n",
    "\n",
    "nswCps_lalonde_subset = nswCps_lalonde[annSpecification_1]\n",
    "nswPsid_lalonde_subset = nswPsid_lalonde[annSpecification_2]\n",
    "\n",
    "# Z-score normalise continous variables\n",
    "sc = MinMaxScaler()\n",
    "\n",
    "nswCps_lalonde_subset[nsw_continuos_vars1] = sc.fit_transform(nswCps_lalonde_subset[nsw_continuos_vars1])\n",
    "#nswPsid_lalonde_subset[nsw_continuos_vars2] = sc.fit_transform(nswPsid_lalonde_subset[nsw_continuos_vars2])\n",
    "\n",
    "nswCps_lalonde_subset = nswCps_lalonde_subset.sample(frac=1).reset_index(drop=True)\n",
    "nswPsid_lalonde_subset = nswPsid_lalonde_subset.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Create targets and featues\n",
    "nswCps_lalonde_target = nswCps_lalonde_subset['treat']\n",
    "nswCps_lalonde_features = nswCps_lalonde_subset.drop('treat', axis=1)\n",
    "nswPsid_lalonde_target = nswPsid_lalonde_subset['treat']\n",
    "nswPsid_lalonde_features = nswPsid_lalonde_subset.drop('treat', axis=1)\n",
    "\n",
    "# ======== Dehejia & Wahba (1986) sub sample ======== #\n",
    "\n",
    "\n",
    "nswCps_dehWab_subset = nswCps_dehWab[annSpecification_3]\n",
    "nswPsid_dehWab_subset = nswPsid_dehWab[annSpecification_4]\n",
    "\n",
    "nswCps_dehWab_subset[nswre74_continuos_vars1] = sc.fit_transform(nswCps_dehWab_subset[nswre74_continuos_vars1])\n",
    "nswPsid_dehWab_subset[nswre74_continuos_vars2] = sc.fit_transform(nswPsid_dehWab_subset[nswre74_continuos_vars2])\n",
    "\n",
    "nswCps_dehWab_subset = nswCps_dehWab_subset.sample(frac=1).reset_index(drop=True)\n",
    "nswPsid_dehWab_subset = nswPsid_dehWab_subset.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# create targets and features \n",
    "nswCps_dehWab_target = nswCps_dehWab_subset['treat']\n",
    "nswCps_dehWab_features = nswCps_dehWab_subset.drop('treat', axis=1)\n",
    "nswPsid_dehWab_target = nswPsid_dehWab_subset['treat']\n",
    "nswPsid_dehWab_features = nswPsid_dehWab_subset.drop('treat', axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Neural network </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN model \n",
    "hid_1 = 12\n",
    "hid_2 = 12\n",
    "\n",
    "class twoLayerNN1(nn.Module):\n",
    "    def __init__(self,num_features = nswCps_lalonde_features.shape[1], num_classes=1, num_hidden_1=hid_1, num_hidden_2=hid_2):\n",
    "       \n",
    "       \n",
    "        super(twoLayerNN1, self).__init__() \n",
    "       \n",
    "        self.input_layer =  nn.Linear(num_features,num_hidden_1)                        \n",
    "        self.hidden_layer1 = nn.Linear(num_hidden_1,num_hidden_2)                          \n",
    "        self.out_layer =  nn.Linear(num_hidden_2, num_classes) \n",
    "\n",
    "        # Activation function\n",
    "        self.relu_activation = nn.ReLU()    \n",
    "        self.output_activation = nn.Sigmoid()\n",
    "        #Drop out and batch normalisation \n",
    "        #self.dropout = nn.Dropout(p=0.35)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(num_hidden_1)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(num_hidden_2)\n",
    "        \n",
    "    # Forward propogation method\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = self.relu_activation(self.input_layer(X))\n",
    "        X = self.batchnorm1(X)\n",
    "        X = self.relu_activation(self.hidden_layer1(X))\n",
    "        X = self.batchnorm2(X)\n",
    "        #X = self.dropout(X)\n",
    "        X = self.output_activation(self.out_layer(X))\n",
    "        return X\n",
    "\n",
    "class twoLayerNN2(nn.Module):\n",
    "    def __init__(self,num_features = nswPsid_lalonde_features.shape[1], num_classes=1, num_hidden_1=hid_1, num_hidden_2=hid_2):\n",
    "       \n",
    "        super(twoLayerNN2, self).__init__() \n",
    "       \n",
    "        self.input_layer =  nn.Linear(num_features,num_hidden_1)                        \n",
    "        self.hidden_layer1 = nn.Linear(num_hidden_1,num_hidden_2)                          \n",
    "        self.out_layer =  nn.Linear(num_hidden_2, num_classes) \n",
    "\n",
    "        # Activation function\n",
    "        self.relu_activation = nn.ReLU()    \n",
    "        self.output_activation = nn.Sigmoid()\n",
    "        #Drop out and batch normalisation \n",
    "        self.dropout = nn.Dropout(p=0.35)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(num_hidden_1)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(num_hidden_2)\n",
    "        \n",
    "    # Forward propogation method\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = self.relu_activation(self.input_layer(X))\n",
    "        X = self.batchnorm1(X)\n",
    "        X = self.relu_activation(self.hidden_layer1(X))\n",
    "        X = self.batchnorm2(X)\n",
    "        #X = self.dropout(X)\n",
    "        X = self.output_activation(self.out_layer(X))\n",
    "        return X       \n",
    "\n",
    "class twoLayerNN3(nn.Module):\n",
    "    def __init__(self,num_features = nswCps_dehWab_features.shape[1], num_classes=1, num_hidden_1=hid_1, num_hidden_2=hid_2):\n",
    "       \n",
    "        super(twoLayerNN3, self).__init__() \n",
    "       \n",
    "        self.input_layer =  nn.Linear(num_features,num_hidden_1)                        \n",
    "        self.hidden_layer1 = nn.Linear(num_hidden_1,num_hidden_2)                          \n",
    "        self.out_layer =  nn.Linear(num_hidden_2, num_classes) \n",
    "\n",
    "        # Activation function\n",
    "        self.relu_activation = nn.ReLU()    \n",
    "        self.output_activation = nn.Sigmoid()\n",
    "        #Drop out and batch normalisation \n",
    "        self.dropout = nn.Dropout(p=0.35)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(num_hidden_1)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(num_hidden_2)\n",
    "        \n",
    "    # Forward propogation method\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = self.relu_activation(self.input_layer(X))\n",
    "        X = self.batchnorm1(X)\n",
    "        X = self.relu_activation(self.hidden_layer1(X))\n",
    "        X = self.batchnorm2(X)\n",
    "       # X = self.dropout(X)\n",
    "        X = self.output_activation(self.out_layer(X))\n",
    "        return X       \n",
    "\n",
    "class twoLayerNN4(nn.Module):\n",
    "    def __init__(self,num_features = nswPsid_dehWab_features.shape[1], num_classes=1, num_hidden_1=hid_1, num_hidden_2=hid_2):\n",
    "        super(twoLayerNN4, self).__init__() \n",
    "       \n",
    "        self.input_layer =  nn.Linear(num_features,num_hidden_1)                        \n",
    "        self.hidden_layer1 = nn.Linear(num_hidden_1,num_hidden_2)                          \n",
    "        self.out_layer =  nn.Linear(num_hidden_2, num_classes) \n",
    "\n",
    "        # Activation function\n",
    "        self.relu_activation = nn.ReLU()    \n",
    "        self.output_activation = nn.Sigmoid()\n",
    "        #Drop out and batch normalisation \n",
    "        self.dropout = nn.Dropout(p=0.35)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(num_hidden_1)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(num_hidden_2)\n",
    "        \n",
    "    # Forward propogation method\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = self.relu_activation(self.input_layer(X))\n",
    "        X = self.batchnorm1(X)\n",
    "        X = self.relu_activation(self.hidden_layer1(X))\n",
    "        X = self.batchnorm2(X)\n",
    "        #X = self.dropout(X)\n",
    "        X = self.output_activation(self.out_layer(X))\n",
    "        return X       \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch import NeuralNetClassifier\n",
    "Epochs = 50\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0018\n",
    "batch_size = 480\n",
    "\n",
    "ANN1 = NeuralNetClassifier(twoLayerNN1,max_epochs=Epochs,lr=learning_rate,optimizer__weight_decay=weight_decay,batch_size =batch_size,optimizer = optim.SGD,criterion = nn.BCELoss,iterator_valid__shuffle=False)\n",
    "ANN2 = NeuralNetClassifier(twoLayerNN2,max_epochs=Epochs,lr=learning_rate,optimizer__weight_decay=weight_decay,batch_size = batch_size,optimizer = optim.SGD,criterion = nn.BCELoss,iterator_valid__shuffle=False )\n",
    "ANN3 = NeuralNetClassifier(twoLayerNN3,max_epochs=Epochs,lr=learning_rate,optimizer__weight_decay=weight_decay,batch_size = batch_size,optimizer = optim.SGD,criterion = nn.BCELoss,iterator_valid__shuffle=False )\n",
    "ANN4 = NeuralNetClassifier(twoLayerNN4,max_epochs=Epochs,lr=learning_rate,optimizer__weight_decay=weight_decay,batch_size = batch_size,optimizer = optim.SGD,criterion = nn.BCELoss,iterator_valid__shuffle=False  )\n",
    "                                                  \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
    "from sklearn import metrics\n",
    "\n",
    "# KFold cross validation \n",
    "def kfold_evaluation_ANN(input_model,features,target,store):\n",
    "\t# prepare the cross-validation procedure\n",
    "\tskf = StratifiedKFold(n_splits=5, shuffle=True,random_state=1)\n",
    "\t\n",
    "\t#Function to get model scores\t\n",
    "\tdef get_score(model,X_train , X_test, y_train , y_test,fold,store):\n",
    "\t\tmodel.fit(X_train,y_train)\n",
    "\t\tprediction = model.predict(X_test)\n",
    "\t\tprediction_proba = model.predict_proba(X_test)\n",
    "\t\tstore.loc['Accuracy:', fold] = metrics.balanced_accuracy_score(y_test, prediction)\n",
    "\t\tstore.loc['Precision:', fold] = metrics.precision_score(y_test, prediction)\n",
    "\t\tstore.loc['Recall:', fold] = metrics.recall_score(y_test, prediction)\n",
    "\t\tstore.loc['MSE:', fold] =  metrics.mean_squared_error(y_test, prediction)\n",
    "\t\tstore.loc['MAE:', fold] =  metrics.mean_absolute_error(y_test, prediction)\n",
    "\t\tstore.loc['R^2:', fold] =  metrics.r2_score(y_test, prediction)\n",
    "\t\tstore.loc['roc_auc:', fold] =  metrics.roc_auc_score(y_test, prediction_proba)\n",
    "\t\tstore.loc['roc_auc:', fold] =  metrics.roc_auc_score(y_test, prediction_proba)\n",
    "\t\tstore.loc['F1:', fold] =  metrics.f1_score(y_test, prediction)\n",
    "\t\tstore.loc['logloss:', fold] =  metrics.log_loss(y_test, prediction_proba)\n",
    "\t\n",
    "\t#Kfold training loop\n",
    "\tfor fold, (train_index , test_index) in enumerate(skf.split(features,target)):\n",
    "\t\t#print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "\n",
    "\t\tX_train , X_test, y_train , y_test = features[train_index],features[test_index],\\\n",
    "\t\t\t\t\t\t\t\t\t\t\t\ttarget[train_index] , target[test_index]\n",
    "\t\t\n",
    "\t\tprint('Xtrain pre augmentation:',X_train.shape)\n",
    "\t\tprint('Ytrain pre augmentation',y_train.shape) \n",
    "\n",
    "\t\tX_train, y_train = resample.fit_resample(X_train, y_train)\t\n",
    "\t\tshuffled = pd.concat([pd.DataFrame(X_train),pd.DataFrame(y_train)],axis=1).sample(frac=1).reset_index(drop=True)\n",
    "        #resplit into test and train \n",
    "\t\tX_train, y_train = shuffled.iloc[:,:-1], shuffled.iloc[:,-1]\n",
    "\t\tX_train = X_train.to_numpy()\n",
    "\t\ty_train = y_train.to_numpy()\n",
    "\t\tX_train = X_train.astype(np.float32)\n",
    "\t\ty_train = y_train.astype(np.float32)\n",
    "\t\ty_train = np.reshape(y_train,(y_train.shape[0],1))\n",
    "\n",
    "\t\tprint('Xtrain:',X_train.shape)\n",
    "\t\tprint('Ytrain',y_train.shape)\t\n",
    "\t\tprint('Xtest',X_test.shape)\t\n",
    "\t\tprint('Ytest',y_test.shape)\t\n",
    "\t\t\t\t\t\t\t\t\t \n",
    "\t\tget_score(input_model,X_train , X_test, y_train , y_test,fold,store)\n",
    "\t\t# return statement here with python dataframe of metrics \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conver features and targets to numpy arrays then floats to feed to ANN\n",
    "X1 = nswCps_lalonde_features.to_numpy()\n",
    "Y1 = nswCps_lalonde_target.to_numpy()\n",
    "X1 = X1.astype(np.float32)\n",
    "Y1 = Y1.astype(np.float32)\n",
    "Y1 = np.reshape(Y1,(Y1.shape[0],1))\n",
    "\n",
    "# ~ ~ ~\n",
    "X2 = nswPsid_lalonde_features.to_numpy()\n",
    "Y2 = nswPsid_lalonde_target.to_numpy()\n",
    "X2 = X2.astype(np.float32)\n",
    "Y2 = Y2.astype(np.float32)\n",
    "Y2 = np.reshape(Y2,(Y2.shape[0],1))\n",
    "# ~ ~ ~\n",
    "X3 = nswCps_dehWab_features.to_numpy()\n",
    "Y3 = nswCps_dehWab_target.to_numpy()\n",
    "X3 = X3.astype(np.float32)\n",
    "Y3 = Y3.astype(np.float32)\n",
    "Y3 = np.reshape(Y3,(Y3.shape[0],1))\n",
    "# ~ ~ ~\n",
    "X4 = nswPsid_dehWab_features.to_numpy()\n",
    "Y4 = nswPsid_dehWab_target.to_numpy()\n",
    "X4 = X4.astype(np.float32)\n",
    "Y4 = Y4.astype(np.float32)\n",
    "Y4 = np.reshape(Y4,(Y4.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain pre augmentation: (13031, 6)\n",
      "Ytrain pre augmentation (13031, 1)\n",
      "Xtrain: (23917, 6)\n",
      "Ytrain (23917, 1)\n",
      "Xtest (3258, 6)\n",
      "Ytest (3258, 1)\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7901\u001b[0m       \u001b[32m0.5040\u001b[0m        \u001b[35m0.7307\u001b[0m  0.3891\n",
      "      2        \u001b[36m0.7233\u001b[0m       0.5040        \u001b[35m0.6981\u001b[0m  0.2744\n",
      "      3        \u001b[36m0.6693\u001b[0m       0.5040        \u001b[35m0.6546\u001b[0m  0.2287\n",
      "      4        \u001b[36m0.6163\u001b[0m       0.5040        \u001b[35m0.6148\u001b[0m  0.2306\n",
      "      5        \u001b[36m0.5716\u001b[0m       0.5040        \u001b[35m0.5795\u001b[0m  0.2316\n",
      "      6        \u001b[36m0.5383\u001b[0m       0.5040        \u001b[35m0.5494\u001b[0m  0.2288\n",
      "      7        \u001b[36m0.5123\u001b[0m       0.5040        \u001b[35m0.5240\u001b[0m  0.2406\n",
      "      8        \u001b[36m0.4905\u001b[0m       0.5040        \u001b[35m0.5017\u001b[0m  0.3047\n",
      "      9        \u001b[36m0.4714\u001b[0m       0.5040        \u001b[35m0.4828\u001b[0m  0.2622\n",
      "     10        \u001b[36m0.4550\u001b[0m       0.5040        \u001b[35m0.4665\u001b[0m  0.2409\n",
      "     11        \u001b[36m0.4402\u001b[0m       0.5040        \u001b[35m0.4519\u001b[0m  0.2367\n",
      "     12        \u001b[36m0.4269\u001b[0m       0.5040        \u001b[35m0.4388\u001b[0m  0.2299\n",
      "     13        \u001b[36m0.4148\u001b[0m       0.5040        \u001b[35m0.4271\u001b[0m  0.2348\n",
      "     14        \u001b[36m0.4035\u001b[0m       0.5040        \u001b[35m0.4152\u001b[0m  0.2404\n",
      "     15        \u001b[36m0.3931\u001b[0m       0.5040        \u001b[35m0.4055\u001b[0m  0.2273\n",
      "     16        \u001b[36m0.3837\u001b[0m       0.5040        \u001b[35m0.3963\u001b[0m  0.2266\n",
      "     17        \u001b[36m0.3749\u001b[0m       0.5040        \u001b[35m0.3878\u001b[0m  0.2273\n",
      "     18        \u001b[36m0.3667\u001b[0m       0.5040        \u001b[35m0.3790\u001b[0m  0.3005\n",
      "     19        \u001b[36m0.3584\u001b[0m       0.5040        \u001b[35m0.3717\u001b[0m  0.3062\n",
      "     20        \u001b[36m0.3514\u001b[0m       0.5040        \u001b[35m0.3653\u001b[0m  0.2450\n",
      "     21        \u001b[36m0.3451\u001b[0m       0.5040        \u001b[35m0.3590\u001b[0m  0.2331\n",
      "     22        \u001b[36m0.3392\u001b[0m       0.5040        \u001b[35m0.3530\u001b[0m  0.2390\n",
      "     23        \u001b[36m0.3337\u001b[0m       0.5040        \u001b[35m0.3474\u001b[0m  0.2385\n",
      "     24        \u001b[36m0.3285\u001b[0m       0.5040        \u001b[35m0.3420\u001b[0m  0.2313\n",
      "     25        \u001b[36m0.3236\u001b[0m       0.5040        \u001b[35m0.3370\u001b[0m  0.2323\n",
      "     26        \u001b[36m0.3189\u001b[0m       0.5040        \u001b[35m0.3323\u001b[0m  0.2354\n",
      "     27        \u001b[36m0.3145\u001b[0m       0.5040        \u001b[35m0.3278\u001b[0m  0.2299\n",
      "     28        \u001b[36m0.3103\u001b[0m       0.5040        \u001b[35m0.3236\u001b[0m  0.2345\n",
      "     29        \u001b[36m0.3063\u001b[0m       0.5040        \u001b[35m0.3196\u001b[0m  0.2317\n",
      "     30        \u001b[36m0.3025\u001b[0m       0.5040        \u001b[35m0.2982\u001b[0m  0.2289\n",
      "     31        \u001b[36m0.2988\u001b[0m       0.5040        \u001b[35m0.2946\u001b[0m  0.2299\n",
      "     32        \u001b[36m0.2954\u001b[0m       0.5040        \u001b[35m0.2910\u001b[0m  0.2270\n",
      "     33        \u001b[36m0.2921\u001b[0m       0.5040        \u001b[35m0.2875\u001b[0m  0.2291\n",
      "     34        \u001b[36m0.2889\u001b[0m       0.5040        \u001b[35m0.2842\u001b[0m  0.2355\n",
      "     35        \u001b[36m0.2859\u001b[0m       0.5040        \u001b[35m0.2811\u001b[0m  0.2296\n",
      "     36        \u001b[36m0.2830\u001b[0m       0.5040        \u001b[35m0.2781\u001b[0m  0.2265\n",
      "     37        \u001b[36m0.2802\u001b[0m       0.5040        \u001b[35m0.2754\u001b[0m  0.2326\n",
      "     38        \u001b[36m0.2775\u001b[0m       0.5040        \u001b[35m0.2728\u001b[0m  0.2370\n",
      "     39        \u001b[36m0.2750\u001b[0m       0.5040        \u001b[35m0.2703\u001b[0m  0.2715\n",
      "     40        \u001b[36m0.2725\u001b[0m       0.5040        \u001b[35m0.2679\u001b[0m  0.2344\n",
      "     41        \u001b[36m0.2701\u001b[0m       0.5040        \u001b[35m0.2654\u001b[0m  0.2333\n",
      "     42        \u001b[36m0.2678\u001b[0m       0.5040        \u001b[35m0.2626\u001b[0m  0.2335\n",
      "     43        \u001b[36m0.2655\u001b[0m       0.5040        \u001b[35m0.2605\u001b[0m  0.2298\n",
      "     44        \u001b[36m0.2633\u001b[0m       0.5040        \u001b[35m0.2582\u001b[0m  0.2289\n",
      "     45        \u001b[36m0.2611\u001b[0m       0.5040        \u001b[35m0.2561\u001b[0m  0.2283\n",
      "     46        \u001b[36m0.2590\u001b[0m       0.5040        \u001b[35m0.2540\u001b[0m  0.2305\n",
      "     47        \u001b[36m0.2570\u001b[0m       0.5040        \u001b[35m0.2519\u001b[0m  0.2287\n",
      "     48        \u001b[36m0.2551\u001b[0m       0.5040        \u001b[35m0.2499\u001b[0m  0.2334\n",
      "     49        \u001b[36m0.2533\u001b[0m       0.5040        \u001b[35m0.2480\u001b[0m  0.2241\n",
      "     50        \u001b[36m0.2515\u001b[0m       0.5040        \u001b[35m0.2462\u001b[0m  0.2290\n",
      "Xtrain pre augmentation: (13031, 6)\n",
      "Ytrain pre augmentation (13031, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain: (24114, 6)\n",
      "Ytrain (24114, 1)\n",
      "Xtest (3258, 6)\n",
      "Ytest (3258, 1)\n",
      "Re-initializing module.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6187\u001b[0m       \u001b[32m0.5030\u001b[0m        \u001b[35m0.5920\u001b[0m  0.2295\n",
      "      2        \u001b[36m0.5458\u001b[0m       0.5030        \u001b[35m0.5196\u001b[0m  0.2457\n",
      "      3        \u001b[36m0.5100\u001b[0m       0.5030        \u001b[35m0.4886\u001b[0m  0.2390\n",
      "      4        \u001b[36m0.4827\u001b[0m       0.5030        \u001b[35m0.4640\u001b[0m  0.2371\n",
      "      5        \u001b[36m0.4598\u001b[0m       0.5030        \u001b[35m0.4427\u001b[0m  0.2389\n",
      "      6        \u001b[36m0.4399\u001b[0m       0.5030        \u001b[35m0.4239\u001b[0m  0.2449\n",
      "      7        \u001b[36m0.4222\u001b[0m       0.5030        \u001b[35m0.4059\u001b[0m  0.2462\n",
      "      8        \u001b[36m0.4056\u001b[0m       0.5030        \u001b[35m0.3895\u001b[0m  0.2417\n",
      "      9        \u001b[36m0.3903\u001b[0m       0.5030        \u001b[35m0.3756\u001b[0m  0.2398\n",
      "     10        \u001b[36m0.3766\u001b[0m       0.5030        \u001b[35m0.3631\u001b[0m  0.2486\n",
      "     11        \u001b[36m0.3645\u001b[0m       0.5030        \u001b[35m0.3522\u001b[0m  0.2450\n",
      "     12        \u001b[36m0.3536\u001b[0m       0.5030        \u001b[35m0.3422\u001b[0m  0.2837\n",
      "     13        \u001b[36m0.3438\u001b[0m       0.5030        \u001b[35m0.3332\u001b[0m  0.2514\n",
      "     14        \u001b[36m0.3350\u001b[0m       0.5030        \u001b[35m0.3250\u001b[0m  0.2633\n",
      "     15        \u001b[36m0.3270\u001b[0m       0.5030        \u001b[35m0.3176\u001b[0m  0.2635\n",
      "     16        \u001b[36m0.3198\u001b[0m       0.5030        \u001b[35m0.3108\u001b[0m  0.2496\n",
      "     17        \u001b[36m0.3132\u001b[0m       0.5030        \u001b[35m0.3046\u001b[0m  0.2561\n",
      "     18        \u001b[36m0.3072\u001b[0m       0.5030        \u001b[35m0.2989\u001b[0m  0.2580\n",
      "     19        \u001b[36m0.3017\u001b[0m       0.5030        \u001b[35m0.2936\u001b[0m  0.2468\n",
      "     20        \u001b[36m0.2966\u001b[0m       0.5030        \u001b[35m0.2888\u001b[0m  0.2517\n",
      "     21        \u001b[36m0.2919\u001b[0m       0.5030        \u001b[35m0.2843\u001b[0m  0.2501\n",
      "     22        \u001b[36m0.2876\u001b[0m       0.5030        \u001b[35m0.2802\u001b[0m  0.2445\n",
      "     23        \u001b[36m0.2836\u001b[0m       0.5030        \u001b[35m0.2764\u001b[0m  0.2533\n",
      "     24        \u001b[36m0.2799\u001b[0m       0.5030        \u001b[35m0.2725\u001b[0m  0.2474\n",
      "     25        \u001b[36m0.2764\u001b[0m       0.5030        \u001b[35m0.2692\u001b[0m  0.2476\n",
      "     26        \u001b[36m0.2732\u001b[0m       0.5030        \u001b[35m0.2661\u001b[0m  0.2826\n",
      "     27        \u001b[36m0.2702\u001b[0m       0.5030        \u001b[35m0.2632\u001b[0m  0.2472\n",
      "     28        \u001b[36m0.2674\u001b[0m       0.5030        \u001b[35m0.2601\u001b[0m  0.2521\n",
      "     29        \u001b[36m0.2648\u001b[0m       0.5030        \u001b[35m0.2576\u001b[0m  0.2456\n",
      "     30        \u001b[36m0.2623\u001b[0m       0.5030        \u001b[35m0.2552\u001b[0m  0.2434\n",
      "     31        \u001b[36m0.2599\u001b[0m       0.5030        \u001b[35m0.2528\u001b[0m  0.2442\n",
      "     32        \u001b[36m0.2577\u001b[0m       0.5030        \u001b[35m0.2502\u001b[0m  0.2400\n",
      "     33        \u001b[36m0.2555\u001b[0m       0.5030        \u001b[35m0.2483\u001b[0m  0.2439\n",
      "     34        \u001b[36m0.2534\u001b[0m       0.5030        \u001b[35m0.2465\u001b[0m  0.2422\n",
      "     35        \u001b[36m0.2515\u001b[0m       0.5030        \u001b[35m0.2447\u001b[0m  0.2435\n",
      "     36        \u001b[36m0.2498\u001b[0m       0.5030        \u001b[35m0.2430\u001b[0m  0.2468\n",
      "     37        \u001b[36m0.2481\u001b[0m       0.5030        \u001b[35m0.2413\u001b[0m  0.2548\n",
      "     38        \u001b[36m0.2465\u001b[0m       0.5030        \u001b[35m0.2398\u001b[0m  0.2466\n",
      "     39        \u001b[36m0.2449\u001b[0m       0.5030        \u001b[35m0.2383\u001b[0m  0.2482\n",
      "     40        \u001b[36m0.2435\u001b[0m       0.5030        \u001b[35m0.2369\u001b[0m  0.2510\n",
      "     41        \u001b[36m0.2421\u001b[0m       0.5030        \u001b[35m0.2355\u001b[0m  0.2422\n",
      "     42        \u001b[36m0.2408\u001b[0m       0.5030        \u001b[35m0.2343\u001b[0m  0.2474\n",
      "     43        \u001b[36m0.2396\u001b[0m       0.5030        \u001b[35m0.2331\u001b[0m  0.2463\n",
      "     44        \u001b[36m0.2384\u001b[0m       0.5030        \u001b[35m0.2319\u001b[0m  0.2421\n",
      "     45        \u001b[36m0.2372\u001b[0m       0.5030        \u001b[35m0.2307\u001b[0m  0.2451\n",
      "     46        \u001b[36m0.2362\u001b[0m       0.5030        \u001b[35m0.2296\u001b[0m  0.2479\n",
      "     47        \u001b[36m0.2351\u001b[0m       0.5030        \u001b[35m0.2286\u001b[0m  0.2522\n",
      "     48        \u001b[36m0.2341\u001b[0m       0.5030        \u001b[35m0.2276\u001b[0m  0.2412\n",
      "     49        \u001b[36m0.2332\u001b[0m       0.5030        \u001b[35m0.2266\u001b[0m  0.2576\n",
      "     50        \u001b[36m0.2322\u001b[0m       0.5030        \u001b[35m0.2257\u001b[0m  0.2440\n",
      "Xtrain pre augmentation: (13031, 6)\n",
      "Ytrain pre augmentation (13031, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain: (23919, 6)\n",
      "Ytrain (23919, 1)\n",
      "Xtest (3258, 6)\n",
      "Ytest (3258, 1)\n",
      "Re-initializing module.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6363\u001b[0m       \u001b[32m0.5059\u001b[0m        \u001b[35m0.6273\u001b[0m  0.2464\n",
      "      2        \u001b[36m0.6186\u001b[0m       0.5059        \u001b[35m0.6092\u001b[0m  0.2499\n",
      "      3        \u001b[36m0.6095\u001b[0m       0.5059        \u001b[35m0.6014\u001b[0m  0.2460\n",
      "      4        \u001b[36m0.6007\u001b[0m       0.5059        \u001b[35m0.5930\u001b[0m  0.2476\n",
      "      5        \u001b[36m0.5916\u001b[0m       0.5059        \u001b[35m0.5841\u001b[0m  0.2447\n",
      "      6        \u001b[36m0.5820\u001b[0m       0.5059        \u001b[35m0.5751\u001b[0m  0.2466\n",
      "      7        \u001b[36m0.5719\u001b[0m       0.5059        \u001b[35m0.5658\u001b[0m  0.2472\n",
      "      8        \u001b[36m0.5614\u001b[0m       0.5059        \u001b[35m0.5562\u001b[0m  0.2504\n",
      "      9        \u001b[36m0.5505\u001b[0m       0.5059        \u001b[35m0.5470\u001b[0m  0.2519\n",
      "     10        \u001b[36m0.5391\u001b[0m       0.5059        \u001b[35m0.5373\u001b[0m  0.2513\n",
      "     11        \u001b[36m0.5273\u001b[0m       0.5059        \u001b[35m0.5272\u001b[0m  0.2612\n",
      "     12        \u001b[36m0.5148\u001b[0m       0.5059        \u001b[35m0.5168\u001b[0m  0.2479\n",
      "     13        \u001b[36m0.5020\u001b[0m       0.5059        \u001b[35m0.5032\u001b[0m  0.2990\n",
      "     14        \u001b[36m0.4893\u001b[0m       0.5059        \u001b[35m0.4897\u001b[0m  0.2540\n",
      "     15        \u001b[36m0.4765\u001b[0m       0.5059        \u001b[35m0.4768\u001b[0m  0.2476\n",
      "     16        \u001b[36m0.4628\u001b[0m       0.5059        \u001b[35m0.4641\u001b[0m  0.2475\n",
      "     17        \u001b[36m0.4472\u001b[0m       0.5059        \u001b[35m0.4413\u001b[0m  0.2516\n",
      "     18        \u001b[36m0.4303\u001b[0m       0.5059        \u001b[35m0.4252\u001b[0m  0.2510\n",
      "     19        \u001b[36m0.4162\u001b[0m       0.5059        \u001b[35m0.4183\u001b[0m  0.2499\n",
      "     20        \u001b[36m0.4046\u001b[0m       0.5059        \u001b[35m0.4052\u001b[0m  0.2513\n",
      "     21        \u001b[36m0.3943\u001b[0m       0.5059        \u001b[35m0.3954\u001b[0m  0.2445\n",
      "     22        \u001b[36m0.3849\u001b[0m       0.5059        \u001b[35m0.3864\u001b[0m  0.2410\n",
      "     23        \u001b[36m0.3761\u001b[0m       0.5059        \u001b[35m0.3786\u001b[0m  0.2398\n",
      "     24        \u001b[36m0.3679\u001b[0m       0.5059        \u001b[35m0.3712\u001b[0m  0.2390\n",
      "     25        \u001b[36m0.3602\u001b[0m       0.5059        \u001b[35m0.3638\u001b[0m  0.2450\n",
      "     26        \u001b[36m0.3529\u001b[0m       0.5059        \u001b[35m0.3551\u001b[0m  0.2421\n",
      "     27        \u001b[36m0.3460\u001b[0m       0.5059        \u001b[35m0.3483\u001b[0m  0.2460\n",
      "     28        \u001b[36m0.3395\u001b[0m       0.5059        \u001b[35m0.3422\u001b[0m  0.2401\n",
      "     29        \u001b[36m0.3333\u001b[0m       0.5059        \u001b[35m0.3365\u001b[0m  0.2419\n",
      "     30        \u001b[36m0.3274\u001b[0m       0.5059        \u001b[35m0.3295\u001b[0m  0.2390\n",
      "     31        \u001b[36m0.3218\u001b[0m       0.5059        \u001b[35m0.3242\u001b[0m  0.2458\n",
      "     32        \u001b[36m0.3165\u001b[0m       0.5059        \u001b[35m0.3196\u001b[0m  0.2438\n",
      "     33        \u001b[36m0.3115\u001b[0m       0.5059        \u001b[35m0.3147\u001b[0m  0.2406\n",
      "     34        \u001b[36m0.3067\u001b[0m       0.5059        \u001b[35m0.3098\u001b[0m  0.2437\n",
      "     35        \u001b[36m0.3021\u001b[0m       0.5059        \u001b[35m0.3049\u001b[0m  0.2413\n",
      "     36        \u001b[36m0.2978\u001b[0m       0.5059        \u001b[35m0.3004\u001b[0m  0.2485\n",
      "     37        \u001b[36m0.2936\u001b[0m       0.5059        \u001b[35m0.2959\u001b[0m  0.2418\n",
      "     38        \u001b[36m0.2896\u001b[0m       0.5059        \u001b[35m0.2918\u001b[0m  0.2401\n",
      "     39        \u001b[36m0.2858\u001b[0m       0.5059        \u001b[35m0.2875\u001b[0m  0.2405\n",
      "     40        \u001b[36m0.2822\u001b[0m       0.5059        \u001b[35m0.2841\u001b[0m  0.2488\n",
      "     41        \u001b[36m0.2788\u001b[0m       0.5059        \u001b[35m0.2807\u001b[0m  0.2433\n",
      "     42        \u001b[36m0.2755\u001b[0m       0.5059        \u001b[35m0.2773\u001b[0m  0.2470\n",
      "     43        \u001b[36m0.2724\u001b[0m       0.5059        \u001b[35m0.2742\u001b[0m  0.2376\n",
      "     44        \u001b[36m0.2694\u001b[0m       0.5059        \u001b[35m0.2713\u001b[0m  0.2466\n",
      "     45        \u001b[36m0.2666\u001b[0m       0.5059        \u001b[35m0.2682\u001b[0m  0.2454\n",
      "     46        \u001b[36m0.2639\u001b[0m       0.5059        \u001b[35m0.2655\u001b[0m  0.2400\n",
      "     47        \u001b[36m0.2612\u001b[0m       0.5059        \u001b[35m0.2628\u001b[0m  0.2420\n",
      "     48        \u001b[36m0.2587\u001b[0m       0.5059        \u001b[35m0.2603\u001b[0m  0.2431\n",
      "     49        \u001b[36m0.2563\u001b[0m       0.5059        \u001b[35m0.2580\u001b[0m  0.2410\n",
      "     50        \u001b[36m0.2539\u001b[0m       0.5059        \u001b[35m0.2556\u001b[0m  0.2427\n",
      "Xtrain pre augmentation: (13031, 6)\n",
      "Ytrain pre augmentation (13031, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain: (23998, 6)\n",
      "Ytrain (23998, 1)\n",
      "Xtest (3258, 6)\n",
      "Ytest (3258, 1)\n",
      "Re-initializing module.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7283\u001b[0m       \u001b[32m0.5042\u001b[0m        \u001b[35m0.6646\u001b[0m  0.2562\n",
      "      2        \u001b[36m0.6577\u001b[0m       0.5042        \u001b[35m0.6305\u001b[0m  0.2485\n",
      "      3        \u001b[36m0.6125\u001b[0m       0.5042        \u001b[35m0.5991\u001b[0m  0.2496\n",
      "      4        \u001b[36m0.5747\u001b[0m       0.5042        \u001b[35m0.5816\u001b[0m  0.2511\n",
      "      5        \u001b[36m0.5423\u001b[0m       0.5042        \u001b[35m0.5530\u001b[0m  0.2487\n",
      "      6        \u001b[36m0.5164\u001b[0m       0.5042        \u001b[35m0.5293\u001b[0m  0.2485\n",
      "      7        \u001b[36m0.4945\u001b[0m       0.5042        \u001b[35m0.5091\u001b[0m  0.2491\n",
      "      8        \u001b[36m0.4756\u001b[0m       0.5042        \u001b[35m0.4911\u001b[0m  0.2447\n",
      "      9        \u001b[36m0.4589\u001b[0m       0.5042        \u001b[35m0.4576\u001b[0m  0.2461\n",
      "     10        \u001b[36m0.4439\u001b[0m       0.5042        \u001b[35m0.4429\u001b[0m  0.2545\n",
      "     11        \u001b[36m0.4303\u001b[0m       0.5042        \u001b[35m0.4293\u001b[0m  0.2667\n",
      "     12        \u001b[36m0.4179\u001b[0m       0.5042        \u001b[35m0.4171\u001b[0m  0.2558\n",
      "     13        \u001b[36m0.4065\u001b[0m       0.5042        \u001b[35m0.4057\u001b[0m  0.2540\n",
      "     14        \u001b[36m0.3960\u001b[0m       0.5042        \u001b[35m0.3952\u001b[0m  0.2652\n",
      "     15        \u001b[36m0.3863\u001b[0m       0.5042        \u001b[35m0.3851\u001b[0m  0.2570\n",
      "     16        \u001b[36m0.3772\u001b[0m       0.5042        \u001b[35m0.3757\u001b[0m  0.2547\n",
      "     17        \u001b[36m0.3686\u001b[0m       0.5042        \u001b[35m0.3667\u001b[0m  0.2716\n",
      "     18        \u001b[36m0.3606\u001b[0m       0.5042        \u001b[35m0.3582\u001b[0m  0.2600\n",
      "     19        \u001b[36m0.3531\u001b[0m       0.5042        \u001b[35m0.3502\u001b[0m  0.2551\n",
      "     20        \u001b[36m0.3461\u001b[0m       0.5042        \u001b[35m0.3427\u001b[0m  0.2522\n",
      "     21        \u001b[36m0.3395\u001b[0m       0.5042        \u001b[35m0.3357\u001b[0m  0.2583\n",
      "     22        \u001b[36m0.3333\u001b[0m       0.5042        \u001b[35m0.3291\u001b[0m  0.2522\n",
      "     23        \u001b[36m0.3274\u001b[0m       0.5042        \u001b[35m0.3230\u001b[0m  0.2615\n",
      "     24        \u001b[36m0.3219\u001b[0m       0.5042        \u001b[35m0.3168\u001b[0m  0.2533\n",
      "     25        \u001b[36m0.3168\u001b[0m       0.5042        \u001b[35m0.3113\u001b[0m  0.2817\n",
      "     26        \u001b[36m0.3120\u001b[0m       0.5042        \u001b[35m0.3063\u001b[0m  0.3698\n",
      "     27        \u001b[36m0.3075\u001b[0m       0.5042        \u001b[35m0.3016\u001b[0m  0.4103\n",
      "     28        \u001b[36m0.3033\u001b[0m       0.5042        \u001b[35m0.2969\u001b[0m  0.4548\n",
      "     29        \u001b[36m0.2993\u001b[0m       0.5042        \u001b[35m0.2925\u001b[0m  0.3097\n",
      "     30        \u001b[36m0.2956\u001b[0m       0.5042        \u001b[35m0.2885\u001b[0m  0.3246\n",
      "     31        \u001b[36m0.2921\u001b[0m       0.5042        \u001b[35m0.2847\u001b[0m  0.3160\n",
      "     32        \u001b[36m0.2887\u001b[0m       0.5042        \u001b[35m0.2810\u001b[0m  0.3253\n",
      "     33        \u001b[36m0.2855\u001b[0m       0.5042        \u001b[35m0.2775\u001b[0m  0.3734\n",
      "     34        \u001b[36m0.2824\u001b[0m       0.5042        \u001b[35m0.2745\u001b[0m  0.3097\n",
      "     35        \u001b[36m0.2793\u001b[0m       0.5042        \u001b[35m0.2706\u001b[0m  0.2833\n",
      "     36        \u001b[36m0.2764\u001b[0m       0.5042        \u001b[35m0.2675\u001b[0m  0.3747\n",
      "     37        \u001b[36m0.2736\u001b[0m       0.5042        \u001b[35m0.2646\u001b[0m  0.4585\n",
      "     38        \u001b[36m0.2709\u001b[0m       0.5042        \u001b[35m0.2616\u001b[0m  0.3601\n",
      "     39        \u001b[36m0.2684\u001b[0m       0.5042        \u001b[35m0.2595\u001b[0m  0.3837\n",
      "     40        \u001b[36m0.2656\u001b[0m       0.5042        \u001b[35m0.2563\u001b[0m  0.5562\n",
      "     41        \u001b[36m0.2633\u001b[0m       0.5042        \u001b[35m0.2539\u001b[0m  0.5030\n",
      "     42        \u001b[36m0.2611\u001b[0m       0.5042        \u001b[35m0.2517\u001b[0m  0.3998\n",
      "     43        \u001b[36m0.2592\u001b[0m       0.5042        \u001b[35m0.2496\u001b[0m  0.4093\n",
      "     44        \u001b[36m0.2574\u001b[0m       0.5042        \u001b[35m0.2477\u001b[0m  0.3999\n",
      "     45        \u001b[36m0.2557\u001b[0m       0.5042        \u001b[35m0.2459\u001b[0m  0.3752\n",
      "     46        \u001b[36m0.2541\u001b[0m       0.5042        \u001b[35m0.2442\u001b[0m  0.3498\n",
      "     47        \u001b[36m0.2525\u001b[0m       0.5042        \u001b[35m0.2427\u001b[0m  0.3299\n",
      "     48        \u001b[36m0.2511\u001b[0m       0.5042        \u001b[35m0.2412\u001b[0m  0.3949\n",
      "     49        \u001b[36m0.2497\u001b[0m       0.5042        \u001b[35m0.2399\u001b[0m  0.3206\n",
      "     50        \u001b[36m0.2484\u001b[0m       0.5042        \u001b[35m0.2387\u001b[0m  0.6231\n",
      "Xtrain pre augmentation: (13032, 6)\n",
      "Ytrain pre augmentation (13032, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain: (23992, 6)\n",
      "Ytrain (23992, 1)\n",
      "Xtest (3257, 6)\n",
      "Ytest (3257, 1)\n",
      "Re-initializing module.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6110\u001b[0m       \u001b[32m0.5039\u001b[0m        \u001b[35m0.5885\u001b[0m  0.2699\n",
      "      2        \u001b[36m0.5636\u001b[0m       0.5039        \u001b[35m0.5371\u001b[0m  0.2891\n",
      "      3        \u001b[36m0.5347\u001b[0m       0.5039        \u001b[35m0.5147\u001b[0m  0.3080\n",
      "      4        \u001b[36m0.5135\u001b[0m       0.5039        \u001b[35m0.4961\u001b[0m  0.2697\n",
      "      5        \u001b[36m0.4963\u001b[0m       0.5039        \u001b[35m0.4809\u001b[0m  0.2966\n",
      "      6        \u001b[36m0.4814\u001b[0m       0.5039        \u001b[35m0.4671\u001b[0m  0.3000\n",
      "      7        \u001b[36m0.4675\u001b[0m       0.5039        \u001b[35m0.4538\u001b[0m  0.2603\n",
      "      8        \u001b[36m0.4545\u001b[0m       0.5039        \u001b[35m0.4413\u001b[0m  0.2737\n",
      "      9        \u001b[36m0.4423\u001b[0m       0.5039        \u001b[35m0.4294\u001b[0m  0.3384\n",
      "     10        \u001b[36m0.4307\u001b[0m       0.5039        \u001b[35m0.4183\u001b[0m  0.2932\n",
      "     11        \u001b[36m0.4199\u001b[0m       0.5039        \u001b[35m0.4079\u001b[0m  0.2822\n",
      "     12        \u001b[36m0.4092\u001b[0m       0.5039        \u001b[35m0.3982\u001b[0m  0.2698\n",
      "     13        \u001b[36m0.3991\u001b[0m       0.5039        \u001b[35m0.3889\u001b[0m  0.3141\n",
      "     14        \u001b[36m0.3901\u001b[0m       0.5039        \u001b[35m0.3799\u001b[0m  0.2743\n",
      "     15        \u001b[36m0.3818\u001b[0m       0.5039        \u001b[35m0.3717\u001b[0m  0.2901\n",
      "     16        \u001b[36m0.3742\u001b[0m       0.5039        \u001b[35m0.3642\u001b[0m  0.2824\n",
      "     17        \u001b[36m0.3672\u001b[0m       0.5039        \u001b[35m0.3573\u001b[0m  0.2692\n",
      "     18        \u001b[36m0.3607\u001b[0m       0.5039        \u001b[35m0.3509\u001b[0m  0.2891\n",
      "     19        \u001b[36m0.3546\u001b[0m       0.5039        \u001b[35m0.3450\u001b[0m  0.3043\n",
      "     20        \u001b[36m0.3489\u001b[0m       0.5039        \u001b[35m0.3393\u001b[0m  0.3127\n",
      "     21        \u001b[36m0.3434\u001b[0m       0.5039        \u001b[35m0.3341\u001b[0m  0.2864\n",
      "     22        \u001b[36m0.3381\u001b[0m       0.5039        \u001b[35m0.3290\u001b[0m  0.2639\n",
      "     23        \u001b[36m0.3331\u001b[0m       0.5039        \u001b[35m0.3242\u001b[0m  0.2744\n",
      "     24        \u001b[36m0.3284\u001b[0m       0.5039        \u001b[35m0.3197\u001b[0m  0.2642\n",
      "     25        \u001b[36m0.3239\u001b[0m       0.5039        \u001b[35m0.3152\u001b[0m  0.2598\n",
      "     26        \u001b[36m0.3195\u001b[0m       0.5039        \u001b[35m0.3110\u001b[0m  0.2599\n",
      "     27        \u001b[36m0.3152\u001b[0m       0.5039        \u001b[35m0.3069\u001b[0m  0.2630\n",
      "     28        \u001b[36m0.3112\u001b[0m       0.5039        \u001b[35m0.3030\u001b[0m  0.2583\n",
      "     29        \u001b[36m0.3074\u001b[0m       0.5039        \u001b[35m0.2992\u001b[0m  0.2602\n",
      "     30        \u001b[36m0.3039\u001b[0m       0.5039        \u001b[35m0.2956\u001b[0m  0.2618\n",
      "     31        \u001b[36m0.3005\u001b[0m       0.5039        \u001b[35m0.2922\u001b[0m  0.2748\n",
      "     32        \u001b[36m0.2972\u001b[0m       0.5039        \u001b[35m0.2889\u001b[0m  0.2815\n",
      "     33        \u001b[36m0.2941\u001b[0m       0.5039        \u001b[35m0.2858\u001b[0m  0.2698\n",
      "     34        \u001b[36m0.2911\u001b[0m       0.5039        \u001b[35m0.2828\u001b[0m  0.2510\n",
      "     35        \u001b[36m0.2882\u001b[0m       0.5039        \u001b[35m0.2800\u001b[0m  0.2664\n",
      "     36        \u001b[36m0.2854\u001b[0m       0.5039        \u001b[35m0.2773\u001b[0m  0.2560\n",
      "     37        \u001b[36m0.2827\u001b[0m       0.5039        \u001b[35m0.2747\u001b[0m  0.2500\n",
      "     38        \u001b[36m0.2801\u001b[0m       0.5039        \u001b[35m0.2723\u001b[0m  0.2514\n",
      "     39        \u001b[36m0.2776\u001b[0m       0.5039        \u001b[35m0.2699\u001b[0m  0.2519\n",
      "     40        \u001b[36m0.2753\u001b[0m       0.5039        \u001b[35m0.2676\u001b[0m  0.2554\n",
      "     41        \u001b[36m0.2730\u001b[0m       0.5039        \u001b[35m0.2654\u001b[0m  0.7548\n",
      "     42        \u001b[36m0.2707\u001b[0m       0.5039        \u001b[35m0.2633\u001b[0m  0.2507\n",
      "     43        \u001b[36m0.2686\u001b[0m       0.5039        \u001b[35m0.2613\u001b[0m  0.2485\n",
      "     44        \u001b[36m0.2665\u001b[0m       0.5039        \u001b[35m0.2593\u001b[0m  0.2586\n",
      "     45        \u001b[36m0.2645\u001b[0m       0.5039        \u001b[35m0.2574\u001b[0m  0.2584\n",
      "     46        \u001b[36m0.2625\u001b[0m       0.5039        \u001b[35m0.2555\u001b[0m  0.2520\n",
      "     47        \u001b[36m0.2606\u001b[0m       0.5039        \u001b[35m0.2534\u001b[0m  0.3001\n",
      "     48        \u001b[36m0.2586\u001b[0m       0.5039        \u001b[35m0.2523\u001b[0m  0.2587\n",
      "     49        \u001b[36m0.2566\u001b[0m       0.5039        \u001b[35m0.2506\u001b[0m  0.2543\n",
      "     50        \u001b[36m0.2548\u001b[0m       0.5039        \u001b[35m0.2491\u001b[0m  0.2640\n",
      "Xtrain pre augmentation: (2229, 5)\n",
      "Ytrain pre augmentation (2229, 1)\n",
      "Xtrain: (3178, 5)\n",
      "Ytrain (3178, 1)\n",
      "Xtest (558, 5)\n",
      "Ytest (558, 1)\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.8629\u001b[0m       \u001b[32m0.5126\u001b[0m        \u001b[35m0.7379\u001b[0m  0.0354\n",
      "      2        \u001b[36m0.8403\u001b[0m       0.5126        0.7463  0.0402\n",
      "      3        \u001b[36m0.8217\u001b[0m       0.5126        0.7527  0.0446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      4        \u001b[36m0.8062\u001b[0m       0.5126        0.7577  0.0444\n",
      "      5        \u001b[36m0.7926\u001b[0m       0.5126        0.7601  0.0414\n",
      "      6        \u001b[36m0.7803\u001b[0m       0.5126        0.7595  0.0454\n",
      "      7        \u001b[36m0.7687\u001b[0m       0.5126        0.7549  0.0470\n",
      "      8        \u001b[36m0.7570\u001b[0m       0.5126        0.7474  0.0496\n",
      "      9        \u001b[36m0.7452\u001b[0m       0.5126        0.7391  0.0452\n",
      "     10        \u001b[36m0.7337\u001b[0m       0.5126        \u001b[35m0.7310\u001b[0m  0.0504\n",
      "     11        \u001b[36m0.7237\u001b[0m       0.5126        \u001b[35m0.7230\u001b[0m  0.0439\n",
      "     12        \u001b[36m0.7145\u001b[0m       0.5126        \u001b[35m0.7153\u001b[0m  0.0466\n",
      "     13        \u001b[36m0.7061\u001b[0m       0.5126        \u001b[35m0.7082\u001b[0m  0.0484\n",
      "     14        \u001b[36m0.6983\u001b[0m       0.5126        \u001b[35m0.7015\u001b[0m  0.0445\n",
      "     15        \u001b[36m0.6912\u001b[0m       0.5126        \u001b[35m0.6953\u001b[0m  0.0491\n",
      "     16        \u001b[36m0.6846\u001b[0m       0.5126        \u001b[35m0.6891\u001b[0m  0.0473\n",
      "     17        \u001b[36m0.6783\u001b[0m       0.5126        \u001b[35m0.6830\u001b[0m  0.0435\n",
      "     18        \u001b[36m0.6721\u001b[0m       0.5126        \u001b[35m0.6770\u001b[0m  0.0443\n",
      "     19        \u001b[36m0.6661\u001b[0m       0.5126        \u001b[35m0.6712\u001b[0m  0.0492\n",
      "     20        \u001b[36m0.6603\u001b[0m       0.5126        \u001b[35m0.6655\u001b[0m  0.0488\n",
      "     21        \u001b[36m0.6546\u001b[0m       0.5126        \u001b[35m0.6600\u001b[0m  0.0442\n",
      "     22        \u001b[36m0.6491\u001b[0m       0.5126        \u001b[35m0.6547\u001b[0m  0.0554\n",
      "     23        \u001b[36m0.6437\u001b[0m       0.5126        \u001b[35m0.6496\u001b[0m  0.0481\n",
      "     24        \u001b[36m0.6385\u001b[0m       0.5126        \u001b[35m0.6446\u001b[0m  0.0493\n",
      "     25        \u001b[36m0.6334\u001b[0m       0.5126        \u001b[35m0.6396\u001b[0m  0.0473\n",
      "     26        \u001b[36m0.6283\u001b[0m       0.5126        \u001b[35m0.6347\u001b[0m  0.0448\n",
      "     27        \u001b[36m0.6234\u001b[0m       0.5126        \u001b[35m0.6299\u001b[0m  0.0446\n",
      "     28        \u001b[36m0.6186\u001b[0m       0.5126        \u001b[35m0.6253\u001b[0m  0.0440\n",
      "     29        \u001b[36m0.6139\u001b[0m       0.5126        \u001b[35m0.6207\u001b[0m  0.0548\n",
      "     30        \u001b[36m0.6093\u001b[0m       0.5126        \u001b[35m0.6162\u001b[0m  0.0436\n",
      "     31        \u001b[36m0.6048\u001b[0m       0.5126        \u001b[35m0.6118\u001b[0m  0.0485\n",
      "     32        \u001b[36m0.6004\u001b[0m       0.5126        \u001b[35m0.6075\u001b[0m  0.0454\n",
      "     33        \u001b[36m0.5960\u001b[0m       0.5126        \u001b[35m0.6032\u001b[0m  0.0494\n",
      "     34        \u001b[36m0.5917\u001b[0m       0.5126        \u001b[35m0.5990\u001b[0m  0.0439\n",
      "     35        \u001b[36m0.5875\u001b[0m       0.5126        \u001b[35m0.5950\u001b[0m  0.0456\n",
      "     36        \u001b[36m0.5834\u001b[0m       0.5126        \u001b[35m0.5910\u001b[0m  0.0465\n",
      "     37        \u001b[36m0.5793\u001b[0m       0.5126        \u001b[35m0.5870\u001b[0m  0.0442\n",
      "     38        \u001b[36m0.5752\u001b[0m       0.5126        \u001b[35m0.5832\u001b[0m  0.0446\n",
      "     39        \u001b[36m0.5713\u001b[0m       0.5126        \u001b[35m0.5794\u001b[0m  0.0442\n",
      "     40        \u001b[36m0.5674\u001b[0m       0.5126        \u001b[35m0.5756\u001b[0m  0.1065\n",
      "     41        \u001b[36m0.5636\u001b[0m       0.5126        \u001b[35m0.5719\u001b[0m  0.0462\n",
      "     42        \u001b[36m0.5599\u001b[0m       0.5126        \u001b[35m0.5683\u001b[0m  0.0433\n",
      "     43        \u001b[36m0.5562\u001b[0m       0.5126        \u001b[35m0.5647\u001b[0m  0.0449\n",
      "     44        \u001b[36m0.5526\u001b[0m       0.5126        \u001b[35m0.5612\u001b[0m  0.0497\n",
      "     45        \u001b[36m0.5490\u001b[0m       0.5126        \u001b[35m0.5578\u001b[0m  0.0459\n",
      "     46        \u001b[36m0.5456\u001b[0m       0.5126        \u001b[35m0.5544\u001b[0m  0.0496\n",
      "     47        \u001b[36m0.5421\u001b[0m       0.5126        \u001b[35m0.5511\u001b[0m  0.0448\n",
      "     48        \u001b[36m0.5388\u001b[0m       0.5126        \u001b[35m0.5478\u001b[0m  0.0444\n",
      "     49        \u001b[36m0.5355\u001b[0m       0.5126        \u001b[35m0.5446\u001b[0m  0.0437\n",
      "     50        \u001b[36m0.5322\u001b[0m       0.5126        \u001b[35m0.5414\u001b[0m  0.0434\n",
      "Xtrain pre augmentation: (2229, 5)\n",
      "Ytrain pre augmentation (2229, 1)\n",
      "Xtrain: (3246, 5)\n",
      "Ytrain (3246, 1)\n",
      "Xtest (558, 5)\n",
      "Ytest (558, 1)\n",
      "Re-initializing module.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.8543\u001b[0m       \u001b[32m0.5185\u001b[0m        \u001b[35m0.7665\u001b[0m  0.0438\n",
      "      2        \u001b[36m0.8427\u001b[0m       0.5185        \u001b[35m0.7586\u001b[0m  0.0479\n",
      "      3        \u001b[36m0.8311\u001b[0m       0.5185        0.7667  0.0467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      4        \u001b[36m0.8191\u001b[0m       0.5185        0.7796  0.0548\n",
      "      5        \u001b[36m0.8061\u001b[0m       0.5185        0.7911  0.0510\n",
      "      6        \u001b[36m0.7917\u001b[0m       0.5185        0.7960  0.0546\n",
      "      7        \u001b[36m0.7744\u001b[0m       0.5185        0.7913  0.0525\n",
      "      8        \u001b[36m0.7597\u001b[0m       0.5185        0.7803  0.0493\n",
      "      9        \u001b[36m0.7480\u001b[0m       0.5185        0.7650  0.0469\n",
      "     10        \u001b[36m0.7374\u001b[0m       0.5185        \u001b[35m0.7488\u001b[0m  0.0493\n",
      "     11        \u001b[36m0.7275\u001b[0m       0.5185        \u001b[35m0.7343\u001b[0m  0.0463\n",
      "     12        \u001b[36m0.7179\u001b[0m       0.5185        \u001b[35m0.7222\u001b[0m  0.0479\n",
      "     13        \u001b[36m0.7088\u001b[0m       0.5185        \u001b[35m0.7117\u001b[0m  0.0530\n",
      "     14        \u001b[36m0.6999\u001b[0m       0.5185        \u001b[35m0.7024\u001b[0m  0.0509\n",
      "     15        \u001b[36m0.6913\u001b[0m       0.5185        \u001b[35m0.6938\u001b[0m  0.0489\n",
      "     16        \u001b[36m0.6830\u001b[0m       0.5185        \u001b[35m0.6857\u001b[0m  0.0481\n",
      "     17        \u001b[36m0.6749\u001b[0m       0.5185        \u001b[35m0.6778\u001b[0m  0.0445\n",
      "     18        \u001b[36m0.6670\u001b[0m       0.5185        \u001b[35m0.6702\u001b[0m  0.0499\n",
      "     19        \u001b[36m0.6593\u001b[0m       0.5185        \u001b[35m0.6629\u001b[0m  0.0508\n",
      "     20        \u001b[36m0.6519\u001b[0m       0.5185        \u001b[35m0.6557\u001b[0m  0.0495\n",
      "     21        \u001b[36m0.6446\u001b[0m       0.5185        \u001b[35m0.6484\u001b[0m  0.0544\n",
      "     22        \u001b[36m0.6375\u001b[0m       0.5185        \u001b[35m0.6414\u001b[0m  0.0584\n",
      "     23        \u001b[36m0.6306\u001b[0m       0.5185        \u001b[35m0.6348\u001b[0m  0.0550\n",
      "     24        \u001b[36m0.6239\u001b[0m       0.5185        \u001b[35m0.6283\u001b[0m  0.0511\n",
      "     25        \u001b[36m0.6174\u001b[0m       0.5185        \u001b[35m0.6220\u001b[0m  0.0535\n",
      "     26        \u001b[36m0.6111\u001b[0m       0.5185        \u001b[35m0.6156\u001b[0m  0.0558\n",
      "     27        \u001b[36m0.6049\u001b[0m       0.5185        \u001b[35m0.6093\u001b[0m  0.0461\n",
      "     28        \u001b[36m0.5988\u001b[0m       0.5185        \u001b[35m0.6031\u001b[0m  0.0485\n",
      "     29        \u001b[36m0.5928\u001b[0m       0.5185        \u001b[35m0.5968\u001b[0m  0.0474\n",
      "     30        \u001b[36m0.5870\u001b[0m       0.5185        \u001b[35m0.5907\u001b[0m  0.0673\n",
      "     31        \u001b[36m0.5812\u001b[0m       0.5185        \u001b[35m0.5848\u001b[0m  0.0808\n",
      "     32        \u001b[36m0.5755\u001b[0m       0.5185        \u001b[35m0.5792\u001b[0m  0.0792\n",
      "     33        \u001b[36m0.5700\u001b[0m       0.5185        \u001b[35m0.5736\u001b[0m  0.0828\n",
      "     34        \u001b[36m0.5646\u001b[0m       0.5185        \u001b[35m0.5684\u001b[0m  0.0640\n",
      "     35        \u001b[36m0.5593\u001b[0m       0.5185        \u001b[35m0.5633\u001b[0m  0.0577\n",
      "     36        \u001b[36m0.5542\u001b[0m       0.5185        \u001b[35m0.5584\u001b[0m  0.0715\n",
      "     37        \u001b[36m0.5492\u001b[0m       0.5185        \u001b[35m0.5537\u001b[0m  0.0787\n",
      "     38        \u001b[36m0.5445\u001b[0m       0.5185        \u001b[35m0.5492\u001b[0m  0.0533\n",
      "     39        \u001b[36m0.5398\u001b[0m       0.5185        \u001b[35m0.5448\u001b[0m  0.0639\n",
      "     40        \u001b[36m0.5353\u001b[0m       0.5185        \u001b[35m0.5405\u001b[0m  0.0600\n",
      "     41        \u001b[36m0.5309\u001b[0m       0.5185        \u001b[35m0.5363\u001b[0m  0.0561\n",
      "     42        \u001b[36m0.5266\u001b[0m       0.5185        \u001b[35m0.5321\u001b[0m  0.0589\n",
      "     43        \u001b[36m0.5224\u001b[0m       0.5185        \u001b[35m0.5281\u001b[0m  0.0584\n",
      "     44        \u001b[36m0.5183\u001b[0m       0.5185        \u001b[35m0.5242\u001b[0m  0.0540\n",
      "     45        \u001b[36m0.5143\u001b[0m       0.5185        \u001b[35m0.5204\u001b[0m  0.0613\n",
      "     46        \u001b[36m0.5103\u001b[0m       0.5185        \u001b[35m0.5166\u001b[0m  0.0587\n",
      "     47        \u001b[36m0.5065\u001b[0m       0.5185        \u001b[35m0.5130\u001b[0m  0.0559\n",
      "     48        \u001b[36m0.5028\u001b[0m       0.5185        \u001b[35m0.5094\u001b[0m  0.0597\n",
      "     49        \u001b[36m0.4992\u001b[0m       0.5185        \u001b[35m0.5059\u001b[0m  0.0619\n",
      "     50        \u001b[36m0.4956\u001b[0m       0.5185        \u001b[35m0.5025\u001b[0m  0.0629\n",
      "Xtrain pre augmentation: (2230, 5)\n",
      "Ytrain pre augmentation (2230, 1)\n",
      "Xtrain: (3244, 5)\n",
      "Ytrain (3244, 1)\n",
      "Xtest (557, 5)\n",
      "Ytest (557, 1)\n",
      "Re-initializing module.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6899\u001b[0m       \u001b[32m0.5208\u001b[0m        \u001b[35m0.6395\u001b[0m  0.0781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2        \u001b[36m0.6543\u001b[0m       0.5208        \u001b[35m0.6392\u001b[0m  0.0805\n",
      "      3        \u001b[36m0.6379\u001b[0m       0.5208        \u001b[35m0.6313\u001b[0m  0.0975\n",
      "      4        \u001b[36m0.6262\u001b[0m       0.5208        \u001b[35m0.6236\u001b[0m  0.0804\n",
      "      5        \u001b[36m0.6165\u001b[0m       0.5208        \u001b[35m0.6175\u001b[0m  0.0799\n",
      "      6        \u001b[36m0.6077\u001b[0m       0.5208        \u001b[35m0.6129\u001b[0m  0.1898\n",
      "      7        \u001b[36m0.5997\u001b[0m       0.5208        \u001b[35m0.6088\u001b[0m  0.0624\n",
      "      8        \u001b[36m0.5923\u001b[0m       0.5208        \u001b[35m0.6031\u001b[0m  0.0586\n",
      "      9        \u001b[36m0.5853\u001b[0m       0.5208        \u001b[35m0.5958\u001b[0m  0.0609\n",
      "     10        \u001b[36m0.5786\u001b[0m       0.5208        \u001b[35m0.5877\u001b[0m  0.0634\n",
      "     11        \u001b[36m0.5722\u001b[0m       0.5208        \u001b[35m0.5800\u001b[0m  0.0551\n",
      "     12        \u001b[36m0.5661\u001b[0m       0.5208        \u001b[35m0.5729\u001b[0m  0.0611\n",
      "     13        \u001b[36m0.5603\u001b[0m       0.5208        \u001b[35m0.5666\u001b[0m  0.0572\n",
      "     14        \u001b[36m0.5547\u001b[0m       0.5208        \u001b[35m0.5609\u001b[0m  0.0634\n",
      "     15        \u001b[36m0.5493\u001b[0m       0.5208        \u001b[35m0.5557\u001b[0m  0.0565\n",
      "     16        \u001b[36m0.5441\u001b[0m       0.5208        \u001b[35m0.5508\u001b[0m  0.0593\n",
      "     17        \u001b[36m0.5392\u001b[0m       0.5208        \u001b[35m0.5462\u001b[0m  0.0531\n",
      "     18        \u001b[36m0.5344\u001b[0m       0.5208        \u001b[35m0.5417\u001b[0m  0.0514\n",
      "     19        \u001b[36m0.5298\u001b[0m       0.5208        \u001b[35m0.5375\u001b[0m  0.0543\n",
      "     20        \u001b[36m0.5253\u001b[0m       0.5208        \u001b[35m0.5335\u001b[0m  0.0531\n",
      "     21        \u001b[36m0.5210\u001b[0m       0.5208        \u001b[35m0.5297\u001b[0m  0.0502\n",
      "     22        \u001b[36m0.5169\u001b[0m       0.5208        \u001b[35m0.5259\u001b[0m  0.0541\n",
      "     23        \u001b[36m0.5129\u001b[0m       0.5208        \u001b[35m0.5223\u001b[0m  0.0528\n",
      "     24        \u001b[36m0.5090\u001b[0m       0.5208        \u001b[35m0.5187\u001b[0m  0.0690\n",
      "     25        \u001b[36m0.5052\u001b[0m       0.5208        \u001b[35m0.5153\u001b[0m  0.0534\n",
      "     26        \u001b[36m0.5016\u001b[0m       0.5208        \u001b[35m0.5119\u001b[0m  0.0562\n",
      "     27        \u001b[36m0.4981\u001b[0m       0.5208        \u001b[35m0.5087\u001b[0m  0.0507\n",
      "     28        \u001b[36m0.4947\u001b[0m       0.5208        \u001b[35m0.5056\u001b[0m  0.0534\n",
      "     29        \u001b[36m0.4914\u001b[0m       0.5208        \u001b[35m0.5026\u001b[0m  0.0515\n",
      "     30        \u001b[36m0.4882\u001b[0m       0.5208        \u001b[35m0.4996\u001b[0m  0.0506\n",
      "     31        \u001b[36m0.4850\u001b[0m       0.5208        \u001b[35m0.4968\u001b[0m  0.0504\n",
      "     32        \u001b[36m0.4820\u001b[0m       0.5208        \u001b[35m0.4941\u001b[0m  0.0492\n",
      "     33        \u001b[36m0.4791\u001b[0m       0.5208        \u001b[35m0.4914\u001b[0m  0.0487\n",
      "     34        \u001b[36m0.4762\u001b[0m       0.5208        \u001b[35m0.4888\u001b[0m  0.0459\n",
      "     35        \u001b[36m0.4735\u001b[0m       0.5208        \u001b[35m0.4863\u001b[0m  0.0474\n",
      "     36        \u001b[36m0.4708\u001b[0m       0.5208        \u001b[35m0.4838\u001b[0m  0.0474\n",
      "     37        \u001b[36m0.4681\u001b[0m       0.5208        \u001b[35m0.4813\u001b[0m  0.0469\n",
      "     38        \u001b[36m0.4656\u001b[0m       0.5208        \u001b[35m0.4790\u001b[0m  0.0522\n",
      "     39        \u001b[36m0.4631\u001b[0m       0.5208        \u001b[35m0.4767\u001b[0m  0.0505\n",
      "     40        \u001b[36m0.4606\u001b[0m       0.5208        \u001b[35m0.4744\u001b[0m  0.0470\n",
      "     41        \u001b[36m0.4582\u001b[0m       0.5208        \u001b[35m0.4722\u001b[0m  0.0478\n",
      "     42        \u001b[36m0.4559\u001b[0m       0.5208        \u001b[35m0.4700\u001b[0m  0.0483\n",
      "     43        \u001b[36m0.4536\u001b[0m       0.5208        \u001b[35m0.4679\u001b[0m  0.0528\n",
      "     44        \u001b[36m0.4514\u001b[0m       0.5208        \u001b[35m0.4658\u001b[0m  0.0570\n",
      "     45        \u001b[36m0.4492\u001b[0m       0.5208        \u001b[35m0.4637\u001b[0m  0.0525\n",
      "     46        \u001b[36m0.4471\u001b[0m       0.5208        \u001b[35m0.4617\u001b[0m  0.0468\n",
      "     47        \u001b[36m0.4450\u001b[0m       0.5208        \u001b[35m0.4598\u001b[0m  0.0499\n",
      "     48        \u001b[36m0.4429\u001b[0m       0.5208        \u001b[35m0.4578\u001b[0m  0.0490\n",
      "     49        \u001b[36m0.4409\u001b[0m       0.5208        \u001b[35m0.4560\u001b[0m  0.0457\n",
      "     50        \u001b[36m0.4390\u001b[0m       0.5208        \u001b[35m0.4541\u001b[0m  0.0477\n",
      "Xtrain pre augmentation: (2230, 5)\n",
      "Ytrain pre augmentation (2230, 1)\n",
      "Xtrain: (3265, 5)\n",
      "Ytrain (3265, 1)\n",
      "Xtest (557, 5)\n",
      "Ytest (557, 1)\n",
      "Re-initializing module.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.4705\u001b[0m       \u001b[32m0.5115\u001b[0m        \u001b[35m0.5062\u001b[0m  0.0483\n",
      "      2        \u001b[36m0.4660\u001b[0m       0.5115        0.5063  0.0508\n",
      "      3        0.4787       0.5115        0.5453  0.0476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2457: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      4        0.4907       0.5115        0.5302  0.0562\n",
      "      5        0.4875       0.5115        0.5159  0.0563\n",
      "      6        0.4844       0.5115        \u001b[35m0.5043\u001b[0m  0.0508\n",
      "      7        0.4814       0.5115        \u001b[35m0.4955\u001b[0m  0.0485\n",
      "      8        0.4784       0.5115        \u001b[35m0.4887\u001b[0m  0.0478\n",
      "      9        0.4756       0.5115        \u001b[35m0.4836\u001b[0m  0.0484\n",
      "     10        0.4730       0.5115        \u001b[35m0.4797\u001b[0m  0.0996\n",
      "     11        0.4705       0.5115        \u001b[35m0.4764\u001b[0m  0.0501\n",
      "     12        0.4681       0.5115        \u001b[35m0.4737\u001b[0m  0.0547\n",
      "     13        \u001b[36m0.4659\u001b[0m       0.5115        \u001b[35m0.4714\u001b[0m  0.0545\n",
      "     14        \u001b[36m0.4637\u001b[0m       0.5115        \u001b[35m0.4692\u001b[0m  0.0488\n",
      "     15        \u001b[36m0.4616\u001b[0m       0.5115        \u001b[35m0.4672\u001b[0m  0.0513\n",
      "     16        \u001b[36m0.4596\u001b[0m       0.5115        \u001b[35m0.4652\u001b[0m  0.0491\n",
      "     17        \u001b[36m0.4577\u001b[0m       0.5115        \u001b[35m0.4634\u001b[0m  0.0468\n",
      "     18        \u001b[36m0.4558\u001b[0m       0.5115        \u001b[35m0.4616\u001b[0m  0.0655\n",
      "     19        \u001b[36m0.4540\u001b[0m       0.5115        \u001b[35m0.4599\u001b[0m  0.0541\n",
      "     20        \u001b[36m0.4523\u001b[0m       0.5115        \u001b[35m0.4582\u001b[0m  0.0538\n",
      "     21        \u001b[36m0.4506\u001b[0m       0.5115        \u001b[35m0.4565\u001b[0m  0.0505\n",
      "     22        \u001b[36m0.4489\u001b[0m       0.5115        \u001b[35m0.4549\u001b[0m  0.0500\n",
      "     23        \u001b[36m0.4472\u001b[0m       0.5115        \u001b[35m0.4533\u001b[0m  0.0502\n",
      "     24        \u001b[36m0.4456\u001b[0m       0.5115        \u001b[35m0.4518\u001b[0m  0.0496\n",
      "     25        \u001b[36m0.4440\u001b[0m       0.5115        \u001b[35m0.4502\u001b[0m  0.0547\n",
      "     26        \u001b[36m0.4424\u001b[0m       0.5115        \u001b[35m0.4487\u001b[0m  0.0495\n",
      "     27        \u001b[36m0.4408\u001b[0m       0.5115        \u001b[35m0.4472\u001b[0m  0.0520\n",
      "     28        \u001b[36m0.4393\u001b[0m       0.5115        \u001b[35m0.4458\u001b[0m  0.0482\n",
      "     29        \u001b[36m0.4377\u001b[0m       0.5115        \u001b[35m0.4443\u001b[0m  0.0505\n",
      "     30        \u001b[36m0.4362\u001b[0m       0.5115        \u001b[35m0.4428\u001b[0m  0.0500\n",
      "     31        \u001b[36m0.4346\u001b[0m       0.5115        \u001b[35m0.4413\u001b[0m  0.0524\n",
      "     32        \u001b[36m0.4331\u001b[0m       0.5115        \u001b[35m0.4399\u001b[0m  0.0546\n",
      "     33        \u001b[36m0.4315\u001b[0m       0.5115        \u001b[35m0.4384\u001b[0m  0.0563\n",
      "     34        \u001b[36m0.4301\u001b[0m       0.5115        \u001b[35m0.4371\u001b[0m  0.0496\n",
      "     35        \u001b[36m0.4287\u001b[0m       0.5115        \u001b[35m0.4358\u001b[0m  0.0465\n",
      "     36        \u001b[36m0.4272\u001b[0m       0.5115        \u001b[35m0.4345\u001b[0m  0.0465\n",
      "     37        \u001b[36m0.4257\u001b[0m       0.5115        \u001b[35m0.4332\u001b[0m  0.0495\n",
      "     38        \u001b[36m0.4240\u001b[0m       0.5115        \u001b[35m0.4315\u001b[0m  0.0510\n",
      "     39        \u001b[36m0.4225\u001b[0m       0.5115        \u001b[35m0.4299\u001b[0m  0.0487\n",
      "     40        \u001b[36m0.4211\u001b[0m       0.5115        \u001b[35m0.4284\u001b[0m  0.0436\n",
      "     41        \u001b[36m0.4197\u001b[0m       0.5115        \u001b[35m0.4270\u001b[0m  0.0463\n",
      "     42        \u001b[36m0.4183\u001b[0m       0.5115        \u001b[35m0.4255\u001b[0m  0.0453\n",
      "     43        \u001b[36m0.4168\u001b[0m       0.5115        \u001b[35m0.4242\u001b[0m  0.0472\n",
      "     44        \u001b[36m0.4154\u001b[0m       0.5115        \u001b[35m0.4226\u001b[0m  0.0480\n",
      "     45        \u001b[36m0.4142\u001b[0m       0.5115        \u001b[35m0.4211\u001b[0m  0.0483\n",
      "     46        \u001b[36m0.4131\u001b[0m       0.5115        \u001b[35m0.4198\u001b[0m  0.0471\n",
      "     47        \u001b[36m0.4120\u001b[0m       0.5115        \u001b[35m0.4185\u001b[0m  0.0479\n",
      "     48        \u001b[36m0.4108\u001b[0m       0.5115        \u001b[35m0.4174\u001b[0m  0.0460\n",
      "     49        \u001b[36m0.4097\u001b[0m       0.5115        \u001b[35m0.4162\u001b[0m  0.0437\n",
      "     50        \u001b[36m0.4086\u001b[0m       0.5115        \u001b[35m0.4151\u001b[0m  0.0462\n",
      "Xtrain pre augmentation: (2230, 5)\n",
      "Ytrain pre augmentation (2230, 1)\n",
      "Xtrain: (3229, 5)\n",
      "Ytrain (3229, 1)\n",
      "Xtest (557, 5)\n",
      "Ytest (557, 1)\n",
      "Re-initializing module.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6896\u001b[0m       \u001b[32m0.5279\u001b[0m        \u001b[35m0.7113\u001b[0m  0.0484\n",
      "      2        \u001b[36m0.6785\u001b[0m       0.5279        \u001b[35m0.6926\u001b[0m  0.0529\n",
      "      3        \u001b[36m0.6685\u001b[0m       0.5279        \u001b[35m0.6768\u001b[0m  0.0446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      4        \u001b[36m0.6592\u001b[0m       0.5279        \u001b[35m0.6604\u001b[0m  0.0500\n",
      "      5        \u001b[36m0.6506\u001b[0m       0.5279        \u001b[35m0.6437\u001b[0m  0.0469\n",
      "      6        \u001b[36m0.6425\u001b[0m       0.5279        \u001b[35m0.6284\u001b[0m  0.0514\n",
      "      7        \u001b[36m0.6349\u001b[0m       0.5279        \u001b[35m0.6161\u001b[0m  0.0454\n",
      "      8        \u001b[36m0.6277\u001b[0m       0.5279        \u001b[35m0.6074\u001b[0m  0.0466\n",
      "      9        \u001b[36m0.6207\u001b[0m       0.5279        \u001b[35m0.6018\u001b[0m  0.1041\n",
      "     10        \u001b[36m0.6140\u001b[0m       0.5279        \u001b[35m0.5980\u001b[0m  0.0446\n",
      "     11        \u001b[36m0.6073\u001b[0m       0.5279        \u001b[35m0.5944\u001b[0m  0.0474\n",
      "     12        \u001b[36m0.6005\u001b[0m       0.5279        \u001b[35m0.5899\u001b[0m  0.0533\n",
      "     13        \u001b[36m0.5934\u001b[0m       0.5279        \u001b[35m0.5834\u001b[0m  0.0471\n",
      "     14        \u001b[36m0.5854\u001b[0m       0.5279        \u001b[35m0.5739\u001b[0m  0.0520\n",
      "     15        \u001b[36m0.5737\u001b[0m       0.5279        \u001b[35m0.5555\u001b[0m  0.0596\n",
      "     16        \u001b[36m0.5412\u001b[0m       0.5279        \u001b[35m0.5113\u001b[0m  0.0476\n",
      "     17        \u001b[36m0.5069\u001b[0m       0.5279        \u001b[35m0.4960\u001b[0m  0.0493\n",
      "     18        \u001b[36m0.5007\u001b[0m       0.5279        \u001b[35m0.4910\u001b[0m  0.0509\n",
      "     19        \u001b[36m0.4968\u001b[0m       0.5279        \u001b[35m0.4883\u001b[0m  0.0512\n",
      "     20        \u001b[36m0.4935\u001b[0m       0.5279        \u001b[35m0.4861\u001b[0m  0.0514\n",
      "     21        \u001b[36m0.4904\u001b[0m       0.5279        \u001b[35m0.4838\u001b[0m  0.0500\n",
      "     22        \u001b[36m0.4875\u001b[0m       0.5279        \u001b[35m0.4815\u001b[0m  0.0479\n",
      "     23        \u001b[36m0.4848\u001b[0m       0.5279        \u001b[35m0.4792\u001b[0m  0.0464\n",
      "     24        \u001b[36m0.4821\u001b[0m       0.5279        \u001b[35m0.4769\u001b[0m  0.0472\n",
      "     25        \u001b[36m0.4794\u001b[0m       0.5279        \u001b[35m0.4745\u001b[0m  0.0469\n",
      "     26        \u001b[36m0.4769\u001b[0m       0.5279        \u001b[35m0.4722\u001b[0m  0.0496\n",
      "     27        \u001b[36m0.4744\u001b[0m       0.5279        \u001b[35m0.4698\u001b[0m  0.0490\n",
      "     28        \u001b[36m0.4720\u001b[0m       0.5279        \u001b[35m0.4675\u001b[0m  0.0451\n",
      "     29        \u001b[36m0.4696\u001b[0m       0.5279        \u001b[35m0.4652\u001b[0m  0.0503\n",
      "     30        \u001b[36m0.4673\u001b[0m       0.5279        \u001b[35m0.4630\u001b[0m  0.0450\n",
      "     31        \u001b[36m0.4650\u001b[0m       0.5279        \u001b[35m0.4607\u001b[0m  0.0508\n",
      "     32        \u001b[36m0.4627\u001b[0m       0.5279        \u001b[35m0.4585\u001b[0m  0.0479\n",
      "     33        \u001b[36m0.4605\u001b[0m       0.5279        \u001b[35m0.4563\u001b[0m  0.0492\n",
      "     34        \u001b[36m0.4583\u001b[0m       0.5279        \u001b[35m0.4541\u001b[0m  0.0447\n",
      "     35        \u001b[36m0.4562\u001b[0m       0.5279        \u001b[35m0.4519\u001b[0m  0.0437\n",
      "     36        \u001b[36m0.4541\u001b[0m       0.5279        \u001b[35m0.4498\u001b[0m  0.0469\n",
      "     37        \u001b[36m0.4520\u001b[0m       0.5279        \u001b[35m0.4477\u001b[0m  0.0444\n",
      "     38        \u001b[36m0.4500\u001b[0m       0.5279        \u001b[35m0.4457\u001b[0m  0.0451\n",
      "     39        \u001b[36m0.4480\u001b[0m       0.5279        \u001b[35m0.4436\u001b[0m  0.0456\n",
      "     40        \u001b[36m0.4460\u001b[0m       0.5279        \u001b[35m0.4416\u001b[0m  0.0434\n",
      "     41        \u001b[36m0.4441\u001b[0m       0.5279        \u001b[35m0.4396\u001b[0m  0.0465\n",
      "     42        \u001b[36m0.4421\u001b[0m       0.5279        \u001b[35m0.4376\u001b[0m  0.0458\n",
      "     43        \u001b[36m0.4402\u001b[0m       0.5279        \u001b[35m0.4356\u001b[0m  0.0435\n",
      "     44        \u001b[36m0.4384\u001b[0m       0.5279        \u001b[35m0.4337\u001b[0m  0.0438\n",
      "     45        \u001b[36m0.4365\u001b[0m       0.5279        \u001b[35m0.4318\u001b[0m  0.0445\n",
      "     46        \u001b[36m0.4347\u001b[0m       0.5279        \u001b[35m0.4300\u001b[0m  0.0490\n",
      "     47        \u001b[36m0.4329\u001b[0m       0.5279        \u001b[35m0.4282\u001b[0m  0.0489\n",
      "     48        \u001b[36m0.4312\u001b[0m       0.5279        \u001b[35m0.4264\u001b[0m  0.0445\n",
      "     49        \u001b[36m0.4294\u001b[0m       0.5279        \u001b[35m0.4246\u001b[0m  0.0446\n",
      "     50        \u001b[36m0.4277\u001b[0m       0.5279        \u001b[35m0.4228\u001b[0m  0.0436\n",
      "Xtrain pre augmentation: (12941, 6)\n",
      "Ytrain pre augmentation (12941, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain: (19849, 6)\n",
      "Ytrain (19849, 1)\n",
      "Xtest (3236, 6)\n",
      "Ytest (3236, 1)\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.5015\u001b[0m       \u001b[32m0.4778\u001b[0m        \u001b[35m0.4988\u001b[0m  0.2193\n",
      "      2        \u001b[36m0.4581\u001b[0m       0.4778        \u001b[35m0.4572\u001b[0m  0.2382\n",
      "      3        \u001b[36m0.4390\u001b[0m       0.4778        \u001b[35m0.4387\u001b[0m  0.2309\n",
      "      4        \u001b[36m0.4269\u001b[0m       0.4778        \u001b[35m0.4284\u001b[0m  0.2220\n",
      "      5        \u001b[36m0.4177\u001b[0m       0.4778        \u001b[35m0.4204\u001b[0m  0.2305\n",
      "      6        \u001b[36m0.4102\u001b[0m       0.4778        \u001b[35m0.4136\u001b[0m  0.2313\n",
      "      7        \u001b[36m0.4037\u001b[0m       0.4778        \u001b[35m0.4078\u001b[0m  0.2227\n",
      "      8        \u001b[36m0.3980\u001b[0m       0.4778        \u001b[35m0.4026\u001b[0m  0.2112\n",
      "      9        \u001b[36m0.3930\u001b[0m       0.4778        \u001b[35m0.3980\u001b[0m  0.2181\n",
      "     10        \u001b[36m0.3885\u001b[0m       0.4778        \u001b[35m0.3938\u001b[0m  0.2165\n",
      "     11        \u001b[36m0.3844\u001b[0m       0.4778        \u001b[35m0.3899\u001b[0m  0.2159\n",
      "     12        \u001b[36m0.3807\u001b[0m       0.4778        \u001b[35m0.3865\u001b[0m  0.2241\n",
      "     13        \u001b[36m0.3773\u001b[0m       0.4778        \u001b[35m0.3833\u001b[0m  0.2302\n",
      "     14        \u001b[36m0.3742\u001b[0m       0.4778        \u001b[35m0.3804\u001b[0m  0.2700\n",
      "     15        \u001b[36m0.3713\u001b[0m       0.4778        \u001b[35m0.3777\u001b[0m  0.2152\n",
      "     16        \u001b[36m0.3686\u001b[0m       0.4778        \u001b[35m0.3754\u001b[0m  0.2135\n",
      "     17        \u001b[36m0.3662\u001b[0m       0.4778        \u001b[35m0.3729\u001b[0m  0.2153\n",
      "     18        \u001b[36m0.3639\u001b[0m       0.4778        \u001b[35m0.3709\u001b[0m  0.2304\n",
      "     19        \u001b[36m0.3617\u001b[0m       0.4778        \u001b[35m0.3688\u001b[0m  0.2267\n",
      "     20        \u001b[36m0.3597\u001b[0m       0.4778        \u001b[35m0.3669\u001b[0m  0.2188\n",
      "     21        \u001b[36m0.3579\u001b[0m       0.4778        \u001b[35m0.3655\u001b[0m  0.2193\n",
      "     22        \u001b[36m0.3561\u001b[0m       0.4778        \u001b[35m0.3638\u001b[0m  0.2236\n",
      "     23        \u001b[36m0.3545\u001b[0m       0.4778        \u001b[35m0.3622\u001b[0m  0.2188\n",
      "     24        \u001b[36m0.3529\u001b[0m       0.4778        \u001b[35m0.3607\u001b[0m  0.2235\n",
      "     25        \u001b[36m0.3514\u001b[0m       0.4778        \u001b[35m0.3593\u001b[0m  0.2124\n",
      "     26        \u001b[36m0.3500\u001b[0m       0.4778        \u001b[35m0.3580\u001b[0m  0.2127\n",
      "     27        \u001b[36m0.3487\u001b[0m       0.4778        \u001b[35m0.3567\u001b[0m  0.2170\n",
      "     28        \u001b[36m0.3474\u001b[0m       0.4778        \u001b[35m0.3555\u001b[0m  0.2176\n",
      "     29        \u001b[36m0.3462\u001b[0m       0.4778        \u001b[35m0.3544\u001b[0m  0.2160\n",
      "     30        \u001b[36m0.3451\u001b[0m       0.4778        \u001b[35m0.3533\u001b[0m  0.2147\n",
      "     31        \u001b[36m0.3440\u001b[0m       0.4778        \u001b[35m0.3522\u001b[0m  0.2143\n",
      "     32        \u001b[36m0.3430\u001b[0m       0.4778        \u001b[35m0.3509\u001b[0m  0.2175\n",
      "     33        \u001b[36m0.3420\u001b[0m       0.4778        \u001b[35m0.3504\u001b[0m  0.2115\n",
      "     34        \u001b[36m0.3410\u001b[0m       0.4778        \u001b[35m0.3495\u001b[0m  0.2147\n",
      "     35        \u001b[36m0.3401\u001b[0m       0.4778        \u001b[35m0.3481\u001b[0m  0.2126\n",
      "     36        \u001b[36m0.3393\u001b[0m       0.4778        \u001b[35m0.3473\u001b[0m  0.2155\n",
      "     37        \u001b[36m0.3384\u001b[0m       0.4778        \u001b[35m0.3465\u001b[0m  0.2106\n",
      "     38        \u001b[36m0.3376\u001b[0m       0.4778        \u001b[35m0.3457\u001b[0m  0.2119\n",
      "     39        \u001b[36m0.3369\u001b[0m       0.4778        \u001b[35m0.3450\u001b[0m  0.2085\n",
      "     40        \u001b[36m0.3361\u001b[0m       0.4778        \u001b[35m0.3443\u001b[0m  0.2106\n",
      "     41        \u001b[36m0.3354\u001b[0m       0.4778        \u001b[35m0.3436\u001b[0m  0.2128\n",
      "     42        \u001b[36m0.3347\u001b[0m       0.4778        \u001b[35m0.3429\u001b[0m  0.2095\n",
      "     43        \u001b[36m0.3340\u001b[0m       0.4778        \u001b[35m0.3422\u001b[0m  0.2124\n",
      "     44        \u001b[36m0.3334\u001b[0m       0.4778        \u001b[35m0.3416\u001b[0m  0.2133\n",
      "     45        \u001b[36m0.3328\u001b[0m       0.4778        \u001b[35m0.3410\u001b[0m  0.2100\n",
      "     46        \u001b[36m0.3322\u001b[0m       0.4778        \u001b[35m0.3404\u001b[0m  0.2164\n",
      "     47        \u001b[36m0.3316\u001b[0m       0.4778        \u001b[35m0.3399\u001b[0m  0.2270\n",
      "     48        \u001b[36m0.3311\u001b[0m       0.4778        \u001b[35m0.3393\u001b[0m  0.2168\n",
      "     49        \u001b[36m0.3305\u001b[0m       0.4778        \u001b[35m0.3387\u001b[0m  0.2141\n",
      "     50        \u001b[36m0.3300\u001b[0m       0.4778        \u001b[35m0.3382\u001b[0m  0.2193\n",
      "Xtrain pre augmentation: (12941, 6)\n",
      "Ytrain pre augmentation (12941, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain: (19108, 6)\n",
      "Ytrain (19108, 1)\n",
      "Xtest (3236, 6)\n",
      "Ytest (3236, 1)\n",
      "Re-initializing module.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.5492\u001b[0m       \u001b[32m0.4780\u001b[0m        \u001b[35m0.5410\u001b[0m  0.2043\n",
      "      2        \u001b[36m0.5305\u001b[0m       0.4780        \u001b[35m0.5200\u001b[0m  0.2689\n",
      "      3        \u001b[36m0.5133\u001b[0m       0.4780        \u001b[35m0.5082\u001b[0m  0.2192\n",
      "      4        \u001b[36m0.4983\u001b[0m       0.4780        \u001b[35m0.4940\u001b[0m  0.2171\n",
      "      5        \u001b[36m0.4826\u001b[0m       0.4780        \u001b[35m0.4809\u001b[0m  0.2156\n",
      "      6        \u001b[36m0.4695\u001b[0m       0.4780        \u001b[35m0.4685\u001b[0m  0.2111\n",
      "      7        \u001b[36m0.4571\u001b[0m       0.4780        \u001b[35m0.4562\u001b[0m  0.2130\n",
      "      8        \u001b[36m0.4455\u001b[0m       0.4780        \u001b[35m0.4459\u001b[0m  0.2181\n",
      "      9        \u001b[36m0.4352\u001b[0m       0.4780        \u001b[35m0.4361\u001b[0m  0.2144\n",
      "     10        \u001b[36m0.4253\u001b[0m       0.4780        \u001b[35m0.4267\u001b[0m  0.2281\n",
      "     11        \u001b[36m0.4160\u001b[0m       0.4780        \u001b[35m0.4178\u001b[0m  0.2157\n",
      "     12        \u001b[36m0.4071\u001b[0m       0.4780        \u001b[35m0.4095\u001b[0m  0.2177\n",
      "     13        \u001b[36m0.3987\u001b[0m       0.4780        \u001b[35m0.4016\u001b[0m  0.2186\n",
      "     14        \u001b[36m0.3909\u001b[0m       0.4780        \u001b[35m0.3942\u001b[0m  0.2281\n",
      "     15        \u001b[36m0.3836\u001b[0m       0.4780        \u001b[35m0.3872\u001b[0m  0.2127\n",
      "     16        \u001b[36m0.3769\u001b[0m       0.4780        \u001b[35m0.3807\u001b[0m  0.2131\n",
      "     17        \u001b[36m0.3706\u001b[0m       0.4780        \u001b[35m0.3745\u001b[0m  0.2170\n",
      "     18        \u001b[36m0.3647\u001b[0m       0.4780        \u001b[35m0.3688\u001b[0m  0.2203\n",
      "     19        \u001b[36m0.3591\u001b[0m       0.4780        \u001b[35m0.3635\u001b[0m  0.2207\n",
      "     20        \u001b[36m0.3539\u001b[0m       0.4780        \u001b[35m0.3583\u001b[0m  0.2169\n",
      "     21        \u001b[36m0.3490\u001b[0m       0.4780        \u001b[35m0.3536\u001b[0m  0.2171\n",
      "     22        \u001b[36m0.3444\u001b[0m       0.4780        \u001b[35m0.3490\u001b[0m  0.2073\n",
      "     23        \u001b[36m0.3399\u001b[0m       0.4780        \u001b[35m0.3446\u001b[0m  0.2120\n",
      "     24        \u001b[36m0.3357\u001b[0m       0.4780        \u001b[35m0.3405\u001b[0m  0.2099\n",
      "     25        \u001b[36m0.3317\u001b[0m       0.4780        \u001b[35m0.3365\u001b[0m  0.2127\n",
      "     26        \u001b[36m0.3279\u001b[0m       0.4780        \u001b[35m0.3327\u001b[0m  0.2143\n",
      "     27        \u001b[36m0.3242\u001b[0m       0.4780        \u001b[35m0.3291\u001b[0m  0.2146\n",
      "     28        \u001b[36m0.3207\u001b[0m       0.4780        \u001b[35m0.3256\u001b[0m  0.2133\n",
      "     29        \u001b[36m0.3173\u001b[0m       0.4780        \u001b[35m0.3223\u001b[0m  0.2200\n",
      "     30        \u001b[36m0.3140\u001b[0m       0.4780        \u001b[35m0.3191\u001b[0m  0.2097\n",
      "     31        \u001b[36m0.3109\u001b[0m       0.4780        \u001b[35m0.3158\u001b[0m  0.2150\n",
      "     32        \u001b[36m0.3078\u001b[0m       0.4780        \u001b[35m0.3128\u001b[0m  0.2171\n",
      "     33        \u001b[36m0.3049\u001b[0m       0.4780        \u001b[35m0.3099\u001b[0m  0.2130\n",
      "     34        \u001b[36m0.3020\u001b[0m       0.4780        \u001b[35m0.3070\u001b[0m  0.2142\n",
      "     35        \u001b[36m0.2992\u001b[0m       0.4780        \u001b[35m0.3043\u001b[0m  0.2106\n",
      "     36        \u001b[36m0.2965\u001b[0m       0.4780        \u001b[35m0.3016\u001b[0m  0.2217\n",
      "     37        \u001b[36m0.2939\u001b[0m       0.4780        \u001b[35m0.2990\u001b[0m  0.2132\n",
      "     38        \u001b[36m0.2914\u001b[0m       0.4780        \u001b[35m0.2965\u001b[0m  0.2156\n",
      "     39        \u001b[36m0.2889\u001b[0m       0.4780        \u001b[35m0.2940\u001b[0m  0.2162\n",
      "     40        \u001b[36m0.2866\u001b[0m       0.4780        \u001b[35m0.2916\u001b[0m  0.2124\n",
      "     41        \u001b[36m0.2843\u001b[0m       0.4780        \u001b[35m0.2893\u001b[0m  0.2490\n",
      "     42        \u001b[36m0.2820\u001b[0m       0.4780        \u001b[35m0.2870\u001b[0m  0.2098\n",
      "     43        \u001b[36m0.2798\u001b[0m       0.4780        \u001b[35m0.2848\u001b[0m  0.2134\n",
      "     44        \u001b[36m0.2777\u001b[0m       0.4780        \u001b[35m0.2827\u001b[0m  0.2114\n",
      "     45        \u001b[36m0.2757\u001b[0m       0.4780        \u001b[35m0.2806\u001b[0m  0.2155\n",
      "     46        \u001b[36m0.2736\u001b[0m       0.4780        \u001b[35m0.2786\u001b[0m  0.2152\n",
      "     47        \u001b[36m0.2717\u001b[0m       0.4780        \u001b[35m0.2765\u001b[0m  0.2093\n",
      "     48        \u001b[36m0.2697\u001b[0m       0.4780        \u001b[35m0.2746\u001b[0m  0.2075\n",
      "     49        \u001b[36m0.2678\u001b[0m       0.4780        \u001b[35m0.2728\u001b[0m  0.2124\n",
      "     50        \u001b[36m0.2660\u001b[0m       0.4780        \u001b[35m0.2709\u001b[0m  0.2162\n",
      "Xtrain pre augmentation: (12942, 6)\n",
      "Ytrain pre augmentation (12942, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2457: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain: (19723, 6)\n",
      "Ytrain (19723, 1)\n",
      "Xtest (3235, 6)\n",
      "Ytest (3235, 1)\n",
      "Re-initializing module.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7846\u001b[0m       \u001b[32m0.4824\u001b[0m        \u001b[35m0.7508\u001b[0m  0.2214\n",
      "      2        \u001b[36m0.7182\u001b[0m       0.4824        \u001b[35m0.6830\u001b[0m  0.2433\n",
      "      3        \u001b[36m0.6662\u001b[0m       0.4824        \u001b[35m0.6424\u001b[0m  0.2311\n",
      "      4        \u001b[36m0.6281\u001b[0m       0.4824        \u001b[35m0.6104\u001b[0m  0.2292\n",
      "      5        \u001b[36m0.5966\u001b[0m       0.4824        \u001b[35m0.5828\u001b[0m  0.2319\n",
      "      6        \u001b[36m0.5699\u001b[0m       0.4824        \u001b[35m0.5590\u001b[0m  0.2266\n",
      "      7        \u001b[36m0.5468\u001b[0m       0.4824        \u001b[35m0.5384\u001b[0m  0.2226\n",
      "      8        \u001b[36m0.5268\u001b[0m       0.4824        \u001b[35m0.5202\u001b[0m  0.2309\n",
      "      9        \u001b[36m0.5092\u001b[0m       0.4824        \u001b[35m0.5042\u001b[0m  0.2208\n",
      "     10        \u001b[36m0.4936\u001b[0m       0.4824        \u001b[35m0.4899\u001b[0m  0.2407\n",
      "     11        \u001b[36m0.4798\u001b[0m       0.4824        \u001b[35m0.4773\u001b[0m  0.2302\n",
      "     12        \u001b[36m0.4675\u001b[0m       0.4824        \u001b[35m0.4660\u001b[0m  0.2338\n",
      "     13        \u001b[36m0.4565\u001b[0m       0.4824        \u001b[35m0.4560\u001b[0m  0.2204\n",
      "     14        \u001b[36m0.4467\u001b[0m       0.4824        \u001b[35m0.4470\u001b[0m  0.2232\n",
      "     15        \u001b[36m0.4378\u001b[0m       0.4824        \u001b[35m0.4387\u001b[0m  0.2223\n",
      "     16        \u001b[36m0.4297\u001b[0m       0.4824        \u001b[35m0.4313\u001b[0m  0.2233\n",
      "     17        \u001b[36m0.4223\u001b[0m       0.4824        \u001b[35m0.4245\u001b[0m  0.2233\n",
      "     18        \u001b[36m0.4155\u001b[0m       0.4824        \u001b[35m0.4181\u001b[0m  0.2222\n",
      "     19        \u001b[36m0.4092\u001b[0m       0.4824        \u001b[35m0.4123\u001b[0m  0.2197\n",
      "     20        \u001b[36m0.4035\u001b[0m       0.4824        \u001b[35m0.4069\u001b[0m  0.2294\n",
      "     21        \u001b[36m0.3980\u001b[0m       0.4824        \u001b[35m0.4018\u001b[0m  0.2362\n",
      "     22        \u001b[36m0.3929\u001b[0m       0.4824        \u001b[35m0.3970\u001b[0m  0.2322\n",
      "     23        \u001b[36m0.3879\u001b[0m       0.4824        \u001b[35m0.3922\u001b[0m  0.2252\n",
      "     24        \u001b[36m0.3831\u001b[0m       0.4824        \u001b[35m0.3876\u001b[0m  0.2193\n",
      "     25        \u001b[36m0.3784\u001b[0m       0.4824        \u001b[35m0.3831\u001b[0m  0.2211\n",
      "     26        \u001b[36m0.3737\u001b[0m       0.4824        \u001b[35m0.3784\u001b[0m  0.2189\n",
      "     27        \u001b[36m0.3687\u001b[0m       0.4824        \u001b[35m0.3735\u001b[0m  0.2707\n",
      "     28        \u001b[36m0.3639\u001b[0m       0.4824        \u001b[35m0.3687\u001b[0m  0.2203\n",
      "     29        \u001b[36m0.3594\u001b[0m       0.4824        \u001b[35m0.3642\u001b[0m  0.2205\n",
      "     30        \u001b[36m0.3555\u001b[0m       0.4824        \u001b[35m0.3604\u001b[0m  0.2126\n",
      "     31        \u001b[36m0.3521\u001b[0m       0.4824        \u001b[35m0.3571\u001b[0m  0.2144\n",
      "     32        \u001b[36m0.3491\u001b[0m       0.4824        \u001b[35m0.3542\u001b[0m  0.2163\n",
      "     33        \u001b[36m0.3464\u001b[0m       0.4824        \u001b[35m0.3516\u001b[0m  0.2164\n",
      "     34        \u001b[36m0.3439\u001b[0m       0.4824        \u001b[35m0.3492\u001b[0m  0.2186\n",
      "     35        \u001b[36m0.3415\u001b[0m       0.4824        \u001b[35m0.3469\u001b[0m  0.2181\n",
      "     36        \u001b[36m0.3392\u001b[0m       0.4824        \u001b[35m0.3446\u001b[0m  0.2167\n",
      "     37        \u001b[36m0.3370\u001b[0m       0.4824        \u001b[35m0.3425\u001b[0m  0.2153\n",
      "     38        \u001b[36m0.3349\u001b[0m       0.4824        \u001b[35m0.3405\u001b[0m  0.2350\n",
      "     39        \u001b[36m0.3329\u001b[0m       0.4824        \u001b[35m0.3385\u001b[0m  0.2333\n",
      "     40        \u001b[36m0.3309\u001b[0m       0.4824        \u001b[35m0.3366\u001b[0m  0.2134\n",
      "     41        \u001b[36m0.3291\u001b[0m       0.4824        \u001b[35m0.3348\u001b[0m  0.2392\n",
      "     42        \u001b[36m0.3272\u001b[0m       0.4824        \u001b[35m0.3330\u001b[0m  0.2269\n",
      "     43        \u001b[36m0.3255\u001b[0m       0.4824        \u001b[35m0.3312\u001b[0m  0.2251\n",
      "     44        \u001b[36m0.3237\u001b[0m       0.4824        \u001b[35m0.3295\u001b[0m  0.2174\n",
      "     45        \u001b[36m0.3221\u001b[0m       0.4824        \u001b[35m0.3279\u001b[0m  0.2195\n",
      "     46        \u001b[36m0.3205\u001b[0m       0.4824        \u001b[35m0.3263\u001b[0m  0.2263\n",
      "     47        \u001b[36m0.3189\u001b[0m       0.4824        \u001b[35m0.3247\u001b[0m  0.2247\n",
      "     48        \u001b[36m0.3173\u001b[0m       0.4824        \u001b[35m0.3232\u001b[0m  0.2296\n",
      "     49        \u001b[36m0.3158\u001b[0m       0.4824        \u001b[35m0.3217\u001b[0m  0.2213\n",
      "     50        \u001b[36m0.3144\u001b[0m       0.4824        \u001b[35m0.3202\u001b[0m  0.2162\n",
      "Xtrain pre augmentation: (12942, 6)\n",
      "Ytrain pre augmentation (12942, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain: (19854, 6)\n",
      "Ytrain (19854, 1)\n",
      "Xtest (3235, 6)\n",
      "Ytest (3235, 1)\n",
      "Re-initializing module.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.5051\u001b[0m       \u001b[32m0.4734\u001b[0m        \u001b[35m0.5258\u001b[0m  0.2136\n",
      "      2        \u001b[36m0.4928\u001b[0m       0.4734        \u001b[35m0.4924\u001b[0m  0.2421\n",
      "      3        \u001b[36m0.4817\u001b[0m       0.4734        \u001b[35m0.4814\u001b[0m  0.2242\n",
      "      4        \u001b[36m0.4719\u001b[0m       0.4734        \u001b[35m0.4723\u001b[0m  0.2306\n",
      "      5        \u001b[36m0.4630\u001b[0m       0.4734        \u001b[35m0.4640\u001b[0m  0.2223\n",
      "      6        \u001b[36m0.4550\u001b[0m       0.4734        \u001b[35m0.4566\u001b[0m  0.2243\n",
      "      7        \u001b[36m0.4478\u001b[0m       0.4734        \u001b[35m0.4498\u001b[0m  0.2248\n",
      "      8        \u001b[36m0.4412\u001b[0m       0.4734        \u001b[35m0.4437\u001b[0m  0.2219\n",
      "      9        \u001b[36m0.4352\u001b[0m       0.4734        \u001b[35m0.4380\u001b[0m  0.2239\n",
      "     10        \u001b[36m0.4298\u001b[0m       0.4734        \u001b[35m0.4329\u001b[0m  0.2287\n",
      "     11        \u001b[36m0.4247\u001b[0m       0.4734        \u001b[35m0.4281\u001b[0m  0.2237\n",
      "     12        \u001b[36m0.4201\u001b[0m       0.4734        \u001b[35m0.4237\u001b[0m  0.2454\n",
      "     13        \u001b[36m0.4158\u001b[0m       0.4734        \u001b[35m0.4196\u001b[0m  0.2594\n",
      "     14        \u001b[36m0.4118\u001b[0m       0.4734        \u001b[35m0.4158\u001b[0m  0.2596\n",
      "     15        \u001b[36m0.4082\u001b[0m       0.4734        \u001b[35m0.4123\u001b[0m  0.3113\n",
      "     16        \u001b[36m0.4047\u001b[0m       0.4734        \u001b[35m0.4090\u001b[0m  0.2653\n",
      "     17        \u001b[36m0.4015\u001b[0m       0.4734        \u001b[35m0.4059\u001b[0m  0.2410\n",
      "     18        \u001b[36m0.3985\u001b[0m       0.4734        \u001b[35m0.4031\u001b[0m  0.2185\n",
      "     19        \u001b[36m0.3957\u001b[0m       0.4734        \u001b[35m0.4003\u001b[0m  0.2577\n",
      "     20        \u001b[36m0.3930\u001b[0m       0.4734        \u001b[35m0.3978\u001b[0m  0.2332\n",
      "     21        \u001b[36m0.3905\u001b[0m       0.4734        \u001b[35m0.3954\u001b[0m  0.2259\n",
      "     22        \u001b[36m0.3881\u001b[0m       0.4734        \u001b[35m0.3931\u001b[0m  0.2366\n",
      "     23        \u001b[36m0.3859\u001b[0m       0.4734        \u001b[35m0.3909\u001b[0m  0.2234\n",
      "     24        \u001b[36m0.3837\u001b[0m       0.4734        \u001b[35m0.3888\u001b[0m  0.2175\n",
      "     25        \u001b[36m0.3817\u001b[0m       0.4734        \u001b[35m0.3868\u001b[0m  0.2195\n",
      "     26        \u001b[36m0.3797\u001b[0m       0.4734        \u001b[35m0.3850\u001b[0m  0.2215\n",
      "     27        \u001b[36m0.3779\u001b[0m       0.4734        \u001b[35m0.3831\u001b[0m  0.2183\n",
      "     28        \u001b[36m0.3761\u001b[0m       0.4734        \u001b[35m0.3814\u001b[0m  0.2301\n",
      "     29        \u001b[36m0.3743\u001b[0m       0.4734        \u001b[35m0.3797\u001b[0m  0.2159\n",
      "     30        \u001b[36m0.3726\u001b[0m       0.4734        \u001b[35m0.3781\u001b[0m  0.2301\n",
      "     31        \u001b[36m0.3710\u001b[0m       0.4734        \u001b[35m0.3765\u001b[0m  0.2238\n",
      "     32        \u001b[36m0.3694\u001b[0m       0.4734        \u001b[35m0.3749\u001b[0m  0.2190\n",
      "     33        \u001b[36m0.3679\u001b[0m       0.4734        \u001b[35m0.3734\u001b[0m  0.2541\n",
      "     34        \u001b[36m0.3664\u001b[0m       0.4734        \u001b[35m0.3719\u001b[0m  0.2188\n",
      "     35        \u001b[36m0.3649\u001b[0m       0.4734        \u001b[35m0.3705\u001b[0m  0.2240\n",
      "     36        \u001b[36m0.3634\u001b[0m       0.4734        \u001b[35m0.3691\u001b[0m  0.2341\n",
      "     37        \u001b[36m0.3620\u001b[0m       0.4734        \u001b[35m0.3677\u001b[0m  0.2237\n",
      "     38        \u001b[36m0.3606\u001b[0m       0.4734        \u001b[35m0.3663\u001b[0m  0.2163\n",
      "     39        \u001b[36m0.3592\u001b[0m       0.4734        \u001b[35m0.3649\u001b[0m  0.2186\n",
      "     40        \u001b[36m0.3577\u001b[0m       0.4734        \u001b[35m0.3635\u001b[0m  0.2260\n",
      "     41        \u001b[36m0.3563\u001b[0m       0.4734        \u001b[35m0.3621\u001b[0m  0.2191\n",
      "     42        \u001b[36m0.3549\u001b[0m       0.4734        \u001b[35m0.3606\u001b[0m  0.2233\n",
      "     43        \u001b[36m0.3534\u001b[0m       0.4734        \u001b[35m0.3591\u001b[0m  0.2167\n",
      "     44        \u001b[36m0.3518\u001b[0m       0.4734        \u001b[35m0.3576\u001b[0m  0.2185\n",
      "     45        \u001b[36m0.3503\u001b[0m       0.4734        \u001b[35m0.3559\u001b[0m  0.2242\n",
      "     46        \u001b[36m0.3486\u001b[0m       0.4734        \u001b[35m0.3542\u001b[0m  0.2203\n",
      "     47        \u001b[36m0.3468\u001b[0m       0.4734        \u001b[35m0.3524\u001b[0m  0.2328\n",
      "     48        \u001b[36m0.3450\u001b[0m       0.4734        \u001b[35m0.3504\u001b[0m  0.2191\n",
      "     49        \u001b[36m0.3430\u001b[0m       0.4734        \u001b[35m0.3483\u001b[0m  0.2257\n",
      "     50        \u001b[36m0.3410\u001b[0m       0.4734        \u001b[35m0.3460\u001b[0m  0.2207\n",
      "Xtrain pre augmentation: (12942, 6)\n",
      "Ytrain pre augmentation (12942, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain: (18930, 6)\n",
      "Ytrain (18930, 1)\n",
      "Xtest (3235, 6)\n",
      "Ytest (3235, 1)\n",
      "Re-initializing module.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.5922\u001b[0m       \u001b[32m0.4694\u001b[0m        \u001b[35m0.6294\u001b[0m  0.2009\n",
      "      2        \u001b[36m0.5351\u001b[0m       0.4694        \u001b[35m0.5175\u001b[0m  0.2727\n",
      "      3        \u001b[36m0.5066\u001b[0m       0.4694        \u001b[35m0.4832\u001b[0m  0.2123\n",
      "      4        \u001b[36m0.4858\u001b[0m       0.4694        \u001b[35m0.4652\u001b[0m  0.2224\n",
      "      5        \u001b[36m0.4685\u001b[0m       0.4694        \u001b[35m0.4506\u001b[0m  0.2154\n",
      "      6        \u001b[36m0.4535\u001b[0m       0.4694        \u001b[35m0.4377\u001b[0m  0.2141\n",
      "      7        \u001b[36m0.4400\u001b[0m       0.4694        \u001b[35m0.4263\u001b[0m  0.2129\n",
      "      8        \u001b[36m0.4278\u001b[0m       0.4694        \u001b[35m0.4158\u001b[0m  0.2149\n",
      "      9        \u001b[36m0.4164\u001b[0m       0.4694        \u001b[35m0.4061\u001b[0m  0.2171\n",
      "     10        \u001b[36m0.4046\u001b[0m       0.4694        \u001b[35m0.3971\u001b[0m  0.2213\n",
      "     11        \u001b[36m0.3918\u001b[0m       0.4694        \u001b[35m0.3896\u001b[0m  0.2198\n",
      "     12        \u001b[36m0.3810\u001b[0m       0.4694        \u001b[35m0.3841\u001b[0m  0.2221\n",
      "     13        \u001b[36m0.3745\u001b[0m       0.4694        \u001b[35m0.3790\u001b[0m  0.2289\n",
      "     14        \u001b[36m0.3690\u001b[0m       0.4694        \u001b[35m0.3745\u001b[0m  0.2197\n",
      "     15        \u001b[36m0.3641\u001b[0m       0.4694        \u001b[35m0.3702\u001b[0m  0.2264\n",
      "     16        \u001b[36m0.3596\u001b[0m       0.4694        \u001b[35m0.3662\u001b[0m  0.2207\n",
      "     17        \u001b[36m0.3554\u001b[0m       0.4694        \u001b[35m0.3625\u001b[0m  0.2285\n",
      "     18        \u001b[36m0.3515\u001b[0m       0.4694        \u001b[35m0.3590\u001b[0m  0.2204\n",
      "     19        \u001b[36m0.3477\u001b[0m       0.4694        \u001b[35m0.3557\u001b[0m  0.2189\n",
      "     20        \u001b[36m0.3442\u001b[0m       0.4694        \u001b[35m0.3525\u001b[0m  0.2218\n",
      "     21        \u001b[36m0.3409\u001b[0m       0.4694        \u001b[35m0.3494\u001b[0m  0.2191\n",
      "     22        \u001b[36m0.3377\u001b[0m       0.4694        \u001b[35m0.3465\u001b[0m  0.2178\n",
      "     23        \u001b[36m0.3346\u001b[0m       0.4694        \u001b[35m0.3438\u001b[0m  0.2174\n",
      "     24        \u001b[36m0.3318\u001b[0m       0.4694        \u001b[35m0.3412\u001b[0m  0.2111\n",
      "     25        \u001b[36m0.3290\u001b[0m       0.4694        \u001b[35m0.3387\u001b[0m  0.2145\n",
      "     26        \u001b[36m0.3264\u001b[0m       0.4694        \u001b[35m0.3363\u001b[0m  0.4232\n",
      "     27        \u001b[36m0.3239\u001b[0m       0.4694        \u001b[35m0.3340\u001b[0m  0.3141\n",
      "     28        \u001b[36m0.3215\u001b[0m       0.4694        \u001b[35m0.3318\u001b[0m  0.2501\n",
      "     29        \u001b[36m0.3191\u001b[0m       0.4694        \u001b[35m0.3297\u001b[0m  0.2694\n",
      "     30        \u001b[36m0.3169\u001b[0m       0.4694        \u001b[35m0.3276\u001b[0m  0.2906\n",
      "     31        \u001b[36m0.3147\u001b[0m       0.4694        \u001b[35m0.3256\u001b[0m  0.2989\n",
      "     32        \u001b[36m0.3127\u001b[0m       0.4694        \u001b[35m0.3237\u001b[0m  0.2852\n",
      "     33        \u001b[36m0.3106\u001b[0m       0.4694        \u001b[35m0.3219\u001b[0m  0.2843\n",
      "     34        \u001b[36m0.3087\u001b[0m       0.4694        \u001b[35m0.3200\u001b[0m  0.3053\n",
      "     35        \u001b[36m0.3068\u001b[0m       0.4694        \u001b[35m0.3183\u001b[0m  0.3499\n",
      "     36        \u001b[36m0.3050\u001b[0m       0.4694        \u001b[35m0.3166\u001b[0m  0.2868\n",
      "     37        \u001b[36m0.3032\u001b[0m       0.4694        \u001b[35m0.3150\u001b[0m  0.2555\n",
      "     38        \u001b[36m0.3015\u001b[0m       0.4694        \u001b[35m0.3134\u001b[0m  0.2599\n",
      "     39        \u001b[36m0.2998\u001b[0m       0.4694        \u001b[35m0.3119\u001b[0m  0.2603\n",
      "     40        \u001b[36m0.2982\u001b[0m       0.4694        \u001b[35m0.3104\u001b[0m  0.3524\n",
      "     41        \u001b[36m0.2967\u001b[0m       0.4694        \u001b[35m0.3089\u001b[0m  0.2605\n",
      "     42        \u001b[36m0.2951\u001b[0m       0.4694        \u001b[35m0.3075\u001b[0m  0.2327\n",
      "     43        \u001b[36m0.2937\u001b[0m       0.4694        \u001b[35m0.3061\u001b[0m  0.2457\n",
      "     44        \u001b[36m0.2922\u001b[0m       0.4694        \u001b[35m0.3047\u001b[0m  0.2477\n",
      "     45        \u001b[36m0.2908\u001b[0m       0.4694        \u001b[35m0.3034\u001b[0m  0.2350\n",
      "     46        \u001b[36m0.2894\u001b[0m       0.4694        \u001b[35m0.3022\u001b[0m  0.2539\n",
      "     47        \u001b[36m0.2881\u001b[0m       0.4694        \u001b[35m0.3009\u001b[0m  0.2342\n",
      "     48        \u001b[36m0.2868\u001b[0m       0.4694        \u001b[35m0.2997\u001b[0m  0.2373\n",
      "     49        \u001b[36m0.2856\u001b[0m       0.4694        \u001b[35m0.2986\u001b[0m  0.2318\n",
      "     50        \u001b[36m0.2844\u001b[0m       0.4694        \u001b[35m0.2974\u001b[0m  0.2288\n",
      "Xtrain pre augmentation: (2140, 6)\n",
      "Ytrain pre augmentation (2140, 1)\n",
      "Xtrain: (3681, 6)\n",
      "Ytrain (3681, 1)\n",
      "Xtest (535, 6)\n",
      "Ytest (535, 1)\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.8002\u001b[0m       \u001b[32m0.4912\u001b[0m        \u001b[35m0.7183\u001b[0m  0.0467\n",
      "      2        \u001b[36m0.7884\u001b[0m       0.4912        0.7332  0.0579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      3        \u001b[36m0.7780\u001b[0m       0.4912        0.7492  0.0594\n",
      "      4        \u001b[36m0.7694\u001b[0m       0.4912        0.7628  0.0627\n",
      "      5        \u001b[36m0.7619\u001b[0m       0.4912        0.7744  0.0547\n",
      "      6        \u001b[36m0.7550\u001b[0m       0.4912        0.7808  0.0583\n",
      "      7        \u001b[36m0.7478\u001b[0m       0.4912        0.7823  0.0569\n",
      "      8        \u001b[36m0.7403\u001b[0m       0.4912        0.7800  0.0593\n",
      "      9        \u001b[36m0.7333\u001b[0m       0.4912        0.7757  0.0585\n",
      "     10        \u001b[36m0.7265\u001b[0m       0.4912        0.7717  0.0561\n",
      "     11        \u001b[36m0.7201\u001b[0m       0.4912        0.8780  0.0598\n",
      "     12        \u001b[36m0.7137\u001b[0m       0.4912        0.8689  0.0552\n",
      "     13        \u001b[36m0.7078\u001b[0m       0.4912        0.8608  0.0566\n",
      "     14        \u001b[36m0.7021\u001b[0m       0.4912        0.8534  0.0561\n",
      "     15        \u001b[36m0.6963\u001b[0m       0.4912        0.8468  0.0598\n",
      "     16        \u001b[36m0.6908\u001b[0m       0.4912        0.8410  0.0554\n",
      "     17        \u001b[36m0.6853\u001b[0m       0.4912        0.8356  0.0584\n",
      "     18        \u001b[36m0.6797\u001b[0m       0.4912        0.8307  0.0547\n",
      "     19        \u001b[36m0.6740\u001b[0m       0.4912        0.8258  0.0607\n",
      "     20        \u001b[36m0.6679\u001b[0m       0.4912        0.8208  0.0548\n",
      "     21        \u001b[36m0.6617\u001b[0m       0.4912        0.8158  0.0585\n",
      "     22        \u001b[36m0.6552\u001b[0m       0.4912        0.8104  0.0654\n",
      "     23        \u001b[36m0.6486\u001b[0m       0.4912        0.8050  0.0605\n",
      "     24        \u001b[36m0.6417\u001b[0m       0.4912        0.7994  0.0555\n",
      "     25        \u001b[36m0.6348\u001b[0m       0.4912        0.7935  0.0598\n",
      "     26        \u001b[36m0.6277\u001b[0m       0.4912        0.7877  0.0598\n",
      "     27        \u001b[36m0.6207\u001b[0m       0.4912        0.7820  0.0844\n",
      "     28        \u001b[36m0.6135\u001b[0m       0.4912        0.7763  0.1064\n",
      "     29        \u001b[36m0.6064\u001b[0m       0.4912        0.7706  0.0604\n",
      "     30        \u001b[36m0.5992\u001b[0m       0.4912        0.7649  0.0656\n",
      "     31        \u001b[36m0.5919\u001b[0m       0.4912        0.7595  0.0537\n",
      "     32        \u001b[36m0.5844\u001b[0m       0.4912        0.7537  0.0582\n",
      "     33        \u001b[36m0.5772\u001b[0m       0.4912        0.7478  0.1061\n",
      "     34        \u001b[36m0.5703\u001b[0m       0.4912        0.7413  0.0539\n",
      "     35        \u001b[36m0.5638\u001b[0m       0.4912        0.7347  0.0573\n",
      "     36        \u001b[36m0.5576\u001b[0m       0.4912        0.7280  0.0546\n",
      "     37        \u001b[36m0.5517\u001b[0m       0.4912        0.7192  0.0561\n",
      "     38        \u001b[36m0.5465\u001b[0m       0.4912        \u001b[35m0.7103\u001b[0m  0.0542\n",
      "     39        \u001b[36m0.5413\u001b[0m       0.4912        \u001b[35m0.7029\u001b[0m  0.0552\n",
      "     40        \u001b[36m0.5360\u001b[0m       0.4912        \u001b[35m0.6965\u001b[0m  0.0540\n",
      "     41        \u001b[36m0.5308\u001b[0m       0.4912        \u001b[35m0.6902\u001b[0m  0.0552\n",
      "     42        \u001b[36m0.5258\u001b[0m       0.4912        \u001b[35m0.6844\u001b[0m  0.0523\n",
      "     43        \u001b[36m0.5210\u001b[0m       0.4912        \u001b[35m0.6790\u001b[0m  0.0643\n",
      "     44        \u001b[36m0.5162\u001b[0m       0.4912        \u001b[35m0.6739\u001b[0m  0.0532\n",
      "     45        \u001b[36m0.5116\u001b[0m       0.4912        \u001b[35m0.6689\u001b[0m  0.0534\n",
      "     46        \u001b[36m0.5073\u001b[0m       0.4912        \u001b[35m0.6643\u001b[0m  0.0540\n",
      "     47        \u001b[36m0.5032\u001b[0m       0.4912        \u001b[35m0.6597\u001b[0m  0.0546\n",
      "     48        \u001b[36m0.4994\u001b[0m       0.4912        \u001b[35m0.6553\u001b[0m  0.0551\n",
      "     49        \u001b[36m0.4958\u001b[0m       0.4912        \u001b[35m0.6512\u001b[0m  0.0552\n",
      "     50        \u001b[36m0.4923\u001b[0m       0.4912        \u001b[35m0.6473\u001b[0m  0.0547\n",
      "Xtrain pre augmentation: (2140, 6)\n",
      "Ytrain pre augmentation (2140, 1)\n",
      "Xtrain: (3688, 6)\n",
      "Ytrain (3688, 1)\n",
      "Xtest (535, 6)\n",
      "Ytest (535, 1)\n",
      "Re-initializing module.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7113\u001b[0m       \u001b[32m0.4932\u001b[0m        \u001b[35m0.6828\u001b[0m  0.0516\n",
      "      2        \u001b[36m0.6833\u001b[0m       0.4932        \u001b[35m0.6817\u001b[0m  0.0565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2457: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      3        \u001b[36m0.6665\u001b[0m       0.4932        0.6850  0.0570\n",
      "      4        \u001b[36m0.6548\u001b[0m       0.4932        \u001b[35m0.6798\u001b[0m  0.0597\n",
      "      5        \u001b[36m0.6454\u001b[0m       0.4932        \u001b[35m0.6657\u001b[0m  0.0627\n",
      "      6        \u001b[36m0.6378\u001b[0m       0.4932        \u001b[35m0.6509\u001b[0m  0.0569\n",
      "      7        \u001b[36m0.6313\u001b[0m       0.4932        \u001b[35m0.6390\u001b[0m  0.0577\n",
      "      8        \u001b[36m0.6253\u001b[0m       0.4932        \u001b[35m0.6312\u001b[0m  0.0586\n",
      "      9        \u001b[36m0.6196\u001b[0m       0.4932        \u001b[35m0.6258\u001b[0m  0.0628\n",
      "     10        \u001b[36m0.6142\u001b[0m       0.4932        \u001b[35m0.6212\u001b[0m  0.0595\n",
      "     11        \u001b[36m0.6090\u001b[0m       0.4932        \u001b[35m0.6171\u001b[0m  0.0589\n",
      "     12        \u001b[36m0.6039\u001b[0m       0.4932        \u001b[35m0.6118\u001b[0m  0.0543\n",
      "     13        \u001b[36m0.5985\u001b[0m       0.4932        \u001b[35m0.6054\u001b[0m  0.0593\n",
      "     14        \u001b[36m0.5931\u001b[0m       0.4932        \u001b[35m0.5988\u001b[0m  0.0562\n",
      "     15        \u001b[36m0.5877\u001b[0m       0.4932        \u001b[35m0.5929\u001b[0m  0.0587\n",
      "     16        \u001b[36m0.5828\u001b[0m       0.4932        \u001b[35m0.5878\u001b[0m  0.0581\n",
      "     17        \u001b[36m0.5781\u001b[0m       0.4932        \u001b[35m0.5830\u001b[0m  0.0568\n",
      "     18        \u001b[36m0.5736\u001b[0m       0.4932        \u001b[35m0.5784\u001b[0m  0.0575\n",
      "     19        \u001b[36m0.5693\u001b[0m       0.4932        \u001b[35m0.5743\u001b[0m  0.0570\n",
      "     20        \u001b[36m0.5652\u001b[0m       0.4932        \u001b[35m0.5708\u001b[0m  0.0607\n",
      "     21        \u001b[36m0.5612\u001b[0m       0.4932        \u001b[35m0.5670\u001b[0m  0.0581\n",
      "     22        \u001b[36m0.5573\u001b[0m       0.4932        \u001b[35m0.5631\u001b[0m  0.0583\n",
      "     23        \u001b[36m0.5535\u001b[0m       0.4932        \u001b[35m0.5597\u001b[0m  0.0606\n",
      "     24        \u001b[36m0.5497\u001b[0m       0.4932        \u001b[35m0.5563\u001b[0m  0.0644\n",
      "     25        \u001b[36m0.5461\u001b[0m       0.4932        \u001b[35m0.5528\u001b[0m  0.0681\n",
      "     26        \u001b[36m0.5425\u001b[0m       0.4932        \u001b[35m0.5492\u001b[0m  0.0574\n",
      "     27        \u001b[36m0.5389\u001b[0m       0.4932        \u001b[35m0.5458\u001b[0m  0.1146\n",
      "     28        \u001b[36m0.5355\u001b[0m       0.4932        \u001b[35m0.5425\u001b[0m  0.0615\n",
      "     29        \u001b[36m0.5320\u001b[0m       0.4932        \u001b[35m0.5391\u001b[0m  0.0662\n",
      "     30        \u001b[36m0.5287\u001b[0m       0.4932        \u001b[35m0.5355\u001b[0m  0.0597\n",
      "     31        \u001b[36m0.5253\u001b[0m       0.4932        \u001b[35m0.5325\u001b[0m  0.0569\n",
      "     32        \u001b[36m0.5221\u001b[0m       0.4932        \u001b[35m0.5296\u001b[0m  0.0638\n",
      "     33        \u001b[36m0.5188\u001b[0m       0.4932        \u001b[35m0.5264\u001b[0m  0.0586\n",
      "     34        \u001b[36m0.5156\u001b[0m       0.4932        \u001b[35m0.5240\u001b[0m  0.0561\n",
      "     35        \u001b[36m0.5124\u001b[0m       0.4932        \u001b[35m0.5208\u001b[0m  0.0576\n",
      "     36        \u001b[36m0.5092\u001b[0m       0.4932        \u001b[35m0.5177\u001b[0m  0.0562\n",
      "     37        \u001b[36m0.5062\u001b[0m       0.4932        \u001b[35m0.5147\u001b[0m  0.0529\n",
      "     38        \u001b[36m0.5031\u001b[0m       0.4932        \u001b[35m0.5118\u001b[0m  0.0592\n",
      "     39        \u001b[36m0.5002\u001b[0m       0.4932        \u001b[35m0.5088\u001b[0m  0.0541\n",
      "     40        \u001b[36m0.4973\u001b[0m       0.4932        \u001b[35m0.5058\u001b[0m  0.0572\n",
      "     41        \u001b[36m0.4944\u001b[0m       0.4932        \u001b[35m0.5028\u001b[0m  0.0661\n",
      "     42        \u001b[36m0.4917\u001b[0m       0.4932        \u001b[35m0.5001\u001b[0m  0.0561\n",
      "     43        \u001b[36m0.4889\u001b[0m       0.4932        \u001b[35m0.4974\u001b[0m  0.0582\n",
      "     44        \u001b[36m0.4862\u001b[0m       0.4932        \u001b[35m0.4945\u001b[0m  0.0590\n",
      "     45        \u001b[36m0.4837\u001b[0m       0.4932        \u001b[35m0.4922\u001b[0m  0.0569\n",
      "     46        \u001b[36m0.4811\u001b[0m       0.4932        \u001b[35m0.4896\u001b[0m  0.0551\n",
      "     47        \u001b[36m0.4786\u001b[0m       0.4932        \u001b[35m0.4875\u001b[0m  0.0567\n",
      "     48        \u001b[36m0.4762\u001b[0m       0.4932        \u001b[35m0.4852\u001b[0m  0.0541\n",
      "     49        \u001b[36m0.4737\u001b[0m       0.4932        \u001b[35m0.4830\u001b[0m  0.0565\n",
      "     50        \u001b[36m0.4714\u001b[0m       0.4932        \u001b[35m0.4807\u001b[0m  0.0591\n",
      "Xtrain pre augmentation: (2140, 6)\n",
      "Ytrain pre augmentation (2140, 1)\n",
      "Xtrain: (3670, 6)\n",
      "Ytrain (3670, 1)\n",
      "Xtest (535, 6)\n",
      "Ytest (535, 1)\n",
      "Re-initializing module.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7427\u001b[0m       \u001b[32m0.4986\u001b[0m        \u001b[35m0.7295\u001b[0m  0.0513\n",
      "      2        \u001b[36m0.7227\u001b[0m       0.4986        \u001b[35m0.7181\u001b[0m  0.0566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      3        \u001b[36m0.7068\u001b[0m       0.4986        \u001b[35m0.7111\u001b[0m  0.0595\n",
      "      4        \u001b[36m0.6949\u001b[0m       0.4986        \u001b[35m0.7007\u001b[0m  0.0668\n",
      "      5        \u001b[36m0.6854\u001b[0m       0.4986        \u001b[35m0.6875\u001b[0m  0.0635\n",
      "      6        \u001b[36m0.6775\u001b[0m       0.4986        \u001b[35m0.6765\u001b[0m  0.0590\n",
      "      7        \u001b[36m0.6703\u001b[0m       0.4986        \u001b[35m0.6673\u001b[0m  0.0605\n",
      "      8        \u001b[36m0.6637\u001b[0m       0.4986        \u001b[35m0.6589\u001b[0m  0.0597\n",
      "      9        \u001b[36m0.6576\u001b[0m       0.4986        \u001b[35m0.6515\u001b[0m  0.0574\n",
      "     10        \u001b[36m0.6519\u001b[0m       0.4986        \u001b[35m0.6451\u001b[0m  0.0598\n",
      "     11        \u001b[36m0.6465\u001b[0m       0.4986        \u001b[35m0.6392\u001b[0m  0.0613\n",
      "     12        \u001b[36m0.6414\u001b[0m       0.4986        \u001b[35m0.6339\u001b[0m  0.0582\n",
      "     13        \u001b[36m0.6365\u001b[0m       0.4986        \u001b[35m0.6290\u001b[0m  0.0593\n",
      "     14        \u001b[36m0.6318\u001b[0m       0.4986        \u001b[35m0.6243\u001b[0m  0.0594\n",
      "     15        \u001b[36m0.6273\u001b[0m       0.4986        \u001b[35m0.6199\u001b[0m  0.0609\n",
      "     16        \u001b[36m0.6230\u001b[0m       0.4986        \u001b[35m0.6158\u001b[0m  0.0587\n",
      "     17        \u001b[36m0.6189\u001b[0m       0.4986        \u001b[35m0.6118\u001b[0m  0.0607\n",
      "     18        \u001b[36m0.6150\u001b[0m       0.4986        \u001b[35m0.6080\u001b[0m  0.0611\n",
      "     19        \u001b[36m0.6112\u001b[0m       0.4986        \u001b[35m0.6044\u001b[0m  0.0588\n",
      "     20        \u001b[36m0.6076\u001b[0m       0.4986        \u001b[35m0.6009\u001b[0m  0.1091\n",
      "     21        \u001b[36m0.6041\u001b[0m       0.4986        \u001b[35m0.5975\u001b[0m  0.0629\n",
      "     22        \u001b[36m0.6007\u001b[0m       0.4986        \u001b[35m0.5942\u001b[0m  0.0620\n",
      "     23        \u001b[36m0.5973\u001b[0m       0.4986        \u001b[35m0.5910\u001b[0m  0.0639\n",
      "     24        \u001b[36m0.5941\u001b[0m       0.4986        \u001b[35m0.5878\u001b[0m  0.0610\n",
      "     25        \u001b[36m0.5910\u001b[0m       0.4986        \u001b[35m0.5848\u001b[0m  0.0596\n",
      "     26        \u001b[36m0.5879\u001b[0m       0.4986        \u001b[35m0.5818\u001b[0m  0.0584\n",
      "     27        \u001b[36m0.5849\u001b[0m       0.4986        \u001b[35m0.5788\u001b[0m  0.0622\n",
      "     28        \u001b[36m0.5820\u001b[0m       0.4986        \u001b[35m0.5759\u001b[0m  0.0633\n",
      "     29        \u001b[36m0.5791\u001b[0m       0.4986        \u001b[35m0.5731\u001b[0m  0.0837\n",
      "     30        \u001b[36m0.5763\u001b[0m       0.4986        \u001b[35m0.5703\u001b[0m  0.0727\n",
      "     31        \u001b[36m0.5735\u001b[0m       0.4986        \u001b[35m0.5676\u001b[0m  0.0625\n",
      "     32        \u001b[36m0.5708\u001b[0m       0.4986        \u001b[35m0.5649\u001b[0m  0.0633\n",
      "     33        \u001b[36m0.5680\u001b[0m       0.4986        \u001b[35m0.5622\u001b[0m  0.0642\n",
      "     34        \u001b[36m0.5654\u001b[0m       0.4986        \u001b[35m0.5596\u001b[0m  0.0526\n",
      "     35        \u001b[36m0.5628\u001b[0m       0.4986        \u001b[35m0.5570\u001b[0m  0.0552\n",
      "     36        \u001b[36m0.5602\u001b[0m       0.4986        \u001b[35m0.5544\u001b[0m  0.0578\n",
      "     37        \u001b[36m0.5576\u001b[0m       0.4986        \u001b[35m0.5519\u001b[0m  0.0552\n",
      "     38        \u001b[36m0.5550\u001b[0m       0.4986        \u001b[35m0.5493\u001b[0m  0.0563\n",
      "     39        \u001b[36m0.5524\u001b[0m       0.4986        \u001b[35m0.5468\u001b[0m  0.0541\n",
      "     40        \u001b[36m0.5499\u001b[0m       0.4986        \u001b[35m0.5443\u001b[0m  0.0581\n",
      "     41        \u001b[36m0.5474\u001b[0m       0.4986        \u001b[35m0.5418\u001b[0m  0.0535\n",
      "     42        \u001b[36m0.5448\u001b[0m       0.4986        \u001b[35m0.5394\u001b[0m  0.0549\n",
      "     43        \u001b[36m0.5423\u001b[0m       0.4986        \u001b[35m0.5369\u001b[0m  0.0541\n",
      "     44        \u001b[36m0.5398\u001b[0m       0.4986        \u001b[35m0.5344\u001b[0m  0.0544\n",
      "     45        \u001b[36m0.5373\u001b[0m       0.4986        \u001b[35m0.5319\u001b[0m  0.0565\n",
      "     46        \u001b[36m0.5348\u001b[0m       0.4986        \u001b[35m0.5295\u001b[0m  0.0533\n",
      "     47        \u001b[36m0.5323\u001b[0m       0.4986        \u001b[35m0.5270\u001b[0m  0.0561\n",
      "     48        \u001b[36m0.5297\u001b[0m       0.4986        \u001b[35m0.5245\u001b[0m  0.0583\n",
      "     49        \u001b[36m0.5272\u001b[0m       0.4986        \u001b[35m0.5219\u001b[0m  0.0582\n",
      "     50        \u001b[36m0.5246\u001b[0m       0.4986        \u001b[35m0.5194\u001b[0m  0.0554\n",
      "Xtrain pre augmentation: (2140, 6)\n",
      "Ytrain pre augmentation (2140, 1)\n",
      "Xtrain: (3601, 6)\n",
      "Ytrain (3601, 1)\n",
      "Xtest (535, 6)\n",
      "Ytest (535, 1)\n",
      "Re-initializing module.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.9205\u001b[0m       \u001b[32m0.4979\u001b[0m        \u001b[35m0.8558\u001b[0m  0.0528\n",
      "      2        \u001b[36m0.9061\u001b[0m       0.4979        \u001b[35m0.8391\u001b[0m  0.0550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      3        \u001b[36m0.8973\u001b[0m       0.4979        0.8433  0.0584\n",
      "      4        \u001b[36m0.8896\u001b[0m       0.4979        0.8504  0.0641\n",
      "      5        \u001b[36m0.8827\u001b[0m       0.4979        0.8554  0.0612\n",
      "      6        \u001b[36m0.8762\u001b[0m       0.4979        0.8579  0.0608\n",
      "      7        \u001b[36m0.8696\u001b[0m       0.4979        0.8573  0.0610\n",
      "      8        \u001b[36m0.8625\u001b[0m       0.4979        0.8543  0.0591\n",
      "      9        \u001b[36m0.8546\u001b[0m       0.4979        0.8492  0.0614\n",
      "     10        \u001b[36m0.8454\u001b[0m       0.4979        0.8422  0.0623\n",
      "     11        \u001b[36m0.8357\u001b[0m       0.4979        \u001b[35m0.8349\u001b[0m  0.0552\n",
      "     12        \u001b[36m0.8259\u001b[0m       0.4979        \u001b[35m0.8278\u001b[0m  0.0588\n",
      "     13        \u001b[36m0.8171\u001b[0m       0.4979        \u001b[35m0.8228\u001b[0m  0.0608\n",
      "     14        \u001b[36m0.8101\u001b[0m       0.4979        \u001b[35m0.8194\u001b[0m  0.0566\n",
      "     15        \u001b[36m0.8052\u001b[0m       0.4979        \u001b[35m0.8163\u001b[0m  0.0579\n",
      "     16        \u001b[36m0.8012\u001b[0m       0.4979        \u001b[35m0.8130\u001b[0m  0.0600\n",
      "     17        \u001b[36m0.7976\u001b[0m       0.4979        \u001b[35m0.8092\u001b[0m  0.0620\n",
      "     18        \u001b[36m0.7941\u001b[0m       0.4979        \u001b[35m0.8057\u001b[0m  0.0571\n",
      "     19        \u001b[36m0.7906\u001b[0m       0.4979        \u001b[35m0.8023\u001b[0m  0.0601\n",
      "     20        \u001b[36m0.7872\u001b[0m       0.4979        \u001b[35m0.7990\u001b[0m  0.0593\n",
      "     21        \u001b[36m0.7839\u001b[0m       0.4979        \u001b[35m0.7957\u001b[0m  0.0570\n",
      "     22        \u001b[36m0.7806\u001b[0m       0.4979        \u001b[35m0.7924\u001b[0m  0.1193\n",
      "     23        \u001b[36m0.7775\u001b[0m       0.4979        \u001b[35m0.7893\u001b[0m  0.0582\n",
      "     24        \u001b[36m0.7744\u001b[0m       0.4979        \u001b[35m0.7863\u001b[0m  0.0573\n",
      "     25        \u001b[36m0.7714\u001b[0m       0.4979        \u001b[35m0.7833\u001b[0m  0.0584\n",
      "     26        \u001b[36m0.7685\u001b[0m       0.4979        \u001b[35m0.7804\u001b[0m  0.0577\n",
      "     27        \u001b[36m0.7656\u001b[0m       0.4979        \u001b[35m0.7775\u001b[0m  0.0574\n",
      "     28        \u001b[36m0.7627\u001b[0m       0.4979        \u001b[35m0.7746\u001b[0m  0.0710\n",
      "     29        \u001b[36m0.7599\u001b[0m       0.4979        \u001b[35m0.7717\u001b[0m  0.0586\n",
      "     30        \u001b[36m0.7570\u001b[0m       0.4979        \u001b[35m0.7688\u001b[0m  0.0609\n",
      "     31        \u001b[36m0.7541\u001b[0m       0.4979        \u001b[35m0.7659\u001b[0m  0.0691\n",
      "     32        \u001b[36m0.7513\u001b[0m       0.4979        \u001b[35m0.7631\u001b[0m  0.0660\n",
      "     33        \u001b[36m0.7485\u001b[0m       0.4979        \u001b[35m0.7603\u001b[0m  0.0631\n",
      "     34        \u001b[36m0.7457\u001b[0m       0.4979        \u001b[35m0.7576\u001b[0m  0.0529\n",
      "     35        \u001b[36m0.7430\u001b[0m       0.4979        \u001b[35m0.7549\u001b[0m  0.0538\n",
      "     36        \u001b[36m0.7404\u001b[0m       0.4979        \u001b[35m0.7523\u001b[0m  0.0515\n",
      "     37        \u001b[36m0.7378\u001b[0m       0.4979        \u001b[35m0.7497\u001b[0m  0.0549\n",
      "     38        \u001b[36m0.7353\u001b[0m       0.4979        \u001b[35m0.7472\u001b[0m  0.0562\n",
      "     39        \u001b[36m0.7328\u001b[0m       0.4979        \u001b[35m0.7447\u001b[0m  0.0529\n",
      "     40        \u001b[36m0.7304\u001b[0m       0.4979        \u001b[35m0.7422\u001b[0m  0.0584\n",
      "     41        \u001b[36m0.7281\u001b[0m       0.4979        \u001b[35m0.7397\u001b[0m  0.0545\n",
      "     42        \u001b[36m0.7258\u001b[0m       0.4979        \u001b[35m0.7373\u001b[0m  0.0525\n",
      "     43        \u001b[36m0.7235\u001b[0m       0.4979        \u001b[35m0.7349\u001b[0m  0.0546\n",
      "     44        \u001b[36m0.7212\u001b[0m       0.4979        \u001b[35m0.7325\u001b[0m  0.0537\n",
      "     45        \u001b[36m0.7190\u001b[0m       0.4979        \u001b[35m0.7301\u001b[0m  0.0512\n",
      "     46        \u001b[36m0.7168\u001b[0m       0.4979        \u001b[35m0.7278\u001b[0m  0.0553\n",
      "     47        \u001b[36m0.7146\u001b[0m       0.4979        \u001b[35m0.7254\u001b[0m  0.0535\n",
      "     48        \u001b[36m0.7124\u001b[0m       0.4979        \u001b[35m0.7231\u001b[0m  0.0531\n",
      "     49        \u001b[36m0.7103\u001b[0m       0.4979        \u001b[35m0.7208\u001b[0m  0.0533\n",
      "     50        \u001b[36m0.7082\u001b[0m       0.4979        \u001b[35m0.7185\u001b[0m  0.0546\n",
      "Xtrain pre augmentation: (2140, 6)\n",
      "Ytrain pre augmentation (2140, 1)\n",
      "Xtrain: (3702, 6)\n",
      "Ytrain (3702, 1)\n",
      "Xtest (535, 6)\n",
      "Ytest (535, 1)\n",
      "Re-initializing module.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0641\u001b[0m       \u001b[32m0.4953\u001b[0m        \u001b[35m0.7413\u001b[0m  0.0525\n",
      "      2        \u001b[36m1.0026\u001b[0m       0.4953        0.7860  0.0594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      3        \u001b[36m0.9654\u001b[0m       0.4953        0.8372  0.0740\n",
      "      4        \u001b[36m0.9295\u001b[0m       0.4953        0.8816  0.0643\n",
      "      5        \u001b[36m0.8928\u001b[0m       0.4953        0.9037  0.0577\n",
      "      6        \u001b[36m0.8595\u001b[0m       0.4953        0.8885  0.0602\n",
      "      7        \u001b[36m0.8262\u001b[0m       0.4953        0.8441  0.0609\n",
      "      8        \u001b[36m0.7939\u001b[0m       0.4953        0.8051  0.0584\n",
      "      9        \u001b[36m0.7736\u001b[0m       0.4953        0.7726  0.0586\n",
      "     10        \u001b[36m0.7582\u001b[0m       0.4953        0.7526  0.0607\n",
      "     11        \u001b[36m0.7450\u001b[0m       0.4953        0.7423  0.0583\n",
      "     12        \u001b[36m0.7332\u001b[0m       0.4953        \u001b[35m0.7325\u001b[0m  0.0605\n",
      "     13        \u001b[36m0.7224\u001b[0m       0.4953        \u001b[35m0.7227\u001b[0m  0.0574\n",
      "     14        \u001b[36m0.7124\u001b[0m       0.4953        \u001b[35m0.7135\u001b[0m  0.0606\n",
      "     15        \u001b[36m0.7029\u001b[0m       0.4953        \u001b[35m0.7043\u001b[0m  0.0643\n",
      "     16        \u001b[36m0.6940\u001b[0m       0.4953        \u001b[35m0.6948\u001b[0m  0.0583\n",
      "     17        \u001b[36m0.6855\u001b[0m       0.4953        \u001b[35m0.6868\u001b[0m  0.0621\n",
      "     18        \u001b[36m0.6774\u001b[0m       0.4953        \u001b[35m0.6791\u001b[0m  0.0599\n",
      "     19        \u001b[36m0.6696\u001b[0m       0.4953        \u001b[35m0.6713\u001b[0m  0.0598\n",
      "     20        \u001b[36m0.6622\u001b[0m       0.4953        \u001b[35m0.6630\u001b[0m  0.0621\n",
      "     21        \u001b[36m0.6549\u001b[0m       0.4953        \u001b[35m0.6557\u001b[0m  0.0633\n",
      "     22        \u001b[36m0.6480\u001b[0m       0.4953        \u001b[35m0.6488\u001b[0m  0.0606\n",
      "     23        \u001b[36m0.6412\u001b[0m       0.4953        \u001b[35m0.6419\u001b[0m  0.0612\n",
      "     24        \u001b[36m0.6347\u001b[0m       0.4953        \u001b[35m0.6366\u001b[0m  0.0608\n",
      "     25        \u001b[36m0.6283\u001b[0m       0.4953        \u001b[35m0.6300\u001b[0m  0.0588\n",
      "     26        \u001b[36m0.6222\u001b[0m       0.4953        \u001b[35m0.6236\u001b[0m  0.0617\n",
      "     27        \u001b[36m0.6162\u001b[0m       0.4953        \u001b[35m0.6175\u001b[0m  0.0610\n",
      "     28        \u001b[36m0.6105\u001b[0m       0.4953        \u001b[35m0.6117\u001b[0m  0.0614\n",
      "     29        \u001b[36m0.6049\u001b[0m       0.4953        \u001b[35m0.6061\u001b[0m  0.0743\n",
      "     30        \u001b[36m0.5994\u001b[0m       0.4953        \u001b[35m0.6007\u001b[0m  0.0581\n",
      "     31        \u001b[36m0.5941\u001b[0m       0.4953        \u001b[35m0.5956\u001b[0m  0.0695\n",
      "     32        \u001b[36m0.5890\u001b[0m       0.4953        \u001b[35m0.5905\u001b[0m  0.0621\n",
      "     33        \u001b[36m0.5840\u001b[0m       0.4953        \u001b[35m0.5856\u001b[0m  0.0573\n",
      "     34        \u001b[36m0.5791\u001b[0m       0.4953        \u001b[35m0.5808\u001b[0m  0.0531\n",
      "     35        \u001b[36m0.5743\u001b[0m       0.4953        \u001b[35m0.5760\u001b[0m  0.0570\n",
      "     36        \u001b[36m0.5697\u001b[0m       0.4953        \u001b[35m0.5713\u001b[0m  0.0555\n",
      "     37        \u001b[36m0.5652\u001b[0m       0.4953        \u001b[35m0.5667\u001b[0m  0.0556\n",
      "     38        \u001b[36m0.5608\u001b[0m       0.4953        \u001b[35m0.5630\u001b[0m  0.0593\n",
      "     39        \u001b[36m0.5565\u001b[0m       0.4953        \u001b[35m0.5585\u001b[0m  0.0599\n",
      "     40        \u001b[36m0.5523\u001b[0m       0.4953        \u001b[35m0.5543\u001b[0m  0.0560\n",
      "     41        \u001b[36m0.5482\u001b[0m       0.4953        \u001b[35m0.5503\u001b[0m  0.0565\n",
      "     42        \u001b[36m0.5442\u001b[0m       0.4953        \u001b[35m0.5462\u001b[0m  0.1118\n",
      "     43        \u001b[36m0.5403\u001b[0m       0.4953        \u001b[35m0.5422\u001b[0m  0.0573\n",
      "     44        \u001b[36m0.5364\u001b[0m       0.4953        \u001b[35m0.5383\u001b[0m  0.0591\n",
      "     45        \u001b[36m0.5327\u001b[0m       0.4953        \u001b[35m0.5345\u001b[0m  0.0551\n",
      "     46        \u001b[36m0.5290\u001b[0m       0.4953        \u001b[35m0.5307\u001b[0m  0.0567\n",
      "     47        \u001b[36m0.5255\u001b[0m       0.4953        \u001b[35m0.5273\u001b[0m  0.0526\n",
      "     48        \u001b[36m0.5219\u001b[0m       0.4953        \u001b[35m0.5235\u001b[0m  0.0564\n",
      "     49        \u001b[36m0.5185\u001b[0m       0.4953        \u001b[35m0.5200\u001b[0m  0.0535\n",
      "     50        \u001b[36m0.5151\u001b[0m       0.4953        \u001b[35m0.5166\u001b[0m  0.0559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mawuliagamah/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# ANN 1  - Lalonde CPS \n",
    "ANN1_metrics = pd.DataFrame()\n",
    "ANN2_metrics = pd.DataFrame()\n",
    "ANN3_metrics = pd.DataFrame()\n",
    "ANN4_metrics = pd.DataFrame()\n",
    "\n",
    "kfold_evaluation_ANN(ANN1,X1,Y1,ANN1_metrics)\n",
    "kfold_evaluation_ANN(ANN2,X2,Y2,ANN2_metrics)\n",
    "kfold_evaluation_ANN(ANN3,X3,Y3,ANN3_metrics)\n",
    "kfold_evaluation_ANN(ANN4,X4,Y4,ANN4_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN1_metrics['avg'] = np.mean(ANN1_metrics,axis=1)\n",
    "ANN2_metrics['avg'] = np.mean(ANN2_metrics,axis=1)\n",
    "ANN3_metrics['avg'] = np.mean(ANN3_metrics,axis=1)\n",
    "ANN4_metrics['avg'] = np.mean(ANN4_metrics,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy:</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision:</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall:</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE:</th>\n",
       "      <td>0.018109</td>\n",
       "      <td>0.018109</td>\n",
       "      <td>0.018416</td>\n",
       "      <td>0.018416</td>\n",
       "      <td>0.018115</td>\n",
       "      <td>0.018233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE:</th>\n",
       "      <td>0.018109</td>\n",
       "      <td>0.018109</td>\n",
       "      <td>0.018416</td>\n",
       "      <td>0.018416</td>\n",
       "      <td>0.018115</td>\n",
       "      <td>0.018233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R^2:</th>\n",
       "      <td>-0.018443</td>\n",
       "      <td>-0.018443</td>\n",
       "      <td>-0.018762</td>\n",
       "      <td>-0.018762</td>\n",
       "      <td>-0.018449</td>\n",
       "      <td>-0.018572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc:</th>\n",
       "      <td>0.967413</td>\n",
       "      <td>0.938018</td>\n",
       "      <td>0.939004</td>\n",
       "      <td>0.959021</td>\n",
       "      <td>0.952947</td>\n",
       "      <td>0.951281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1:</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logloss:</th>\n",
       "      <td>0.276963</td>\n",
       "      <td>0.323777</td>\n",
       "      <td>0.331986</td>\n",
       "      <td>0.272838</td>\n",
       "      <td>0.316488</td>\n",
       "      <td>0.304410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4       avg\n",
       "Accuracy:   0.500000  0.500000  0.500000  0.500000  0.500000  0.500000\n",
       "Precision:  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "Recall:     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "MSE:        0.018109  0.018109  0.018416  0.018416  0.018115  0.018233\n",
       "MAE:        0.018109  0.018109  0.018416  0.018416  0.018115  0.018233\n",
       "R^2:       -0.018443 -0.018443 -0.018762 -0.018762 -0.018449 -0.018572\n",
       "roc_auc:    0.967413  0.938018  0.939004  0.959021  0.952947  0.951281\n",
       "F1:         0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "logloss:    0.276963  0.323777  0.331986  0.272838  0.316488  0.304410"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ANN1_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8eb45cb580>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from turtle import color\n",
    "history = ANN1.history\n",
    "tloss = history[:, 'train_loss']\n",
    "vloss = history[:, 'valid_loss']\n",
    "plt.plot(tloss, '-',color='b',label='training')\n",
    "plt.plot(vloss, '-',color='r', label='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy:</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision:</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall:</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE:</th>\n",
       "      <td>0.107527</td>\n",
       "      <td>0.107527</td>\n",
       "      <td>0.105925</td>\n",
       "      <td>0.105925</td>\n",
       "      <td>0.105925</td>\n",
       "      <td>0.106566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE:</th>\n",
       "      <td>0.107527</td>\n",
       "      <td>0.107527</td>\n",
       "      <td>0.105925</td>\n",
       "      <td>0.105925</td>\n",
       "      <td>0.105925</td>\n",
       "      <td>0.106566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R^2:</th>\n",
       "      <td>-0.120482</td>\n",
       "      <td>-0.120482</td>\n",
       "      <td>-0.118474</td>\n",
       "      <td>-0.118474</td>\n",
       "      <td>-0.118474</td>\n",
       "      <td>-0.119277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc:</th>\n",
       "      <td>0.892671</td>\n",
       "      <td>0.835659</td>\n",
       "      <td>0.882394</td>\n",
       "      <td>0.872065</td>\n",
       "      <td>0.867402</td>\n",
       "      <td>0.870038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1:</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logloss:</th>\n",
       "      <td>0.503749</td>\n",
       "      <td>0.485303</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.488683</td>\n",
       "      <td>0.471472</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4       avg\n",
       "Accuracy:   0.500000  0.500000  0.500000  0.500000  0.500000  0.500000\n",
       "Precision:  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "Recall:     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "MSE:        0.107527  0.107527  0.105925  0.105925  0.105925  0.106566\n",
       "MAE:        0.107527  0.107527  0.105925  0.105925  0.105925  0.106566\n",
       "R^2:       -0.120482 -0.120482 -0.118474 -0.118474 -0.118474 -0.119277\n",
       "roc_auc:    0.892671  0.835659  0.882394  0.872065  0.867402  0.870038\n",
       "F1:         0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "logloss:    0.503749  0.485303       inf  0.488683  0.471472       inf"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ANN2_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8eb4b609a0>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from turtle import color\n",
    "history = ANN2.history\n",
    "tloss = history[:, 'train_loss']\n",
    "vloss = history[:, 'valid_loss']\n",
    "plt.plot(tloss, '-',color='b',label='training')\n",
    "plt.plot(vloss, '-',color='r', label='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy:</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision:</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall:</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE:</th>\n",
       "      <td>0.011434</td>\n",
       "      <td>0.011434</td>\n",
       "      <td>0.011437</td>\n",
       "      <td>0.011437</td>\n",
       "      <td>0.011437</td>\n",
       "      <td>0.011436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE:</th>\n",
       "      <td>0.011434</td>\n",
       "      <td>0.011434</td>\n",
       "      <td>0.011437</td>\n",
       "      <td>0.011437</td>\n",
       "      <td>0.011437</td>\n",
       "      <td>0.011436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R^2:</th>\n",
       "      <td>-0.011566</td>\n",
       "      <td>-0.011566</td>\n",
       "      <td>-0.011570</td>\n",
       "      <td>-0.011570</td>\n",
       "      <td>-0.011570</td>\n",
       "      <td>-0.011568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc:</th>\n",
       "      <td>0.915594</td>\n",
       "      <td>0.835379</td>\n",
       "      <td>0.912686</td>\n",
       "      <td>0.891157</td>\n",
       "      <td>0.936624</td>\n",
       "      <td>0.898288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1:</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logloss:</th>\n",
       "      <td>0.404307</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.391819</td>\n",
       "      <td>0.407514</td>\n",
       "      <td>0.428711</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4       avg\n",
       "Accuracy:   0.500000  0.500000  0.500000  0.500000  0.500000  0.500000\n",
       "Precision:  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "Recall:     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "MSE:        0.011434  0.011434  0.011437  0.011437  0.011437  0.011436\n",
       "MAE:        0.011434  0.011434  0.011437  0.011437  0.011437  0.011436\n",
       "R^2:       -0.011566 -0.011566 -0.011570 -0.011570 -0.011570 -0.011568\n",
       "roc_auc:    0.915594  0.835379  0.912686  0.891157  0.936624  0.898288\n",
       "F1:         0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "logloss:    0.404307       inf  0.391819  0.407514  0.428711       inf"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ANN3_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8eb50079d0>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD/CAYAAAD/qh1PAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAAygUlEQVR4nO3dd3xcV5338c+d3jQzmtGoW83luMeOE2M7NnESQkIoG2oKJWF3IWRZCDzsQ91dnn322QUWFgi9hc5CCiGEhCQQEmIndqp7u26SrN67NCONZp4/7kiWZNlWLMkj3fm9Xy+9RnPuHekcj/y9Z84991wtmUwihBDCvCzproAQQojZJUEvhBAmJ0EvhBAmJ0EvhBAmJ0EvhBAmZ0t3BSZwApcDDcBwmusihBDzhRUoAF4CYhM3zrWgvxzYnu5KCCHEPLUFeHZi4VwL+gaAjo4+EokLm98fDvtoa+ud0UrNB9LuzCLtzizna7fFopGd7YVUhk4014J+GCCRSF5w0I+8PhNJuzOLtDuzTLHdkw55y8lYIYQwOQl6IYQwOQl6IYQwOQl6IYQwOQl6IYQwOQl6IYQwOdMEfTIep+qfP0Pn3n3prooQQswppgl6gMHGBroPH0l3NYQQYk4xTdBrNhtWXxZDHR3prooQQswppgl6AGsgwKAEvRBCjGOqoLcFAgy2d6a7GkIIMaeYLOiD0qMXQogJTBX01kCAoc5OksnMXPRICCEmY6qgtwWDJONxEn196a6KEELMGeYK+kAQgHhXZ1rrIYQQc4mpgt4aCAAQ7+xMb0WEEGIOMVXQ21JBP9zVleaaCCHE3GGyoA8CMnQjhBBjTelWgkopO/A14BYgCfwI+Kyu64lJ9vWl9n0HEAd+A3xC1/XBmar02VhcLiwuF3Hp0QshxKip3jP2C8C1wA1AFvALoBP44iT7/gxQqf29wK+AduDz06zrlDhC2QxLj14IIUadd+hGKeUC7gQ+ruv6C7quPwl8GvioUkqbsO8y4G3Arbquv6zr+jPAvwOXz3zVJ+fIzpYevRBCjDGVHv0awANsG1O2DSgAyoDKMeXXAEd0XR9dK1jX9e8D359uRafKHgwycPzExfp1Qggx500l6IuAbl3Xe8eUNaYeixkf9AuBE0qpO4GPAQ7gXuBfdF0fmn51z88RyibeKT16IYQYMZWg9wDRCWWx1KNzQnkWsBkj4G8DIsB3ADvwiQuv5tQ5srNJxqIkolEsLtfF+JVCCDGnTSXoBzgz0Eee908ojwMu4F26rnfC6Cyce5RS/3uyWTqTCYd9U9ltUs3Z2QD4bXHckawL/jnzUSTD2jtC2p1ZpN2v3lSCvhYIKKU8uq6PBHtB6rFuwr71QMNIyKccAdwYvfumqVSqra2XROLCFiZzhIygbz5Zh8d24QeM+SYSyaKlpSfd1bjopN2ZRdo9OYtFO2cHeSoXTO3F6LlvHlO2BajXdb16wr47gGKlVO6YshVAN9A2hd81bfZgEJCrY4UQYsR5g17X9QHgHuDbSqlNSqlrMObPfx1AKRVSSgVSuz8F7Ab+Rym1Sil1NcYc/O/puh6fjQaM9fCzlXRbjHF5uTpWCCEMU71g6pMYY++PY5yYvQf4Smrbg0AVcLuu6wml1JuAbwI7Mcb3fwz88wzWeVLx4QS/f64Sp9NGudUqc+mFECJlSkGv63oU+GDqa+K2rROeNwM3zUTlXg2b1UJB2MvJhm4WBwJydawQQqSYalGzkjwfJ+u6sAWC0qMXQogUcwV9bhZtXVGS3ixZk14IIVLMFfR5xvSifrtHZt0IIUSKyYLeuKCgS3My3NtDMj7rE32EEGLOM1XQ+9x2coJumuMOAOLd3WmukRBCpJ+pgh5gYVGAuqjRLJl5I4QQJgz6iqIAtf1Gs2TmjRBCmDDoywsD9NrcgFwdK4QQYMKgX1gUoM/qJokmM2+EEAITBn0k243b7SDudEuPXgghMGHQa5rGglwffTa3jNELIQQmDHow5tN34JSrY4UQApMGfWleFr0WN4MdHemuihBCpJ0pg35Bno9eq5tETw/JxJTuXiiEEKZlyqAvCHsYcHjQEsMk+vrSXR0hhEgrUwa91WLBmbp3rMy8EUJkOlMGPYA/LweAITkhK4TIcKYN+kixcX/yrsaWNNdECCHSy7RBX1hWAEBHvQS9ECKzmTboFxSFiWl2elra010VIYRIK9MGvdNhJer0yFx6IUTGM23QAyS8WdArNx8RQmQ2Uwe9LRDEGeujd2Ao3VURQoi0MXXQe3JC+OID1DT1pLsqQgiRNqYO+kB+Do5knFO1bemuihBCpI2pg94XCQPQXNOU5poIIUT6mDrorYEgAJ0NMpdeCJG5TB30tkAAgFh7B4NDw2mujRBCpIfJgz4IgCfeT22LrGIphMhMpg56i9cLVhve+ACnmmXmjRAiM5k66DVNwxYIEEjGqGqQC6eEEJnJ1EEPYAsGyLcP8YrewlBcxumFEJnH9EFv9QfI1mL0RePsPtaa7uoIIcRFZ/qgtwWCWPt7CfmdPLu/Id3VEUKIi878QR8MkujrZfOyXA6ebKe9O5ruKgkhxEVl+qC3pubSbyj3kAR2HGhMb4WEEOIiM33Qj8ylDyRiLC0J8uz+BpLJZHorJYQQF1EGBL3Rox/u6uSKVQU0dwxwrLYrzbUSQoiLx/RBP7LeTbyrk8tULi6Hle376tNbKSGEuIhMH/Q2vx80jXhXF06HlfXLcnnpSDMDsXi6qyaEEBeF6YNes1qx+rKIp+4du3l1IYNDCV4+0pzmmgkhxMVh+qAHcJWX03/4IMlkkoWFfvJDHrbLnHohRIbIiKD3rb2UeFsbsZpTaJrGltUFHK/torG9P91VE0KIWZcRQe9dsxY0jd5drwCwcWU+Fk3j2X3SqxdCmF9GBL0ty4978RJ6d+8CIOhzsqoixI4DDQwnEmmunRBCzK6MCHowhm8G62oZbDLuH7t5dQGdvYMcrGxPc82EEGJ22aayk1LKDnwNuAVIAj8CPqvr+hndYaXUR4BvTCh+VNf1N02zrtPiW3spLff+mt7drxC6/gYuWZSDz21n+94GVi/MSWfVhBBiVk21R/8F4FrgBuBm4Dbgk2fZdwXwU6BgzNd7plXLGWDPieAsKR0dvrFZLWy5pIBdR1uoa5XbDAohzOu8Qa+UcgF3Ah/Xdf0FXdefBD4NfFQppU3ykuXAXl3XG8d8dc5orS+Qb+2lRE8cJ97ZCcD160twOqw8tP1keismhBCzaCo9+jWAB9g2pmwbRk+9bJL9lwH6dCs2G3yXrgOgd4/Rq8/yOHj95Qt4RW+hqlFuNSiEMKepBH0R0K3reu+YspG1fovH7qiUygVygJuUUieVUseVUl9QSjlnprrT4ygswp6bNzp8A3Dd+hK8LhsPbpNevRDCnKZyMtYDTLxbRyz1ODHAl6UeO4EbgSXA3UAQY/hnSsJh31R3nVQkknXWbf1XbKD+4UfIdluw+bwAvOt1S/jJI4do7hlkRUV4Wr87nc7VbjOTdmcWaferN5WgH+DMQB95Pu7SUl3Xn1FK5ei63pYq2qeUAvi1UuouXdcHp1KptrZeEokLWzM+EsmipaXnrNstS1eR/N3vqX76WfwbNgGwXkV48GkHP/79fj717kvRtMlOPcxt52u3WUm7M4u0e3IWi3bODvJUhm5qgYBSyjOmrCD1WDdx5zEhP+IQxgEldwq/a9a5yiuwBoLjhm+cditv2lTG0doumVcvhDCdqQT9Xoye++YxZVuAel3Xq8fuqJS6IzU2P/bnrgW6gTmxCLxmseBbs5a+/ftIDJ7+gHHlmkJyAi5+u+2k3IFKCGEq5w16XdcHgHuAbyulNimlrgG+CHwdQCkVUkoFUrs/AUSAbymlFiul3gx8BfjSZBdXpYvv0nUkBwfpP3RwtMxmtfCWK8qpbuxh19GWNNZOCCFm1lQvmPok8DTwOPBrjAuivpLa9iDGCVd0Xa8CrgcuAfYA3wW+g3HB1ZzhUUuxuN2ji5yN2Lgyj4Kwh99tr7zgcwRCCDHXTGkJBF3Xo8AHU18Tt22d8Pw54IqZqNxs0Ww2vKsvoXffHpLDw2hWKwBWi4Ubt1Tw3YcO8PyhRjatLDjPTxJCiLkvYxY1m8i3dh2J3l4Gjh0dV75ORSjJ8/HQ9kqG4nNmtEkIIS5Yxga9d+UqNJvtjOEbi6bxzq2LaO2K8sfnq8/yaiGEmD8yNugtLheelavoefEFhvvGL2q2ojzEa5bn8ejOKhraZMEzIcT8lrFBDxB+y40M9/XS+tCDZ2y7+ZrFOGxWfv64LtMthRDzWkYHvauklOBV19D116eIVleN2xbwOnjnVQvRazp5Vm4kLoSYxzI66AHCN74Vqy+L5l/9guSE2wpuuaSQxcUB7nvqON39U1q9QQgh5pyMD3qrx0vknTcRPXmC7h3Pjttm0TTed/1SooPD3PuX42mqoRBCTE/GBz1A1sZNuBcvofWB+xnu7R23rSjHyxs2lLDzYCOHqmQdHCHE/CNBD2iaRu6t72W4v2/SE7Nv2lhGbrabnz+hMzg0nIYaCiHEhZOgT3EuWEDw6mvoeuZpolVV47Y57Fbed52iuWOAR3bK3HohxPwiQT9G+C1vxZqVRfOvfn7GidnlZSE2rsjnseerqWnuPctPEEKIuUeCfgyrx0PknTcTrTxJ97Pbz9h+0zWL8Lrt/ODhgzKEI4SYNyToJ8jasBH34iW0/PY+4p0d47b5PQ7+/o3LqGvt496nZRaOEGJ+kKCfQNM08t53O8mhIRp+9IMzhnBWVoR5/eULeHpXHbuPybr1Qoi5T4J+Eo6CQnJvfS8DRw7T/ugfztj+9isXUpLr4yd/PEJHT2ySnyCEEHOHBP1Z+K/YTNaGjbQ9/BD9+pFx2+w2C3f8zQoG48P86JFDJGQtHCHEHCZBfxaappH3nvdhz82j4YffI97TPW57QdjLra9bwuHqDp548VSaaimEEOcnQX8OFpebgjvuJNHbS+M9PzpzLZzVBaxTER585iSVDd1n+SlCCJFeEvTn4SopJXLTLfQf2EfHn58Yt03TNG67fil+r4PvP3yQ6GA8TbUUQoizk6CfgsDWq/Gtu4zWBx9g4MT4aZU+t50Pvnk5LR0D/PSxI7J2vRBizpGgnwJN08i77f3YsrNp+MF3z1j4TJVk87YrK3jxcDOPvyDj9UKIuUWCfoqsHi8FH7yT4a4u6r/3bZLx8cM0N2wo5bKluTzw1xPsP9mWploKIcSZJOhfBXfFQvLe934Gjhym6Vc/HzdMo2kaf3fDMooiPr7/+4M0dfSnsaZCCHGaBP2r5N90BaEb3kT39m10Tjg563RY+cjbV6Fp8M3f7mcgJidnhRDpJ0F/AcI3vg3fustouf9eevfsHrctEnRz540raWzrl4uphBBzggT9BdAsFvL/9gM4S8to+OH3iNWMPwG7vCzEu65exO5jrTzyXFV6KimEECkS9BfI4nRS9I93YfV4qfvm14l3do7bfu1lxWxckc9Dz1ay66gsfiaESB8J+mmwBYMUfuQuhvv6qPvW3SQGB0e3GRdTKcoLsvjBHw5ysl6unBVCpIcE/TS5Skop+Ps7iFVX0fD974ybdumwW/noOy7B73Hw9fv3ykwcIURaSNDPAN/aS8l993vp27uHxh+PXxMn4HXwv25aA8DX7t1Ld9/gWX6KEELMDgn6GRLcejU5b38XPS8+T/MvfzZujn1+yMNd71hNZ2+Mux/YS2xQbkMohLh4JOhnUOgNNxB645vp2vYMrff9ZlzYLywKcMffrKCqsYfv/v4AwxNWwhRCiNkiQT/Dwje+jeDVr6Pjz0/Q/sjD47atXRzhva9X7DvRxi+e0GUBNCHERWFLdwXMRtM0IjffSiIape33v8PidJH9+utGt29dW0R7T4xHdlQR9Dm5cUtFGmsrhMgEEvSzQLNYyLvt/SRiUVru+zWa00nwyq2j29+6pZyOnigPP1eF123n2ssWpK+yQgjTk6CfJZrVSsEHPkT94Ddo/sVPYThO8OrXGds0jdvfsJSB2DC/fvIYLruVLZcUprfCQgjTkjH6WaTZbBT8w0fwrllL8//8kvYnHhvdZrVYuOMtK1hZHuKnjx/hxcNNaaypEMLMJOhnmcVup/BDHybr8vW03n8vbX/4/ehJWLvNwofftorFRQF++IdD7D3emubaCiHMSIL+ItBsNvI/8CH8m66g7fe/o/XBB0bD3pm6erY418e3f3eAw9Udaa6tEMJsJOgvEs1iIe/2vyNw5VV0PPYoLb/5n9Gw97hsfOKmNeRlu/nGA/s4UdeV5toKIcxEgv4i0iwWct/zPoKvez2df/kzTT//yehyCT63nU/cvIaA18HX7ttLbXPveX6aEEJMjQT9RaZpGpGbbhm9S1X9d745uupl0Ofkn25eg9Nh5b/v20Nz50CaayuEMAMJ+jTQNI2ct72DyC3vpm/vHmr/+78Y7jV68DlBN//rpjXE4wm++ps9dPXG0lxbIcR8J0GfRtnXXEvBHXcSq66i5kv/yVBbGwBFOV4+9q5L6Oob5Kv37aU/OpTmmgoh5jMJ+jTLumw9RR/7BPHODk594d+J1dYAsLAwwIfftpL61j7ufmAfsSFZ8VIIcWGmFPRKKbtS6ltKqTalVKtS6otKqfO+Vin1mFLqN9Ovprl5li5jwac+C0DNl/6T/iOHAVhZHuYDb17O8douvvfQAeLDsuKlEOLVm2qP/gvAtcANwM3AbcAnz/UCpdRtwPXTql0GcRYvoOQz/4ItmE3d1/+b7p3PAbB+WR7vuU6x90QbP/njYRKy4qUQ4lWaSq/cBdwJfFzX9Rd0XX8S+DTwUaWUdpbX5ANfBF6cycqanT0cZsGnPotr0WIa7/khLb+9n2QiwVVri3jrayvYebCJB585me5qCiHmman06NcAHmDbmLJtQAFQdpbXfAf4HnB4GnXLSFafj+KPfYLAa7fS8dijxvTLaJQ3bSxl65pC/vh8Nc/ua0h3NYUQ88hUgr4I6NZ1fewVPI2px+KJOyul3gUojOEecQE0m43c995G5GZj+mXNl/6DeHs7t167hGWl2fzs8SPop2SpBCHE1ExlmWIPEJ1QNjK52zm2UCkVBu4G3qrr+qBS6oIqFQ77Luh1IyKRrGm9fq7IveVtRFQ5+pe/Su1//l+WfvZT/Ovfb+CfvrGd7zx0gK/c9VoKc07/W5ml3a+WtDuzSLtfvakE/QATAn3M8/4J5d8A7tN1/fkLrhHQ1tZLInFhJx0jkSxaWnqm8+vnlgWLKP70P1P/za9x4HP/Ss47b+Ifb9zIf/xyN5///k4+9751eF1287V7iqTdmUXaPTmLRTtnB3kqQze1QEAp5RlTVpB6rJuw763AB5RSvUqpXuDdwNtT34sL5CwspORzn8e9dDktv/4VQz/7Lh++ppiWzgG+8zuZdimEOLepBP1ejJ775jFlW4B6XderJ+y7GFiNcQJ3DfAw8ETqezENVp+Pors+Tu573sfA8WNYf/Bl7qiIcri6g1/9+ajcaFwIcVbnHbrRdX1AKXUP8O3U3Hg3xtTJrwIopULAsK7rXbquHx/7WqVUDzA0sVxcGE3TCG69Gs+y5TTe80P8j9/Lh8pW8NNXYtxfEOCqSwrO/0OEEBlnqveM/STgAh7HODF7D/CV1LYHgSrg9hmumzgLR14+Cz71WdofexT+8HvutFfzx/sb6O/byhs3VaS7ekKIOUabYx/5y4BKORk7ddGqKhp//EMG6+todmQztPn1bL3lejRt0mvZTCfT3u8R0u7M8ipOxpZjdLzHb5+1momLwlVWRun/+XcWffwufHYoeupe9n/qM/Tu2S3j9kIIQILeFDSLhbytr2XVV77M4UvfyEBPH/XfuptT//F/jcBPyKwcITLZVMfoxTxgtdt484fewY8fWUbP8zu4tvUQsW/djaOwiOzr3oD/NRvQbPKWC5FppEdvMhaLxt++aQX+TZv5et6bqNzwFpKaRtNPfkTlZz5Jx5+eIBGdeKGzEMLMpHtnQhaLxvtvWIbDYeXeXXWEc6/n3RsShPc/R8t9v6btkYcJXnU1gSuvwh4Kpbu6QohZJkFvUhaLxntfr1i/NJdf/fko39jbx8ryN3DTtW9B2/EU7X98hPbHHsV36TqCV12De4nKmJk6QmQaCXqTUyXZfP79l/PUK3U89OxJ/u1UguvW38B1b30nA889Q9f2bfS+/BKOomKCV1+Df8MmLM6JSxsJIeYzCfoMYLVYuPbyBaxflsv9fz3Bozur2XHAyRs3XsEVb3wL0V0v0fmXJ2n+xc9ofeA+sjZsJLDlSlwlpemuuhBiBkjQZ5CAz8nfv2k5r72kkAeeOcEv/3SUPz7v5I0bF7L5c1cQrzpJ59NP0f3sdrqefgpnSSmBLVeS9ZoNWD2e8/8CIcScJFfGmsSrbXcymeRQVQcPPXuSE3XdhP1O3ripjM2rCtCiA/S8sJOu7c8Qq6lBczjIWnc5/is2G2P5lrkzWUve78wi7Z7c+a6MlR59htI0jRXlIZaXZXOwsp2Hnq3k54/rPLqjitevL2HL5q0ErrqGWHU1XdufoeeFnXTvfA5bKIx/4yb8GzfhyJdF1ISYD6RHbxLTbXcymeRAZTt/2FHF8douvC4bW9cWcc26YoI+J4lYjN69u+nesYP+g/shmcRVUYF/4xVkXbYea1Z67voj73dmkXZP7nw9egl6k5jJdh+v6+KJF06x62gLVqvGhhX5XLe+hKIcLwDxzk56Xnye7p3PEaupAYsFz/IVZF3+GnxrL72o4/nyfmcWaffkJOgzxGy0u6mjnz+9VMNz+xoYjCdYUZbN1ZcWc8miHCwWY859rOYU3S++QO9LLzLU2oJms+FZucoI/UvWYHG5ZrROE8n7nVmk3ZOToM8Qs9nunv5B/rq7jr/uqaejJ0bY72Tr2iK2rC7E73UAxtBPtLKSnpdeoPflF4l3dKDZ7UboX7oO7yVrsHq8M143eb8zi7R7chL0GeJitHs4kWDPsVae2lXH4eoObFaNy5bmsnVNEYuLA6NX1iYTCQaOH6P35Zfo3f0K8Y4OsFrxLFuO79J1+NZeii3LPyN1kvc7s0i7JydBnyEudrsb2vp4alcdOw40MBAbJj/kYcvqAjatzCfgO31lbTKRIFp5kt5dL9P7yisMtbaApuFetBjvmrX41qzFkZd/wfWQ9zuzSLsnJ0GfIdLV7tjgMC8daWb7vnqO1XZh0TQuWRRmy+pCVi0MYR0z5z6ZTBKrOUXv7l307dllnMgFHAWFo6HvKq94VfP05f3OLNLuyUnQZ4i50O6Gtj6272tgx/4GuvuHCHgdvGZ5HhtX5FOS5ztj0bSh1hZ69+yhd88uBo7qkEhgzfLjXbUa7yWX4Fm+Eqvbfc7fORfanQ7S7swiQT+B/CGkX3w4wb4TbTy3v4F9J9oYTiQpinjZtCKf1yzPI+Q/cybOcF8ffQf20bdvL33795Po7zPG9dVSvKsvwbtqNfbcvDMOFnOp3ReTtDuzSNBPIH8Ic0vvwBAvHW5ix4FGTtR3owFLS7N5zfI8Ll0Swee2n/Ga5PAwAyeO07d3D3379jLYUA+APRLBs2IV3pWr8CxdisXlnrPtnm3S7swiQT+B/CHMXU3t/ew82Mjzh5po7hjAatFYWR5i/fI81izKwe2cfEWOwZZm+g/sp+/gAfoPHyIZi4HVinvRYnLXryNZshBnadmcWoNnts2H93s2SLsnJ0GfIeZTu5PJJNVNPbx4qJkXjzTR3h3DbrOwemGYy5fmsqoifNbQT8bjDBw/Rt+B/fQf3D96Qtfi8eBZugzPshV4li+fdJjHTObT+z2TpN2Tk6DPEPO13YlkkhN1Xbx4qJmX9Ga6+waxWS2sLA+xTkVYuzgHj+vM4Z0RAXuCmudepP/QQfoPHSTe3g6ALRTCo5bhXroUz9Jl2MM5F6tJF8V8fb+nS9o9OVm9UsxpFk1jcXGQxcVBbnndYo7XdfGy3swregt7jrditWgsK8vm0sUR1izOIegbf/crRzCAf/0G/Os3kEwmGWpuMkL/yGF69++le+dzANhzIrjVUjxLl+JeokwX/EKci/ToTcJs7U4kk1Q2dPOK3sIrejMtnVEAKgr9rF2cw5rFEQrDHnJz/WdtdzKRYLC+jv4jR+jXDzOg68ZsHsAWCuNesgT3EoVnicKelz+vhnrM9n5PlbR7cjJ0kyHM3O5kMkldax+7j7Wy51gLlQ1GO3Oz3WxcVcjiwiyWLAhis577ZGwykSBWW8PAsaMMHNUZOHqU4Z5uAKxZftyLFuNatMh4LC1Ds83dD7xmfr/PRdo9ORm6EfOepmkUR3wUR3y8eVMZHT0x9hxrYfexVh59rpL4cAKXw8qK8hCrF4ZZXREetwzD6M+xWHCVlOIqKSX7mmuNoZ6mJgaO6vQf04keP0bv7leMfe12XGXluBYtxr1wEa6Khdj8M7M+jxAXmwS9mHeys5xcdWkxV11ajM/vZvvLp9h7oo39J9t4RW8BoDQ/i1UVIVaWh6ko9E/a29c0DUd+Po78fAKvvRIw1tofOHGMgWPHGDh+jI4/PU7H8DBgzON3VSzCtXAh7opFOIuL53SvX4gR8lcq5jW308baJRHWLomQTCapae5l74k2Dpxs4487T/HIjmrcTivLSkOsLDe+coJnX1bBFgySte5ystZdDkAiFiNaXUX05AmiJ0/Qf+QwPS/sBIxev7OkFFd5xeiXPRKZV2P9IjNI0AvT0DSNkrwsSvKyePOmMvqjQxyq6uBAZTsHKtvYddTo7edmu1lRZtwvd2lpNt5zTN+0OJ14UidswThfEG9vJ3riONHKk0SrKuna9lc6n/yTsb/Xa4R+WRmu0nKcZeXYs7Nnv/FCnIMEvTAtj8vOZUtzuWxpLslkkoa2fg5UtnO4qp0dBxt5encdmgZl+VksLwuxtDSbRUUBnHbrWX+mpmnYw2Hs4TBZ618DGEs2DNbXMXDypBH+lSdpP3QQEgkArIGgEfxl5cYngNJSbEEJf3HxSNCLjKBpGoU5XgpzvLz+8gXEhxOcrO/mUFU7h6o7eOz5Uzy6sxqrRWNhoZ+lpdksLclmYZEfu+3swQ+gWa04F5TgXFACV24FjCGfWM0polVVRKsriVVV0bdvL6RmuVkDAVwlpThLS3GWlOFaUIItJ0eGfcSskKAXGclmtbBkQZAlC4LcuAUGYnGO1XZx5FQHR6o7+MOOKh5+rgqb1cLCQr+xb0mQRYUBnI5zBz8YQz7uRYtxL1o8WpaIDhCrqSFaXUWsuproqWr6DuwfDX+L220cMEpKRg8cjoJCLPazDy0JMRUS9EJgnNRdvTDM6oVhAPqjQxytMYJfr+nkkZ1VJHeA1aJRmm/M219SHGRRcWDSFTgnY3G5cS9egnvxktGyRCxGrK6WWM0pYqdOEas5Rde2Z0gODho7WK048gtwLlhghH+x8Ugka8b/DYR5SdALMQmPy86axTmsWWwslTAQi3O8roujNZ3oNZ38+aUaHn/hFAAFYU9qGYcAi4sDRILuKQ/BWJxO3BULcVcsHC1LJhIMNTcZwV9bQ/TUKWO2z/M7R/epyQ5iLyjCWbwAR1ExzuJiHIWFWOyOGfxXEGYhQS/EFLidNlZVhFlVYfT4B4eGqWzo5lhtl7E+z5Fmtu011s33ex0sLPSzsCjAwkI/ZQX+c57gnUizWHDkF+DILxg94Qsw3NNDrLaGWM0ptNZGuo5X0vn0X0gODRk7WCw4cvNwFBXhKCzCWVSEo7AYR26uzPfPcPLuC3EBHHYrqiQbVWLMnkkkk9S39nG8totjtV2cqO9i97FWwBjuKc71sagwQEWhn4pCP7nZU+/1j7BmZeFZthzPsuWjl8Qnh4eN3n9dLbHa2tRjDb27Xhkd+x8d/ikyev3OomIcRcXYc3Iyag3/TCZBL8QMsIxZpmHr2iIAuvsHOVnXzYn6Lk7UdbF9fz1/2VULgNdlo7zQT0WBn4rCAOUFWWR5Xv2wi2a14igoxFFQSNZl60fLE4ODDDY2MFhXS6yujsG6WgZOHKPnxedPv9bhMF5bWDj6CcJRUIAjN08+AZiMvJtCzBK/xzFunH84kaC+tZ+T9V2crO/mZEM3fzhZxcjyfTkBF2UFfsoLsijL91OWn3XWG7Ccj8XhGF3XZ6xEdIBYff2YA0AdA0eO0LNzx5gXW7BHIuPDP/W91ee7oPqI9JKgF+IisVosLMj1sSDXx5VrjF7/QCxOdWMPlY3dVDb0UNXQzctHmgHQgLyQh7L8LErzsyjLN676vdDwh9TMnwknf8E4AAw2NjHYWM9gQwODDfUMNjbSf/AAyXj8dBt8WdhT6wM58vKx5xnf2yO5Mg10DpOgFyKN3E6bcXFW6ekrZXv6B6lq7KGyoZvqxh70mk6eP9Q0uj0v5EGVZJMXdKWWfPBd0LDPWBaXO3X1btm48mQiwVBr6+kDQGMDQ42N9O3bS3f39tM7ahr2cA72vDwceXnGASD1aA/LuYB0k6AXYo7J8jjGzfAB6OobpLqxh+rGbqoaezhc3c62PQOj20N+JyW5RugvyM1iQZ6PSMA17SttNYsFR24ujtxcWL1m3Lbh/n6GmpuMcwFNTQylHrtPHCcRjZ7e0Wo1hoJy87Dnpg4EuXk4cvOwhUJo1qnPSBIXRoJeiHkg4HWMu6ArEsmi8lQ7p5p6ONXUy6mmHqqbeth7onV0so3baaU4YgwVleRlURzxUZTjndKVvVNh9XiwlpXjKisfV55MJhnu7jLCv6nReGxpZrCpif4jh09fDAbGQSCcgz0SwR7JHX10RCLYIxEsrrOvNCqmToJeiHnK57azvCzE8rLQaFlsaJi6lj5qmns41dxLTXMvzx1o5KlddYAx7p8b8rAg4qU418eCiI+iiJecoBvLDK2zo2katkAQWyAIqVU/RySTSYa7Oo3wb25iqKXFOAi0tBCteoFEX9+4/a1ZWeMOAomKEqIuP/acCLZgUIaEpmhKQa+UsgNfA24BksCPgM/qup6YZN81wN3AZUAL8B1d1/9rpioshDg7p906Old/RCKZpLVzgJrmPmpbeqlt7uVUcy+v6C2jM36cdiuFOV6KIl6j5x/xUpzjxe91zOhCa5qmYQtmG6t3qqVnbB/u70uFv3EAML5aGDhxnJ4XX6B9zK1PNZsNW04O9pxc7JEc7DkR7DkjjxGsXu+M1Xu+m2qP/gvAtcANQBbwC6AT+OLYnZRSfuAJ4H7gb4HlwK+UUk26rv9shuoshHgVLJpGbraH3GwP61RktDw6GKeutY+6lj5qm3upa+1j7/FWnt3XMLqPz203DgCpg0BRagXQ6Z78PRurx4u11IurtOyMbcl4nKzkAE1HqxlqHXswaCF64hiJgYFx+1vcbuw5OdhSwW8P56QOBMbBwOJyzUob5qLzBr1SygXcCbxT1/UXUmWfBr6glPqSrutj7+JdAjwJ3KXr+jBwQin1JHAlIEEvxBzicthYWBhgYWFgXHlX3yC1Lb3Ut/QZB4LWXp4/1MhAbHh0nyyPncKwl4IcL4VhD4U5XgrCXoK+mf0EMJZms+GOFOK1T76g23B/H0OtrcYBoLWFodZW4q0tDDWlpomOPTcAWHw+7KGwcTAI54weCGyhEPbsEBafzzTLRk+lR78G8ADbxpRtAwqAMqBypFDX9QPAuwGUUhqwGSPkPzIjtRVCzLqA10HAG2LFmLH/ZDJJR0+MutY+6lv7aGjro761nxcPNdEfOz3P3u20kh/yUhD2pL6M7yNB96T37Z1JVo8Xa4n3jIvERuo/3NNzOvxbWxhqa2OorY3Bhgb6Duw/40Cg2e3YskPYQiFs2dnYR74PhbGnHq0ez6y2aaZMJeiLgG5d13vHlDWmHosZE/QTdAAB4A/AvRdcQyFE2mmaRsjvIuR3jZv2mUwm6e4bpL61j/q2fhrb+qlv6+NwdQc7DjSO7mfRNCJBF/khD/lhj/GY+prp8wBnq7/N78fm90NFxRnbk8kkw709xFtbGWpvJ97RTjz1ONTezoB+hJ7OztG7ho22y+XCFgobnwJCodEDgz0UNr7PzsbidM5q26ZiKkHvAaITymKpx0lboJSyAK8DFgDfxTiR+9GpViocnt5l1pEMXatb2p1Z5kq7c4FF5TlnlPdHh6hr6aWmqZe6ll7qmo3Hw9V1DMZPB6bbaaMo4qUw4qMo4jPuBBbxURjxTbrW/6y1O9cPFUVn3ZwcHmawo5NYSwux1jYGW1uJtbQSazWe9++tZqir+4zXWb1enDlhHKEQjnAIRyiEMxzGkSpzhkPY/P7zHuym0+6pBP0AZwb6yPP+yV6Qmo3zMvCyUioL+KFS6p90XR+cbP+J2tp6SSSS599xEiOr+mUaaXdmmS/tDrpsBEuDrCoNjpYlkknau6I0tvfT2N5PU/sAjR39HDrZxvbddYz9n+9z28kLucnL9pCX7WZxaRiXDXKDHjyudMwOd0BOEeQU4TCeMTZ+E0ODxNs7Tn8i6OxgqKOdeEcHAx2d9JysYri76/TKoimazYYtmE3wmteRfe11Z/zW873fFot2zg7yVP6laoGAUsqj6/pIsBekHuvG7qiUKgOW67r+xzHFBzD+PfxA6xR+nxDCxCyaRk7QTU7Qzcoxw0AAQ/FhmjsGaOoYoKnDOAg0d/SfHgrafnqkOMtjJzfbTW7QOAhEst3kBo3HLLc9LSdSLXYHjtQyEGeTjMeJd3cR7+gg3tlBvKMz9diONRA46+umYypBvxej574Z+FOqbAtQr+t69YR9NwD3KKXyxozprwOadV2XkBdCnJPdZqUoNYQzUWxwmCFNQz/ZOnowaO7oR6/pYOfBxnH7up1WIsFU8KfCf+R5yO/EmsYLrTSbzZjtEwqff+cZct6g13V9QCl1D/BtpdRtgBtj/vxXAZRSIWBY1/Uu4BGgGfixUupfgKXAfwL/Nkv1F0JkCKfDSnEkC5/9zJAeig/T0hmluXOA5o4BWjoGaO4coKaljz3HW4kPnx4qsVo0wn4XOUEXOQE3kdRjTtBFJOAmy5OeTwOzaaqDXJ8EXMDjGCdm7wG+ktr2IFAF3K7req9S6jrgGxhj9F3Al3Vd/9ZMVloIIcay24wrewtzzrwaNpFI0tkbMw4AncYBoKVzgJbOKLuPtdDTPzRuf6fdSk7ARTjgIieQOgiknocDrrQNC02Hlkxe2EnPWVIGVMrJ2FdP2p1ZpN0zJzoYp7UrSmtnlJauAVo7o7R2DdDWFaW1KzruOgEAh91C2O8yvgITHv0uglmOGR8aehUnY8sxOt7jyKJmQoiM5nLYRm8DOZn+6JBxIOiK0tYVpa07OnoQqGrsoXdg/CcCTYPsLCehVPCHUt+H/E7jud+F12W7qJ8KJOiFEOIcPC47JS47JXmTz2OPDQ7T1h2lvTt1EOiO0dZlPD9Z38XL3TGGJ4xQOOwWsrNSB4EsJ9mpA8LqhWFC/plfg0eCXgghpsHpOPv5ATCuG+jpG6S9J3UA6InR3h2loydGe0+UQ9UddPbGSCbhNcvzuOMtK2a8jhL0QggxiyyaRsDnJOBzUl7gn3Sf4USCrt5B/N7ZWRVUgl4IIdLMarHMypDNCLk9ixBCmJwEvRBCmJwEvRBCmJwEvRBCmJwEvRBCmJwEvRBCmNxcm15pBWPdhumY7uvnK2l3ZpF2Z5ZztXvMNutk2+faomabge3proQQQsxTW4BnJxbOtaB3ApcDDcBwmusihBDzhRXjzn8vcfqe3qPmWtALIYSYYXIyVgghTE6CXgghTE6CXgghTE6CXgghTE6CXgghTE6CXgghTE6CXgghTG6uLYFwQZRSduBrwC1AEvgR8Fld1xNprdgsUUq5gFeAT+i6/niqLAB8F3gj0Av8t67rX01fLWeOUqoY4/29CogDj2K0vdPM7QZQSpUD3wSuxGjfz4HP6boeN3vbAZRS3wPW6Lq+IfXc1G1WSr0ZeHhC8UFd11dOp+1m6dF/AbgWuAG4GbgN+GRaazRLlFIe4H5g+YRN9wClGJdA3wX8m1Lq5otcvRmnlLIAvwP8wNXAW4A1wE9Su5iy3QBKKQ14BIgCl2H8bd8KfC61i2nbDqCU2gp8cEKxqdsMrAD+gnGV68jXlaltF9z2ed+jT/Vu7wTeqev6C6myTwNfUEp9Sdd101z6q5Rah9GjG5xQXgq8DVip6/ohYJ9SagXwMeA3F7ueM2w1RsgV6LreCKCU+iiw3eTtBsgHDgD/oOt6G6Arpe4HrjR721Mdmh8CzwH2VJmp25yyHDgw8rc+YrptN0OPfg3gAbaNKduGcSQsS0N9ZtM1GMMWmyaUbwTaUn8AI7YB61LDWvPZKeANE/7wk4CG0bMxa7vRdb1B1/WbUiGPUmo18DfAk5j7PQf4fxiLc/1lTJnZ2wxG0OuTlE+r7fO+Rw8UAd26rveOKRsJhWKg8uJXaXbouv5fI98rpcZuKgLqJ+zeiPH+5gM1s165WaLrejvw+ITij2P8Z8jDpO2eSCm1F+PTzcvA3cCHMGnblVIbMIapVgIfHbPJtH/nMDpUtxTjE9tHMTqwjwGfYpptN0OP3oMxhjnWyOptzotcl3TJmH8DpdSnMD7C3kUGtRu4HeM8lB/4NSZtu1LKCfwY+FjqID+WKds8RgngxfjEeitwB/BaZuD9NkPQD3BmQ0ee91/kuqRLRvwbKKX+Bfgi8FFd158gQ9oNoOv6bl3XnwQ+ALwZ87b9X4Fjuq7fN8k2s7YZAF3Xq4EwcGvq/X4ceB/wBoxQv+C2m2HophYIKKU8uq6PNLgg9ViXpjpdbLWcbvOIAoyTtq0XvzozTyn1NYxe/J26rn8vVWzqdiul8oDNuq7/dkzxgdSjC3O2/VagQCk1MhTrAKyp5/+AOds8apJPMSNj8g6m0XYz9Oj3YhzRNo8p2wLUp46QmWAnkKuUWjKmbAvwsq7rg2d5zbyhlPo88BHgtjEhDyZvN1AOPKCUqhhTtg7jWoJfYM62b8UYm1+T+voesC/1/TOYs80AKKVuUEp1KKX8Y4rXAgmm+X6b4sYjSqlvYHy8uQ1wA78Evqrr+pfTWrFZpJRKYsxGGblg6mEgF2OqaQXwU+BvdV2/P22VnAFKqVXAHuC/ME5CjtWCMcfedO2G0WsIdmB8bP8wRjt/CPxO1/V/Mut7PpZS6v8A14+5YMq0bU5dEHUQeBHjWok8jAPdNl3XPzidtpuhRw/GxVFPY8zO+DXGP8BX0lmhNLgdYyjjOYxA/Gcz/PEDb8f4O/00xi0mx34txrztJnVl91uBJox7Kf8G+C3wmdQut2PStp/D7Zi0zbqudwHXYZyQfR7jwsgnMD7NwjTabooevRBCiLMzS49eCCHEWUjQCyGEyUnQCyGEyUnQCyGEyUnQCyGEyUnQCyGEyUnQCyGEyUnQCyGEyUnQCyGEyf1/UCVD2meoSVcAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from turtle import color\n",
    "history = ANN3.history\n",
    "tloss = history[:, 'train_loss']\n",
    "vloss = history[:, 'valid_loss']\n",
    "plt.plot(tloss, '-',color='b',label='training')\n",
    "plt.plot(vloss, '-',color='r', label='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy:</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision:</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall:</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE:</th>\n",
       "      <td>0.069159</td>\n",
       "      <td>0.069159</td>\n",
       "      <td>0.069159</td>\n",
       "      <td>0.069159</td>\n",
       "      <td>0.069159</td>\n",
       "      <td>0.069159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE:</th>\n",
       "      <td>0.069159</td>\n",
       "      <td>0.069159</td>\n",
       "      <td>0.069159</td>\n",
       "      <td>0.069159</td>\n",
       "      <td>0.069159</td>\n",
       "      <td>0.069159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R^2:</th>\n",
       "      <td>-0.074297</td>\n",
       "      <td>-0.074297</td>\n",
       "      <td>-0.074297</td>\n",
       "      <td>-0.074297</td>\n",
       "      <td>-0.074297</td>\n",
       "      <td>-0.074297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc:</th>\n",
       "      <td>0.941631</td>\n",
       "      <td>0.933301</td>\n",
       "      <td>0.861283</td>\n",
       "      <td>0.461576</td>\n",
       "      <td>0.871784</td>\n",
       "      <td>0.813915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1:</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logloss:</th>\n",
       "      <td>inf</td>\n",
       "      <td>0.544065</td>\n",
       "      <td>0.628422</td>\n",
       "      <td>0.802203</td>\n",
       "      <td>0.489740</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4       avg\n",
       "Accuracy:   0.500000  0.500000  0.500000  0.500000  0.500000  0.500000\n",
       "Precision:  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "Recall:     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "MSE:        0.069159  0.069159  0.069159  0.069159  0.069159  0.069159\n",
       "MAE:        0.069159  0.069159  0.069159  0.069159  0.069159  0.069159\n",
       "R^2:       -0.074297 -0.074297 -0.074297 -0.074297 -0.074297 -0.074297\n",
       "roc_auc:    0.941631  0.933301  0.861283  0.461576  0.871784  0.813915\n",
       "F1:         0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "logloss:         inf  0.544065  0.628422  0.802203  0.489740       inf"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ANN4_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8eb5086d00>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from turtle import color\n",
    "history = ANN4.history\n",
    "tloss = history[:, 'train_loss']\n",
    "vloss = history[:, 'valid_loss']\n",
    "plt.plot(tloss, '-',color='b',label='training')\n",
    "plt.plot(vloss, '-',color='r', label='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propensity_score_functANN(dataset,model,dataset2,dehwab):\n",
    "    \n",
    "    dataset2.columns = dataset2.columns.str.strip() \n",
    "    \n",
    "    if dehwab == True: \n",
    "        dataset2 = dataset2[['treat','age','education*','black','hispanic','married','nodegree','re74','re75','re78']]\n",
    "    else:\n",
    "        dataset2 = dataset2[['treat','age','education*','black','hispanic','married','nodegree','re75','re78']]\n",
    "\n",
    "    treat =  dataset.iloc[:,0]\n",
    "    dataset =  dataset.iloc[:,1:len(dataset)]\n",
    "    columns = dataset.columns\n",
    "    dataset = dataset.to_numpy()\n",
    "    dataset = dataset.astype(np.float32)\n",
    "    #Y1 = np.reshape(Y1,(Y1.shape[0],1))\n",
    "    # Generate propensity score prediction  \n",
    "    probabilities = model.predict_proba(dataset)\n",
    "    probabilities = pd.DataFrame(probabilities)\n",
    "    ps = probabilities # propensity score \n",
    "    # merge prediction and existing dataset \n",
    "    dataset = pd.DataFrame(dataset)\n",
    "    dataset.columns = columns\n",
    "    #dataset_proba = pd.merge(dataset, ps, left_index=True, right_index=True)\n",
    "    dataset_proba = pd.merge(dataset2, ps, left_index=True, right_index=True)\n",
    "    dataset_proba.rename(index=int, columns={0:'propensity_score'}, inplace = True) # rename column\n",
    "    dataset_proba['propensity_logit'] = np.log(dataset_proba['propensity_score'] / (1-dataset_proba['propensity_score']))\n",
    "    return dataset_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict get scores on NSW , CPS and PSID \n",
    "# ============ Lalonde Subsample ============ # \n",
    "# predict propensity scores\n",
    "nswCps_lalonde_ps_ANN = propensity_score_functANN(nswCps_lalonde_subset,ANN1,nswCps_lalonde,False)\n",
    "nswPsid_lalonde_ps_ANN= propensity_score_functANN(nswPsid_lalonde_subset,ANN2,nswPsid_lalonde,False)\n",
    "# ============ Dehejia & Wahba sub sample ============ # \n",
    "# predict propensity scores\n",
    "nswCps_dehWab_ps_ANN = propensity_score_functANN(nswCps_dehWab_subset,ANN3,nswCps_dehWab,True)\n",
    "nswPsid_dehWab_ps_ANN = propensity_score_functANN(nswPsid_dehWab_subset,ANN4,nswPsid_dehWab,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>treat</th>\n",
       "      <th>age</th>\n",
       "      <th>education*</th>\n",
       "      <th>black</th>\n",
       "      <th>hispanic</th>\n",
       "      <th>married</th>\n",
       "      <th>nodegree</th>\n",
       "      <th>re75</th>\n",
       "      <th>re78</th>\n",
       "      <th>propensity_score</th>\n",
       "      <th>propensity_logit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9930</td>\n",
       "      <td>0.271427</td>\n",
       "      <td>-0.987395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3595</td>\n",
       "      <td>0.696632</td>\n",
       "      <td>0.831311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24909</td>\n",
       "      <td>0.059114</td>\n",
       "      <td>-2.767346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>27</td>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7506</td>\n",
       "      <td>0.955577</td>\n",
       "      <td>3.068568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>33</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>289</td>\n",
       "      <td>0.312067</td>\n",
       "      <td>-0.790472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2782</th>\n",
       "      <td>0.0</td>\n",
       "      <td>47</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33837</td>\n",
       "      <td>38568</td>\n",
       "      <td>0.224749</td>\n",
       "      <td>-1.238200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2783</th>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67137</td>\n",
       "      <td>59109</td>\n",
       "      <td>0.710253</td>\n",
       "      <td>0.896611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2784</th>\n",
       "      <td>0.0</td>\n",
       "      <td>47</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>47968</td>\n",
       "      <td>55710</td>\n",
       "      <td>0.362199</td>\n",
       "      <td>-0.565831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2785</th>\n",
       "      <td>0.0</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44220</td>\n",
       "      <td>20540</td>\n",
       "      <td>0.221335</td>\n",
       "      <td>-1.257901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2786</th>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55500</td>\n",
       "      <td>53198</td>\n",
       "      <td>0.210615</td>\n",
       "      <td>-1.321225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2787 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      treat  age  education*  black  hispanic  married  nodegree   re75  \\\n",
       "0       1.0   37          11    1.0       0.0      1.0       1.0      0   \n",
       "1       1.0   22           9    0.0       1.0      0.0       1.0      0   \n",
       "2       1.0   30          12    1.0       0.0      0.0       0.0      0   \n",
       "3       1.0   27          11    1.0       0.0      0.0       1.0      0   \n",
       "4       1.0   33           8    1.0       0.0      0.0       1.0      0   \n",
       "...     ...  ...         ...    ...       ...      ...       ...    ...   \n",
       "2782    0.0   47           8    0.0       0.0      1.0       1.0  33837   \n",
       "2783    0.0   32           8    0.0       0.0      1.0       1.0  67137   \n",
       "2784    0.0   47          10    0.0       0.0      1.0       1.0  47968   \n",
       "2785    0.0   54           0    0.0       1.0      1.0       1.0  44220   \n",
       "2786    0.0   40           8    0.0       0.0      1.0       1.0  55500   \n",
       "\n",
       "       re78  propensity_score  propensity_logit  \n",
       "0      9930          0.271427         -0.987395  \n",
       "1      3595          0.696632          0.831311  \n",
       "2     24909          0.059114         -2.767346  \n",
       "3      7506          0.955577          3.068568  \n",
       "4       289          0.312067         -0.790472  \n",
       "...     ...               ...               ...  \n",
       "2782  38568          0.224749         -1.238200  \n",
       "2783  59109          0.710253          0.896611  \n",
       "2784  55710          0.362199         -0.565831  \n",
       "2785  20540          0.221335         -1.257901  \n",
       "2786  53198          0.210615         -1.321225  \n",
       "\n",
       "[2787 rows x 11 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nswPsid_lalonde_ps_ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save unmatched boost datasets \n",
    "nswCps_lalonde_ps_ANN.to_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/ann/unmatched/nswCps_lalonde_ps_unmatched_ANN_FS1.csv')\n",
    "nswPsid_lalonde_ps_ANN.to_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/ann/unmatched/nswPsid_lalonde_ps_unmatched_ANN_FS1.csv')\n",
    "nswCps_dehWab_ps_ANN.to_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/ann/unmatched/nswCps_dehWab_ps_unmatched_ANN_FS1.csv')\n",
    "nswPsid_dehWab_ps_ANN.to_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/ann/unmatched/nswPsid_dehWab_ps_unmatched_ANN_FS1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bar plots of model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'boost1_metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/python-script/propensity score estimation with DAG feature selection (best params) 2 .ipynb Cell 120\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/python-script/propensity%20score%20estimation%20with%20DAG%20feature%20selection%20%28best%20params%29%202%20.ipynb#ch0000119?line=0'>1</a>\u001b[0m \u001b[39m# Average accuracy\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/python-script/propensity%20score%20estimation%20with%20DAG%20feature%20selection%20%28best%20params%29%202%20.ipynb#ch0000119?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtueplots\u001b[39;00m \u001b[39mimport\u001b[39;00m axes, bundles , figsizes, fonts,fontsizes\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/python-script/propensity%20score%20estimation%20with%20DAG%20feature%20selection%20%28best%20params%29%202%20.ipynb#ch0000119?line=2'>3</a>\u001b[0m nsw_cps_lalonde_acc \u001b[39m=\u001b[39m [ANN1_metrics\u001b[39m.\u001b[39mloc[\u001b[39m'\u001b[39m\u001b[39mAccuracy:\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mavg\u001b[39m\u001b[39m'\u001b[39m],boost1_metrics\u001b[39m.\u001b[39mloc[\u001b[39m'\u001b[39m\u001b[39mAccuracy:\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mavg\u001b[39m\u001b[39m'\u001b[39m],forest1_metrics\u001b[39m.\u001b[39mloc[\u001b[39m'\u001b[39m\u001b[39mAccuracy:\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mavg\u001b[39m\u001b[39m'\u001b[39m],cart1_metrics\u001b[39m.\u001b[39mloc[\u001b[39m'\u001b[39m\u001b[39mAccuracy:\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mavg\u001b[39m\u001b[39m'\u001b[39m],logit1_metrics\u001b[39m.\u001b[39mloc[\u001b[39m'\u001b[39m\u001b[39mAccuracy:\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mavg\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/python-script/propensity%20score%20estimation%20with%20DAG%20feature%20selection%20%28best%20params%29%202%20.ipynb#ch0000119?line=3'>4</a>\u001b[0m nsw_psid_lalonde_acc \u001b[39m=\u001b[39m [ANN2_metrics\u001b[39m.\u001b[39mloc[\u001b[39m'\u001b[39m\u001b[39mAccuracy:\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mavg\u001b[39m\u001b[39m'\u001b[39m],boost2_metrics\u001b[39m.\u001b[39mloc[\u001b[39m'\u001b[39m\u001b[39mAccuracy:\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mavg\u001b[39m\u001b[39m'\u001b[39m],forest2_metrics\u001b[39m.\u001b[39mloc[\u001b[39m'\u001b[39m\u001b[39mAccuracy:\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mavg\u001b[39m\u001b[39m'\u001b[39m],cart2_metrics\u001b[39m.\u001b[39mloc[\u001b[39m'\u001b[39m\u001b[39mAccuracy:\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mavg\u001b[39m\u001b[39m'\u001b[39m],logit2_metrics\u001b[39m.\u001b[39mloc[\u001b[39m'\u001b[39m\u001b[39mAccuracy:\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mavg\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/python-script/propensity%20score%20estimation%20with%20DAG%20feature%20selection%20%28best%20params%29%202%20.ipynb#ch0000119?line=4'>5</a>\u001b[0m nsw_cps_dehWab_acc \u001b[39m=\u001b[39m [ANN3_metrics\u001b[39m.\u001b[39mloc[\u001b[39m'\u001b[39m\u001b[39mAccuracy:\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mavg\u001b[39m\u001b[39m'\u001b[39m],boost3_metrics\u001b[39m.\u001b[39mloc[\u001b[39m'\u001b[39m\u001b[39mAccuracy:\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mavg\u001b[39m\u001b[39m'\u001b[39m],forest3_metrics\u001b[39m.\u001b[39mloc[\u001b[39m'\u001b[39m\u001b[39mAccuracy:\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mavg\u001b[39m\u001b[39m'\u001b[39m],cart3_metrics\u001b[39m.\u001b[39mloc[\u001b[39m'\u001b[39m\u001b[39mAccuracy:\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mavg\u001b[39m\u001b[39m'\u001b[39m],logit3_metrics\u001b[39m.\u001b[39mloc[\u001b[39m'\u001b[39m\u001b[39mAccuracy:\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mavg\u001b[39m\u001b[39m'\u001b[39m]]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'boost1_metrics' is not defined"
     ]
    }
   ],
   "source": [
    "# Average accuracy\n",
    "from tueplots import axes, bundles , figsizes, fonts,fontsizes\n",
    "nsw_cps_lalonde_acc = [ANN1_metrics.loc['Accuracy:','avg'],boost1_metrics.loc['Accuracy:','avg'],forest1_metrics.loc['Accuracy:','avg'],cart1_metrics.loc['Accuracy:','avg'],logit1_metrics.loc['Accuracy:','avg']]\n",
    "nsw_psid_lalonde_acc = [ANN2_metrics.loc['Accuracy:','avg'],boost2_metrics.loc['Accuracy:','avg'],forest2_metrics.loc['Accuracy:','avg'],cart2_metrics.loc['Accuracy:','avg'],logit2_metrics.loc['Accuracy:','avg']]\n",
    "nsw_cps_dehWab_acc = [ANN3_metrics.loc['Accuracy:','avg'],boost3_metrics.loc['Accuracy:','avg'],forest3_metrics.loc['Accuracy:','avg'],cart3_metrics.loc['Accuracy:','avg'],logit3_metrics.loc['Accuracy:','avg']]\n",
    "nsw_psid_dehWab_acc = [ANN4_metrics.loc['Accuracy:','avg'],boost4_metrics.loc['Accuracy:','avg'],forest4_metrics.loc['Accuracy:','avg'],cart4_metrics.loc['Accuracy:','avg'],logit4_metrics.loc['Accuracy:','avg']]\n",
    "\n",
    "# Average precison\n",
    "nsw_cps_lalonde_prec = [ANN1_metrics.loc['Precision:','avg'],boost1_metrics.loc['Precision:','avg'],forest1_metrics.loc['Precision:','avg'],cart1_metrics.loc['Precision:','avg'],logit1_metrics.loc['Precision:','avg']]\n",
    "nsw_psid_lalonde_prec = [ANN2_metrics.loc['Precision:','avg'],boost2_metrics.loc['Precision:','avg'],forest2_metrics.loc['Precision:','avg'],cart2_metrics.loc['Precision:','avg'],logit2_metrics.loc['Precision:','avg']]\n",
    "nsw_cps_dehWab_prec = [ANN3_metrics.loc['Precision:','avg'],boost3_metrics.loc['Precision:','avg'],forest3_metrics.loc['Precision:','avg'],cart3_metrics.loc['Precision:','avg'],logit3_metrics.loc['Precision:','avg']]\n",
    "nsw_psid_dehWab_prec = [ANN4_metrics.loc['Precision:','avg'],boost4_metrics.loc['Precision:','avg'],forest4_metrics.loc['Precision:','avg'],cart4_metrics.loc['Precision:','avg'],logit4_metrics.loc['Precision:','avg']]\n",
    "\n",
    "# Average recall \n",
    "nsw_cps_lalonde_rec = [ANN1_metrics.loc['Recall:','avg'],boost1_metrics.loc['Recall:','avg'],forest1_metrics.loc['Recall:','avg'],cart1_metrics.loc['Recall:','avg'],logit1_metrics.loc['Recall:','avg']]\n",
    "nsw_psid_lalonde_rec = [ANN2_metrics.loc['Recall:','avg'],boost2_metrics.loc['Recall:','avg'],forest2_metrics.loc['Recall:','avg'],cart2_metrics.loc['Recall:','avg'],logit2_metrics.loc['Recall:','avg']]\n",
    "nsw_cps_dehWab_rec = [ANN3_metrics.loc['Recall:','avg'],boost3_metrics.loc['Recall:','avg'],forest3_metrics.loc['Recall:','avg'],cart3_metrics.loc['Recall:','avg'],logit3_metrics.loc['Recall:','avg']]\n",
    "nsw_psid_dehWab_rec = [ANN4_metrics.loc['Recall:','avg'],boost4_metrics.loc['Recall:','avg'],forest4_metrics.loc['Recall:','avg'],cart4_metrics.loc['Recall:','avg'],logit4_metrics.loc['Recall:','avg']]\n",
    "\n",
    "# Average F1 \n",
    "nsw_cps_lalonde_f1 = [ANN1_metrics.loc['F1:','avg'],boost1_metrics.loc['F1:','avg'],forest1_metrics.loc['F1:','avg'],cart1_metrics.loc['F1:','avg'],logit1_metrics.loc['F1:','avg']]\n",
    "nsw_psid_lalonde_f1 = [ANN2_metrics.loc['F1:','avg'],boost2_metrics.loc['F1:','avg'],forest2_metrics.loc['F1:','avg'],cart2_metrics.loc['F1:','avg'],logit2_metrics.loc['F1:','avg']]\n",
    "nsw_cps_dehWab_f1 = [ANN3_metrics.loc['F1:','avg'],boost3_metrics.loc['F1:','avg'],forest3_metrics.loc['F1:','avg'],cart3_metrics.loc['F1:','avg'],logit3_metrics.loc['F1:','avg']]\n",
    "nsw_psid_dehWab_f1 = [ANN4_metrics.loc['F1:','avg'],boost4_metrics.loc['F1:','avg'],forest4_metrics.loc['F1:','avg'],cart4_metrics.loc['F1:','avg'],logit4_metrics.loc['F1:','avg']]\n",
    "\n",
    "# Average log loss\n",
    "nsw_cps_lalonde_logloss = [ANN1_metrics.loc['logloss:','avg'],boost1_metrics.loc['log-loss:','avg'],forest1_metrics.loc['log-loss:','avg'],cart1_metrics.loc['log-loss:','avg'],logit1_metrics.loc['log-loss:','avg']]\n",
    "nsw_psid_lalonde_logloss = [ANN2_metrics.loc['logloss:','avg'],boost2_metrics.loc['log-loss:','avg'],forest2_metrics.loc['log-loss:','avg'],cart2_metrics.loc['log-loss:','avg'],logit2_metrics.loc['log-loss:','avg']]\n",
    "nsw_cps_dehWab_logloss = [ANN3_metrics.loc['logloss:','avg'],boost3_metrics.loc['log-loss:','avg'],forest3_metrics.loc['log-loss:','avg'],cart3_metrics.loc['log-loss:','avg'],logit3_metrics.loc['log-loss:','avg']]\n",
    "nsw_psid_dehWab_logloss = [ANN4_metrics.loc['logloss:','avg'],boost4_metrics.loc['log-loss:','avg'],forest4_metrics.loc['log-loss:','avg'],cart4_metrics.loc['log-loss:','avg'],logit4_metrics.loc['log-loss:','avg']]\n",
    "\n",
    "# Average Roc Auc \n",
    "nsw_cps_lalonde_rocauc = [ANN1_metrics.loc['roc_auc:','avg'],boost1_metrics.loc['roc_auc:','avg'],forest1_metrics.loc['roc_auc:','avg'],cart1_metrics.loc['roc_auc:','avg'],logit1_metrics.loc['roc_auc:','avg']]\n",
    "nsw_psid_lalonde_rocauc = [ANN2_metrics.loc['roc_auc:','avg'],boost2_metrics.loc['roc_auc:','avg'],forest2_metrics.loc['roc_auc:','avg'],cart2_metrics.loc['roc_auc:','avg'],logit2_metrics.loc['roc_auc:','avg']]\n",
    "nsw_cps_dehWab_rocauc = [ANN3_metrics.loc['roc_auc:','avg'],boost3_metrics.loc['roc_auc:','avg'],forest3_metrics.loc['roc_auc:','avg'],cart3_metrics.loc['roc_auc:','avg'],logit3_metrics.loc['roc_auc:','avg']]\n",
    "nsw_psid_dehWab_rocauc = [ANN4_metrics.loc['roc_auc:','avg'],boost4_metrics.loc['roc_auc:','avg'],forest4_metrics.loc['roc_auc:','avg'],cart4_metrics.loc['roc_auc:','avg'],logit4_metrics.loc['roc_auc:','avg']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nsw_cps_lalonde_acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/python-script/propensity score estimation with DAG feature selection (best params).ipynb Cell 121\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/python-script/propensity%20score%20estimation%20with%20DAG%20feature%20selection%20%28best%20params%29.ipynb#ch0000120?line=24'>25</a>\u001b[0m colour3 \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m#003366\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m# CPS dehwab\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/python-script/propensity%20score%20estimation%20with%20DAG%20feature%20selection%20%28best%20params%29.ipynb#ch0000120?line=25'>26</a>\u001b[0m colour4 \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m#006614\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m# PSID dehwab\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/python-script/propensity%20score%20estimation%20with%20DAG%20feature%20selection%20%28best%20params%29.ipynb#ch0000120?line=27'>28</a>\u001b[0m ax[\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mbarh(y,nsw_cps_lalonde_acc,width,color \u001b[39m=\u001b[39m colour1)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/python-script/propensity%20score%20estimation%20with%20DAG%20feature%20selection%20%28best%20params%29.ipynb#ch0000120?line=28'>29</a>\u001b[0m ax[\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mbarh(y\u001b[39m+\u001b[39mwidth,nsw_psid_lalonde_acc,width,color \u001b[39m=\u001b[39m colour2)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/python-script/propensity%20score%20estimation%20with%20DAG%20feature%20selection%20%28best%20params%29.ipynb#ch0000120?line=29'>30</a>\u001b[0m ax[\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mbarh(y\u001b[39m+\u001b[39m\u001b[39m2\u001b[39m\u001b[39m*\u001b[39mwidth,nsw_cps_dehWab_acc,width,color \u001b[39m=\u001b[39m colour3)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nsw_cps_lalonde_acc' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x864 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create plot \n",
    "# Update plot paramaters \n",
    "fig, ax = plt.subplots(nrows=2, ncols=3,figsize = (20,12))\n",
    "\n",
    "# Neat formatting for plots \n",
    "plt.rcParams.update({\"figure.dpi\": 300}) \n",
    "plt.rcParams.update(bundles.neurips2022())\n",
    "plt.rcParams.update(fonts.neurips2022())\n",
    "plt.rcParams.update(axes.tick_direction( y=\"in\"))\n",
    "plt.rcParams.update(axes.color(base=\"black\"))\n",
    "plt.rcParams.update(figsizes.neurips2022(nrows=2, ncols=3))\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "y = np.arange(5)\n",
    "x = np.arange(1)\n",
    "width = 0.2\n",
    "# Balanced accuracy\n",
    "\n",
    "ax[0,0].set_title(\"Balanced accuracy\",fontsize=20)\n",
    "ax[0,0].set_xlabel('score',fontsize=16)\n",
    "ax[0,0].set_yticks(y+2*width, ['neural network', 'boosted tree', 'random forest', 'cart', 'logistic regression'],fontsize=16)\n",
    "ax[0,0].tick_params(axis='x', which='major', labelsize=16)\n",
    "\n",
    "colour1 = \"#E31B23\" # CPS lalonde\n",
    "colour2 = \"#e31b8c\" # PSID lalonde\n",
    "colour3 = \"#003366\" # CPS dehwab\n",
    "colour4 = \"#006614\" # PSID dehwab\n",
    "\n",
    "ax[0,0].barh(y,nsw_cps_lalonde_acc,width,color = colour1)\n",
    "ax[0,0].barh(y+width,nsw_psid_lalonde_acc,width,color = colour2)\n",
    "ax[0,0].barh(y+2*width,nsw_cps_dehWab_acc,width,color = colour3)\n",
    "ax[0,0].barh(y+(3*width),nsw_psid_dehWab_acc,width,color = colour4)\n",
    "\n",
    "ax[0,0].set_facecolor('white')\n",
    "ax[0,0].grid(color='white', axis='y')\n",
    "ax[0,0].grid(color='white', axis='x')\n",
    "ax[0,0].spines['left']\n",
    "ax[0,0].spines['right']\n",
    "ax[0,0].spines['bottom']\n",
    "ax[0,0].tick_params(right='on')\n",
    "ax[0,0].tick_params(left='on')\n",
    "ax[0,0].tick_params(bottom='on')\n",
    "\n",
    "# precision\n",
    "\n",
    "ax[0,1].set_title(\"precision\",fontsize=20)\n",
    "ax[0,1].set_xlabel('score',fontsize=16)\n",
    "ax[0,1].set_yticks(y+2*width, [' ', ' ', ' ', ' ', ' '],fontsize=16)\n",
    "ax[0,1].tick_params(axis='x', which='major', labelsize=16)\n",
    "\n",
    "ax[0,1].barh(y,nsw_cps_lalonde_prec,width,color = colour1)\n",
    "ax[0,1].barh(y+width,nsw_psid_lalonde_prec,width,color = colour2)\n",
    "ax[0,1].barh(y+2*width,nsw_cps_dehWab_prec,width,color = colour3)\n",
    "ax[0,1].barh(y+(3*width),nsw_psid_dehWab_prec,width,color = colour4)\n",
    "\n",
    "ax[0,1].set_facecolor('white')\n",
    "ax[0,1].grid(color='white', axis='y')\n",
    "ax[0,1].grid(color='white', axis='x')\n",
    "ax[0,1].spines['left']\n",
    "ax[0,1].spines['right']\n",
    "ax[0,1].spines['bottom']\n",
    "ax[0,1].tick_params(right='on')\n",
    "ax[0,1].tick_params(left='on')\n",
    "ax[0,1].tick_params(bottom='on')\n",
    "\n",
    "# recall \n",
    "\n",
    "ax[0,2].set_title(\"Recall\",fontsize=20)\n",
    "ax[0,2].set_xlabel('score',fontsize=16)\n",
    "ax[0,2].set_yticks(y+2*width, [' ', ' ', ' ', ' ', ' '],fontsize=16)\n",
    "ax[0,2].tick_params(axis='x', which='major', labelsize=16)\n",
    "\n",
    "ax[0,2].barh(y,nsw_cps_lalonde_rec,width,color = colour1)\n",
    "ax[0,2].barh(y+width,nsw_psid_lalonde_rec,width,color = colour2)\n",
    "ax[0,2].barh(y+2*width,nsw_cps_dehWab_rec,width,color = colour3)\n",
    "ax[0,2].barh(y+(3*width),nsw_psid_dehWab_rec,width,color = colour4)\n",
    "\n",
    "ax[0,2].set_facecolor('white')\n",
    "ax[0,2].grid(color='white', axis='y')\n",
    "ax[0,2].grid(color='white', axis='x')\n",
    "ax[0,2].spines['left']\n",
    "ax[0,2].spines['right']\n",
    "ax[0,2].spines['bottom']\n",
    "ax[0,2].tick_params(right='on')\n",
    "ax[0,2].tick_params(left='on')\n",
    "ax[0,2].tick_params(bottom='on')\n",
    "\n",
    "# F1\n",
    "ax[1,0].set_title(\"F1 score\",fontsize=20)\n",
    "ax[1,0].set_xlabel('score',fontsize=16)\n",
    "ax[1,0].set_yticks(y+2*width, ['neural network', 'boosted tree', 'random forest', 'cart', 'logistic regression'],fontsize=16)\n",
    "ax[1,0].tick_params(axis='x', which='major', labelsize=16)\n",
    "\n",
    "ax[1,0].barh(y,nsw_cps_lalonde_f1,width,color = colour1)\n",
    "ax[1,0].barh(y+width,nsw_psid_lalonde_f1,width,color = colour2)\n",
    "ax[1,0].barh(y+2*width,nsw_cps_dehWab_f1,width,color = colour3)\n",
    "ax[1,0].barh(y+(3*width),nsw_psid_dehWab_f1,width,color = colour4)\n",
    "\n",
    "ax[1,0].set_facecolor('white')\n",
    "ax[1,0].grid(color='white', axis='y')\n",
    "ax[1,0].grid(color='white', axis='x')\n",
    "ax[1,0].spines['left']\n",
    "ax[1,0].spines['right']\n",
    "ax[1,0].spines['bottom']\n",
    "ax[1,0].tick_params(right='on')\n",
    "ax[1,0].tick_params(left='on')\n",
    "ax[1,0].tick_params(bottom='on')\n",
    "\n",
    "\n",
    "# Log loss\n",
    "ax[1,1].set_title(\"Log loss\",fontsize=20)\n",
    "ax[1,1].set_xlabel('score',fontsize=16)\n",
    "ax[1,1].set_yticks(y+2*width, [' ', ' ', ' ', ' ', ' '],fontsize=16)\n",
    "ax[1,1].tick_params(axis='x', which='major', labelsize=16)\n",
    "\n",
    "ax[1,1].barh(y,nsw_cps_lalonde_logloss,width,color = colour1)\n",
    "ax[1,1].barh(y+width,nsw_psid_lalonde_logloss,width,color = colour2)\n",
    "ax[1,1].barh(y+2*width,nsw_cps_dehWab_logloss,width,color = colour3)\n",
    "ax[1,1].barh(y+(3*width),nsw_psid_dehWab_logloss,width,color = colour4)\n",
    "\n",
    "ax[1,1].set_facecolor('white')\n",
    "ax[1,1].grid(color='white', axis='y')\n",
    "ax[1,1].grid(color='white', axis='x')\n",
    "ax[1,1].spines['left']\n",
    "ax[1,1].spines['right']\n",
    "ax[1,1].spines['bottom']\n",
    "ax[1,1].tick_params(right='on')\n",
    "ax[1,1].tick_params(left='on')\n",
    "ax[1,1].tick_params(bottom='on')\n",
    "\n",
    "# Roc-AUC\n",
    "ax[1,2].set_title(\"Roc-Auc\",fontsize=20)\n",
    "ax[1,2].set_xlabel('score',fontsize=16)\n",
    "ax[1,2].set_yticks(y+2*width, [' ', ' ', ' ', ' ', ' '],fontsize=16)\n",
    "ax[1,2].tick_params(axis='x', which='major', labelsize=16)\n",
    "\n",
    "ax[1,2].barh(y,nsw_cps_lalonde_rocauc,width,color = colour1)\n",
    "ax[1,2].barh(y+width,nsw_psid_lalonde_rocauc,width,color = colour2)\n",
    "ax[1,2].barh(y+2*width,nsw_cps_dehWab_rocauc,width,color = colour3)\n",
    "ax[1,2].barh(y+(3*width),nsw_psid_dehWab_rocauc,width,color = colour4)\n",
    "\n",
    "ax[1,2].set_facecolor('white')\n",
    "ax[1,2].grid(color='white', axis='y')\n",
    "ax[1,2].grid(color='white', axis='x')\n",
    "ax[1,2].spines['left']\n",
    "ax[1,2].spines['right']\n",
    "ax[1,2].spines['bottom']\n",
    "ax[1,2].tick_params(right='on')\n",
    "ax[1,2].tick_params(left='on')\n",
    "ax[1,2].tick_params(bottom='on')\n",
    "\n",
    "labels = ['CPS - Lalonde','PSID - Lalonde','CPS - DW','PSID - DW']\n",
    "fig.legend(labels, loc='lower center', bbox_to_anchor=(0.5,-0.1), ncol=len(labels),markerscale=3, bbox_transform=fig.transFigure,prop={'size': 18})\n",
    "\n",
    "plt.savefig('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/Jupyter notebooks/lalonde_notebook_plots/fig8_training_evaluation_averages.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis on model outptus - Probability calibration curves "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nswCps_lalonde =  pd.read_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/unmatched data/Quasi_NswCps_lalonde.csv')\n",
    "nswPsid_lalonde = pd.read_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/unmatched data/Quasi_NswPsid_lalonde.csv')\n",
    "nswCps_dehWab =   pd.read_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/unmatched data/Quasi_NswCps_dehWab.csv')\n",
    "nswPsid_dehWab =  pd.read_csv('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/data/quasi data/unmatched data/Quasi_NswPsid_dehWab.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree      \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Take a random sample from the data with an even number of treated and control for testing\n",
    "\n",
    "nswCps_lalonde_valid = nswCps_lalonde.groupby('treat',group_keys=False).apply(lambda x: x.sample(50))\n",
    "nswCps_lalonde = nswCps_lalonde.drop(nswCps_lalonde_valid.index) # Remove all samples from training which are in the sample \n",
    "\n",
    "nswPsid_lalonde_valid = nswPsid_lalonde.groupby('treat',group_keys=False).apply(lambda x: x.sample(50))\n",
    "nswPsid_lalonde = nswPsid_lalonde.drop(nswPsid_lalonde_valid.index) # Remove all samples from training which are in the sample \n",
    "\n",
    "nswCps_dehWab_valid = nswCps_dehWab.groupby('treat',group_keys=False).apply(lambda x: x.sample(50))\n",
    "nswCps_dehWab = nswCps_dehWab.drop(nswCps_dehWab_valid.index)# Remove all samples from training which are in the sample \n",
    "\n",
    "nswPsid_Dehwab_valid = nswPsid_dehWab.groupby('treat',group_keys=False).apply(lambda x: x.sample(50))\n",
    "nswPsid_dehWab = nswPsid_dehWab.drop(nswPsid_Dehwab_valid.index) # Remove all samples from training which are in the sample \n",
    "\n",
    "# covariates used in each mdoel \n",
    "logit_vars = ['treat','age','agesq','education*','educsq','black','hispanic','married','nodegree','re75','u75','educ_re75']\n",
    "logit_vars2 = ['treat','age','agesq','education*','educsq','black','hispanic','married','nodegree','re74','re75','u74','u75','educ_re75']\n",
    "tree_vars = ['treat','age','education*','black','hispanic','married','nodegree','re75']\n",
    "tree_vars2 = ['treat','age','education*','black','hispanic','married','nodegree','re74','re75']\n",
    "ann_vars = ['treat','ageboxcox','black','hispanic','married','nodegree','lnre75','education_8', 'education_9','education_10', 'education_11', 'education_12', 'education_13','education_14', 'education_15', 'education_16', 'education_17']\n",
    "ann_vars2 = ['treat','ageboxcox','black','hispanic','married','nodegree','lnre74','lnre75','education_8', 'education_9','education_10', 'education_11', 'education_12',  'education_13', 'education_14', 'education_15', 'education_16', 'education_17']\n",
    "continuos_vars = ['ageboxcox','lnre75'] \n",
    "continuos_vars2= ['ageboxcox','lnre74','lnre75'] \n",
    "# ======== Lalonde - sample ======== #\n",
    "\n",
    "# ~~~~~~~~~~\n",
    "# 1 cps\n",
    "# ~~~~~~~~~~\n",
    "\n",
    "#logit model \n",
    "cps_lalonde_subset_logit = nswCps_lalonde[logit_vars]\n",
    "cps_lalonde_subset_valid_logit = nswCps_lalonde_valid[logit_vars]\n",
    "#decision tree models \n",
    "cps_lalonde_subset_trees = nswCps_lalonde[tree_vars]\n",
    "cps_lalonde_subset_valid_trees = nswCps_lalonde_valid[tree_vars]\n",
    "#ann model \n",
    "cps_lalonde_subset_ann = nswCps_lalonde[ann_vars]\n",
    "cps_lalonde_subset_valid_ann = nswCps_lalonde_valid[ann_vars]\n",
    "cps_lalonde_subset_ann[continuos_vars] = sc.fit_transform(cps_lalonde_subset_ann[continuos_vars])\n",
    "cps_lalonde_subset_valid_ann[continuos_vars] = sc.fit_transform(cps_lalonde_subset_valid_ann[continuos_vars])\n",
    "\n",
    "# ~~~~~~~~~~\n",
    "# 2 PSID \n",
    "# ~~~~~~~~~~\n",
    "\n",
    "#logit model \n",
    "psid_lalonde_subset_logit = nswPsid_lalonde[logit_vars]\n",
    "psid_lalonde_subset_valid_logit = nswPsid_lalonde_valid[logit_vars]\n",
    "#decision tree models \n",
    "psid_lalonde_subset_trees = nswPsid_lalonde[tree_vars]\n",
    "psid_lalonde_subset_valid_trees = nswPsid_lalonde_valid[tree_vars]\n",
    "#ann model \n",
    "psid_lalonde_subset_ann = nswPsid_lalonde[ann_vars]\n",
    "psid_lalonde_subset_valid_ann = nswPsid_lalonde_valid[ann_vars]\n",
    "psid_lalonde_subset_ann[continuos_vars] = sc.fit_transform(psid_lalonde_subset_ann[continuos_vars])\n",
    "psid_lalonde_subset_valid_ann[continuos_vars] = sc.fit_transform(psid_lalonde_subset_valid_ann[continuos_vars])\n",
    "\n",
    "\n",
    "# ======== Dehejia - Wahba sample ======== #\n",
    "\n",
    "# ~~~~~~~~~~\n",
    "# 3 CPS\n",
    "# ~~~~~~~~~~\n",
    "# logit \n",
    "cps_dehWab_subset_logit = nswCps_dehWab[logit_vars2]\n",
    "cps_dehWab_subset_valid_logit = nswCps_dehWab_valid[logit_vars2]\n",
    "# decision tree's\n",
    "cps_dehWab_subset_trees = nswCps_dehWab[tree_vars2]\n",
    "cps_dehWab_subset_valid_trees = nswCps_dehWab_valid[tree_vars2]\n",
    "#ann\n",
    "cps_dehWab_subset_ann = nswCps_dehWab[ann_vars2]\n",
    "cps_dehWab_subset_valid_ann = nswCps_dehWab_valid[ann_vars2]\n",
    "cps_dehWab_subset_ann[continuos_vars] = sc.fit_transform(cps_dehWab_subset_ann[continuos_vars])\n",
    "cps_dehWab_subset_valid_ann[continuos_vars] = sc.fit_transform(cps_dehWab_subset_valid_ann[continuos_vars])\n",
    "\n",
    "# ~~~~~~~~~~\n",
    "# 4 PSID\n",
    "# ~~~~~~~~~~\n",
    "# logit \n",
    "psid_dehWab_subset_logit = nswCps_dehWab[logit_vars2]\n",
    "psid_dehWab_subset_valid_logit = nswCps_dehWab_valid[logit_vars2]\n",
    "# decision tree's\n",
    "psid_dehWab_subset_trees = nswCps_dehWab[tree_vars2]\n",
    "psid_dehWab_subset_valid_trees = nswCps_dehWab_valid[tree_vars2]\n",
    "#ann\n",
    "psid_dehWab_subset_ann = nswCps_dehWab[ann_vars2]\n",
    "psid_dehWab_subset_valid_ann = nswCps_dehWab_valid[ann_vars2]\n",
    "psid_dehWab_subset_ann[continuos_vars] = sc.fit_transform(cps_dehWab_subset_ann[continuos_vars])\n",
    "psid_dehWab_subset_valid_ann[continuos_vars] = sc.fit_transform(cps_dehWab_subset_valid_ann[continuos_vars])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to run all models and get a prediction - using the best features from our grid search \n",
    "# Inputs are the dataset's we want to look at \n",
    "\n",
    "\n",
    "def fit_and_predict_all_models(logit_data,logit_valid,tree_data,tree_valid,ann_data,ann_valid,neuralnet):\n",
    "    \n",
    "    '''\n",
    "    could make this function do all the smote and rejoing the validation set again sincce i am making my own, so there are more training examples\n",
    "    \n",
    "    '''\n",
    "   \n",
    "  \n",
    "    \n",
    "    #Train test split \n",
    "    def return_trainTest_split(Dataset):\n",
    "        # shuffle data\n",
    "        Dataset = Dataset.sample(frac = 1,random_state=0)\n",
    "        Features = Dataset.drop('treat', axis=1)\n",
    "        Target = Dataset['treat']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(Features, Target, test_size=0.3, shuffle=True)\n",
    "        return X_train, X_test, y_train, y_test;\n",
    "\n",
    "    \n",
    "    resample=SMOTEENN(random_state=0) # resampling\n",
    "    \n",
    "    '''\n",
    "    '''\n",
    "    \n",
    "    # Logit\n",
    "    \n",
    "    X_train1, X_test1, y_train1, y_test1 = return_trainTest_split(logit_data)\n",
    "    X_train1, y_train1 = resample.fit_resample(X_train1, y_train1)\t\n",
    "    \n",
    "    shuffled = pd.concat([pd.DataFrame(X_train1),pd.DataFrame(y_train1)],axis=1).sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "    X_train1, y_train1 = shuffled.iloc[:,:-1], shuffled.iloc[:,-1]\n",
    "    \n",
    "    logit_demo = LogisticRegression(penalty='none',solver='lbfgs', class_weight=\"balanced\",max_iter=500,random_state=0)\n",
    "    \n",
    "    logit_demo.fit(X_train1,y_train1)\n",
    "    \n",
    "    logit_valid = logit_valid.sample(frac = 1,random_state=0)\n",
    "    X_test1 = logit_valid.drop('treat',axis=1, inplace = False)\n",
    "    y_test1 = logit_valid['treat']\n",
    "    \n",
    "    predicted_proba_logit = logit_demo.predict_proba(X_test1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Decision tree models \n",
    "    \n",
    "    X_train2, X_test2, y_train2, y_test2 = return_trainTest_split(tree_data)\n",
    "\n",
    "    X_train2, y_train2 = resample.fit_resample(X_train2, y_train2)\t\n",
    "    shuffled = pd.concat([pd.DataFrame(X_train2),pd.DataFrame(y_train2)],axis=1).sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "    X_train2, y_train2 = shuffled.iloc[:,:-1], shuffled.iloc[:,-1]\n",
    "\n",
    "    tree_valid = tree_valid.sample(frac = 1,random_state=0)\n",
    "    X_test2 = tree_valid.drop('treat',axis=1, inplace = False)\n",
    "    y_test2 = tree_valid['treat']\n",
    "\n",
    "    # Cart\n",
    "    \n",
    "    CART_demo = tree.DecisionTreeClassifier(random_state=0,criterion = 'entropy',max_depth = 5, max_leaf_nodes=12,min_samples_leaf=2)\n",
    "    CART_demo.fit(X_train2,y_train2)\n",
    "    predicted_proba_cart = CART_demo.predict_proba(X_test2)\n",
    "    #RF\n",
    "    forest_demo = RandomForestClassifier(random_state=0,criterion='entropy',max_depth=8,max_features='auto',min_samples_split=3,n_estimators=500)\n",
    "    forest_demo.fit(X_train2,y_train2)\n",
    "    predicted_proba_forest = forest_demo.predict_proba(X_test2)\n",
    "    #boost\n",
    "    boost_demo = XGBClassifier(objective= 'binary:logistic',booser='dart',colsample_bytree=0.3,gamma=0.5,learning_rate=0.05,mind_child_weight=1,subsample=0.5,seed=0,nthread=4)  \n",
    "    boost_demo.fit(X_train2,y_train2)\n",
    "    predicted_proba_boost = boost_demo.predict_proba(X_test2)\n",
    "\n",
    "    #ANN\n",
    "    X_train3, X_test3, y_train3, y_test3 = return_trainTest_split(ann_data)\n",
    "    X_train3, y_train3 = resample.fit_resample(X_train3, y_train3)\t\n",
    "    shuffled = pd.concat([pd.DataFrame(X_train3),pd.DataFrame(y_train3)],axis=1).sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "    X_train3, y_train3 = shuffled.iloc[:,:-1], shuffled.iloc[:,-1]\n",
    "\n",
    "    ann_valid = ann_valid.sample(frac = 1,random_state=0)\n",
    "    X_test3 = ann_valid.drop('treat',axis=1, inplace = False)\n",
    "    y_test3 = ann_valid['treat']\n",
    "\n",
    "    Epochs = 100\n",
    "    learning_rate = 0.0001\n",
    "    weight_decay = 0.05\n",
    "    batch_size = 256\n",
    "\n",
    "    X_train3 = X_train3.to_numpy()\n",
    "    y_train3 = y_train3.to_numpy()\n",
    "    X_train3 = X_train3.astype(np.float32)\n",
    "    y_train3 = y_train3.astype(np.float32)\n",
    "    y_train3 = np.reshape(y_train3,(y_train3.shape[0],1))\n",
    "\n",
    "    X_test3 = X_test3.to_numpy()\n",
    "    y_test3 = y_test3.to_numpy()\n",
    "    X_test3 = X_test3.astype(np.float32)\n",
    "    y_test3 = y_test3.astype(np.float32)\n",
    "    y_test3 = np.reshape(y_test3,(y_test3.shape[0],1))\n",
    "\n",
    "    ann_demo = NeuralNetClassifier(neuralnet,max_epochs=Epochs,lr=learning_rate,optimizer__weight_decay=weight_decay,batch_size =batch_size,optimizer = optim.Adam,criterion = nn.BCELoss,iterator_valid__shuffle=False,verbose=0 )\n",
    "\n",
    "    ann_demo.fit(X_train3,y_train3)\n",
    "    predicted_proba_ANN = ann_demo.predict_proba(X_test3)\n",
    "    \n",
    "    return predicted_proba_logit,predicted_proba_cart,predicted_proba_forest,predicted_proba_boost,predicted_proba_ANN,y_test1,y_test2,y_test3;\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logit_proba1,cart_proba1,forest_proba1,boost_proba1,ann_proba1,yl1,yt1,ya1 = fit_and_predict_all_models(cps_lalonde_subset_logit,\n",
    "                                                                                            cps_lalonde_subset_valid_logit,\n",
    "                                                                                            cps_lalonde_subset_trees,\n",
    "                                                                                            cps_lalonde_subset_valid_trees,\n",
    "                                                                                            cps_lalonde_subset_ann,\n",
    "                                                                                            cps_lalonde_subset_valid_ann,\n",
    "                                                                                            twoLayerNN1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logit_proba2,cart_proba2,forest_proba2,boost_proba2,ann_proba2,yl2,yt2,ya2 = fit_and_predict_all_models(psid_lalonde_subset_logit,\n",
    "                                                                                            psid_lalonde_subset_valid_logit,\n",
    "                                                                                            psid_lalonde_subset_trees,\n",
    "                                                                                            psid_lalonde_subset_valid_trees,\n",
    "                                                                                            psid_lalonde_subset_ann,\n",
    "                                                                                            psid_lalonde_subset_valid_ann,\n",
    "                                                                                            twoLayerNN1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logit_proba3,cart_proba3,forest_proba3,boost_proba3,ann_proba3,yl3,yt3,ya3 = fit_and_predict_all_models(cps_dehWab_subset_logit,\n",
    "                                                                                            cps_dehWab_subset_valid_logit,\n",
    "                                                                                            cps_dehWab_subset_trees,\n",
    "                                                                                            cps_dehWab_subset_valid_trees,\n",
    "                                                                                            cps_dehWab_subset_ann,\n",
    "                                                                                            cps_dehWab_subset_valid_ann,\n",
    "                                                                                            twoLayerNN2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logit_proba4,cart_proba4,forest_proba4,boost_proba4,ann_proba4,yl4,yt4,ya4 = fit_and_predict_all_models(psid_dehWab_subset_logit,\n",
    "                                                                                            psid_dehWab_subset_valid_logit,\n",
    "                                                                                            psid_dehWab_subset_trees,\n",
    "                                                                                            psid_dehWab_subset_valid_trees,\n",
    "                                                                                            psid_dehWab_subset_ann,\n",
    "                                                                                            psid_dehWab_subset_valid_ann,\n",
    "                                                                                            twoLayerNN2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration curve for each models prediction \n",
    "from sklearn.calibration import calibration_curve\n",
    "logit_fop1  , logit_mpv1  = calibration_curve(yl1, logit_proba1[:,1], n_bins=10, normalize=True)\n",
    "cart_fop1   , cart_mpv1   = calibration_curve(yt1, cart_proba1[:,1], n_bins=10, normalize=True)\n",
    "forest_fop1  , forest_mpv1 = calibration_curve(yt1, forest_proba1[:,1], n_bins=10, normalize=True)\n",
    "boost_fop1  , boost_mpv1  = calibration_curve(yt1, boost_proba1[:,1], n_bins=10, normalize=True)\n",
    "ann_fop1    , ann_mpv1    = calibration_curve(ya1, ann_proba1, n_bins=10, normalize=True)\n",
    "\n",
    "logit_fop2  , logit_mpv2  = calibration_curve(yl2, logit_proba2[:,1], n_bins=10, normalize=True)\n",
    "cart_fop2   , cart_mpv2   = calibration_curve(yt2, cart_proba2[:,1], n_bins=10, normalize=True)\n",
    "forest_fop2  , forest_mpv2 = calibration_curve(yt2, forest_proba2[:,1], n_bins=10, normalize=True)\n",
    "boost_fop2  , boost_mpv2  = calibration_curve(yt2, boost_proba2[:,1], n_bins=10, normalize=True)\n",
    "ann_fop2    , ann_mpv2    = calibration_curve(ya2, ann_proba2, n_bins=10, normalize=True)\n",
    "\n",
    "logit_fop3  , logit_mpv3  = calibration_curve(yl3, logit_proba3[:,1], n_bins=10, normalize=True)\n",
    "cart_fop3   , cart_mpv3   = calibration_curve(yt3, cart_proba3[:,1], n_bins=10, normalize=True)\n",
    "forest_fop3  , forest_mpv3 = calibration_curve(yt3, forest_proba3[:,1], n_bins=10, normalize=True)\n",
    "boost_fop3  , boost_mpv3  = calibration_curve(yt3, boost_proba3[:,1], n_bins=10, normalize=True)\n",
    "ann_fop3    , ann_mpv3    = calibration_curve(ya3, ann_proba3, n_bins=10, normalize=True)\n",
    "\n",
    "logit_fop4  , logit_mpv4  = calibration_curve(yl4, logit_proba4[:,1], n_bins=10, normalize=True)\n",
    "cart_fop4   , cart_mpv4   = calibration_curve(yt4, cart_proba4[:,1], n_bins=10, normalize=True)\n",
    "forest_fop4  , forest_mpv4 = calibration_curve(yt4, forest_proba4[:,1], n_bins=10, normalize=True)\n",
    "boost_fop4  , boost_mpv4  = calibration_curve(yt4, boost_proba4[:,1], n_bins=10, normalize=True)\n",
    "ann_fop4    , ann_mpv4    = calibration_curve(ya4, ann_proba4, n_bins=10, normalize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import scipy.stats as stat\n",
    "import pylab \n",
    "from tueplots import axes, bundles , figsizes, fonts,fontsizes\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2,figsize = (8,6))\n",
    "\n",
    "plt.rcParams.update({\"figure.dpi\": 150}) \n",
    "plt.rcParams.update(bundles.neurips2022())\n",
    "plt.rcParams.update(fonts.neurips2022())\n",
    "plt.rcParams.update(axes.tick_direction( y=\"in\"))\n",
    "plt.rcParams.update(axes.color(base=\"black\"))\n",
    "plt.rcParams.update(axes.lines())\n",
    "\n",
    "\n",
    "ax[0,0].grid(color='white', axis='x')\n",
    "ax[0,0].grid(color='white', axis='y')\n",
    "ax[0,1].grid(color='white', axis='x')\n",
    "ax[0,1].grid(color='white', axis='y')\n",
    "ax[1,0].grid(color='white', axis='x')\n",
    "ax[1,0].grid(color='white', axis='y')\n",
    "ax[1,1].grid(color='white', axis='x')\n",
    "ax[1,1].grid(color='white', axis='y')\n",
    "\n",
    "\n",
    "marker = 's'\n",
    "linewidth = 1\n",
    "marker_size = 3\n",
    "\n",
    "ax[0,0].plot([0, 1], [0, 1], linestyle='dotted', label='perfectly calibrated',linewidth=1,color='black')\n",
    "ax[0,0].plot(logit_mpv1  , logit_fop1 , marker = marker , label='Logit ',linewidth=linewidth,markersize=marker_size)\n",
    "ax[0,0].plot(cart_mpv1   , cart_fop1, marker  = marker  ,label='cart ',linewidth=linewidth,markersize=marker_size)\n",
    "ax[0,0].plot(forest_mpv1 , forest_fop1,marker = marker  , label='forest ',linewidth=linewidth,markersize=marker_size)\n",
    "ax[0,0].plot(boost_mpv1  , boost_fop1,marker = marker  , label='boost ',linewidth=linewidth,markersize=marker_size)\n",
    "ax[0,0].plot(ann_mpv1    , ann_fop1 ,marker = marker , label='ANN ',linewidth=linewidth,markersize=marker_size)\n",
    "ax[0,0].set_xlabel('Mean predicted value')\n",
    "ax[0,0].set_ylabel('Fraction of positives')\n",
    "\n",
    "ax[0,1].plot([0, 1], [0, 1], linestyle='dotted', label='perfectly calibrated',linewidth=1,color='black')\n",
    "ax[0,1].plot(logit_mpv2  , logit_fop2 , marker = marker , label='Logit ',linewidth=linewidth,markersize=marker_size)\n",
    "ax[0,1].plot(cart_mpv2   , cart_fop2  , marker  = marker  ,label='cart ',linewidth=linewidth,markersize=marker_size)\n",
    "ax[0,1].plot(forest_mpv2 , forest_fop2 ,marker = marker  , label='forest ',linewidth=linewidth,markersize=marker_size)\n",
    "ax[0,1].plot(boost_mpv2  , boost_fop2  ,marker = marker  , label='boost ',linewidth=linewidth,markersize=marker_size)\n",
    "ax[0,1].plot(ann_mpv2    , ann_fop2    , marker = marker , label='ANN ',linewidth=linewidth,markersize=marker_size)\n",
    "ax[0,1].set_xlabel('Mean predicted value')\n",
    "ax[0,1].set_ylabel('Fraction of positives')\n",
    "\n",
    "ax[1,0].plot([0, 1], [0, 1], linestyle='dotted', label='perfectly calibrated',linewidth=1,color='black')\n",
    "ax[1,0].plot(logit_mpv3  , logit_fop3  ,marker = marker , label='Logit ',linewidth=linewidth,markersize=marker_size)\n",
    "ax[1,0].plot(cart_mpv3   , cart_fop3   ,marker  = marker  ,label='cart ',linewidth=linewidth,markersize=marker_size)\n",
    "ax[1,0].plot(forest_mpv3 , forest_fop3 ,marker = marker  , label='forest ',linewidth=linewidth,markersize=marker_size)\n",
    "ax[1,0].plot(boost_mpv3  , boost_fop3  ,marker = marker  , label='boost ',linewidth=linewidth,markersize=marker_size)\n",
    "ax[1,0].plot(ann_mpv3    , ann_fop3    , marker = marker , label='ANN ',linewidth=linewidth,markersize=marker_size)\n",
    "ax[1,0].set_xlabel('Mean predicted value')\n",
    "ax[1,0].set_ylabel('Fraction of positives')\n",
    "\n",
    "ax[1,1].plot([0, 1], [0, 1], linestyle='dotted', label='perfectly calibrated',linewidth=linewidth,color='black')\n",
    "ax[1,1].plot(logit_mpv4  , logit_fop4  ,marker = marker , label='Logit ',linewidth=linewidth,markersize=marker_size)\n",
    "ax[1,1].plot(cart_mpv4   , cart_fop4   ,marker  = marker  ,label='cart ',linewidth=linewidth,markersize=marker_size)\n",
    "ax[1,1].plot(forest_mpv4 , forest_fop4 ,marker = marker  , label='forest ',linewidth=linewidth,markersize=marker_size)\n",
    "ax[1,1].plot(boost_mpv4  , boost_fop4  ,marker = marker  , label='boost ',linewidth=linewidth,markersize=marker_size)\n",
    "ax[1,1].plot(ann_mpv4    , ann_fop4    , marker = marker , label='ANN ',linewidth=linewidth,markersize=marker_size)\n",
    "ax[1,1].set_xlabel('Mean predicted value')\n",
    "ax[1,1].set_ylabel('Fraction of positives')\n",
    "\n",
    "\n",
    "plt.legend(loc=(1.04, 0.1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Box plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Box plots of estimated propensity scores across each model - CPS group \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "# Neat formatting for plots \n",
    "plt.rcParams.update({\"figure.dpi\": 150}) \n",
    "plt.rcParams.update(bundles.neurips2022())\n",
    "plt.rcParams.update(fonts.neurips2022())\n",
    "plt.rcParams.update(axes.tick_direction( y=\"in\"))\n",
    "plt.rcParams.update(axes.color(base=\"black\"))\n",
    "plt.rcParams.update(figsizes.neurips2022())\n",
    "plt.rcParams.update(fontsizes.neurips2022())\n",
    "\n",
    "ax.set_facecolor('white')\n",
    "# Set boarder colour \n",
    "for spine in ax.spines:\n",
    "    ax.spines[spine].set_color('black')\n",
    "    ax.spines[spine].set_linewidth(1)\n",
    "\n",
    "ax.grid(color='darkgrey', axis='y', linestyle='--', linewidth=1, alpha=1)\n",
    "ax.grid(color='white', axis='x')\n",
    "\n",
    "# Cusotmise median line \n",
    "medianprops = dict(linestyle='-', linewidth = 1, color='k')\n",
    "boxprops = dict(linewidth = 1)\n",
    "whiskerprops = dict(linewidth = 1)\n",
    "capprops = dict(linewidth = 1)\n",
    "\n",
    "flierprops = {'marker': 'o', 'markersize': 3,'linewidth':1}\n",
    "\n",
    "\n",
    "treated_propensity_PS = logitNswCps_lalonde['propensity_score'][logitNswCps_lalonde['treat']==1]\n",
    "\n",
    "Logit_Cpscomprison_PS = logitNswCps_lalonde['propensity_score'][logitNswCps_lalonde['treat']==0]\n",
    "Logit_Psidcomprison_PS = logitNswPsid_lalonde['propensity_score'][logitNswPsid_lalonde['treat']==0]\n",
    "\n",
    "cart_Cpscomprison_PS = cartNswCps_lalonde['propensity_score'][cartNswCps_lalonde['treat']==0]\n",
    "cart_Psidcomprison_PS = cartNswPsid_lalonde['propensity_score'][cartNswPsid_lalonde['treat']==0]\n",
    "\n",
    "ax.boxplot([treated_propensity_PS,Logit_Cpscomprison_PS,Logit_Psidcomprison_PS,cart_Cpscomprison_PS,cart_Psidcomprison_PS],\n",
    "            medianprops=medianprops,\n",
    "            boxprops = boxprops,\n",
    "            whiskerprops=whiskerprops,\n",
    "            capprops = capprops ,\n",
    "            flierprops = flierprops,\n",
    "            widths=0.3) \n",
    "\n",
    "#ax.set_xlim(0.5,5.5)\n",
    "#ax.set_ylim(-2000,62000)\n",
    "\n",
    "plt.xticks([1,2,3,4,5],['Treated', 'Logit cps ','Logit psid','Cart  cps','Cart psid'])\n",
    "\n",
    "\n",
    "ax.set_xlabel('Model')\n",
    "\n",
    "ax.set_ylabel('Propensity score')\n",
    "\n",
    "plt.savefig('/Users/mawuliagamah/gitprojects/causal_inference/causal_inference/Jupyter notebooks/lalonde_notebook_plots/_boxplots.png', bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "44a7c5cf9512a4ad670122a007d348488b56f79d3796a47615dc74da9f36a764"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
